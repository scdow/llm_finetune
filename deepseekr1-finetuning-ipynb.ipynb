{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./deepseekr1-finetuning-ipynb.ipynb\n",
      "./playground.json\n",
      "./refe link.txt\n",
      "./DeepSeekR1_GoogleGeminiPro_RAG_Streamlit-main\\deepseekgoogle_rag.py\n",
      "./DeepSeekR1_GoogleGeminiPro_RAG_Streamlit-main\\deepseekv3-finetuning-ipynb (1).ipynb\n",
      "./DeepSeekR1_GoogleGeminiPro_RAG_Streamlit-main\\deepseekvsgemini.py\n",
      "./DeepSeekR1_GoogleGeminiPro_RAG_Streamlit-main\\README.md\n",
      "./llama.cpp\\.git\\config\n",
      "./llama.cpp\\.git\\description\n",
      "./llama.cpp\\.git\\HEAD\n",
      "./llama.cpp\\.git\\hooks\\applypatch-msg.sample\n",
      "./llama.cpp\\.git\\hooks\\commit-msg.sample\n",
      "./llama.cpp\\.git\\hooks\\fsmonitor-watchman.sample\n",
      "./llama.cpp\\.git\\hooks\\post-update.sample\n",
      "./llama.cpp\\.git\\hooks\\pre-applypatch.sample\n",
      "./llama.cpp\\.git\\hooks\\pre-commit.sample\n",
      "./llama.cpp\\.git\\hooks\\pre-merge-commit.sample\n",
      "./llama.cpp\\.git\\hooks\\pre-push.sample\n",
      "./llama.cpp\\.git\\hooks\\pre-rebase.sample\n",
      "./llama.cpp\\.git\\hooks\\pre-receive.sample\n",
      "./llama.cpp\\.git\\hooks\\prepare-commit-msg.sample\n",
      "./llama.cpp\\.git\\hooks\\push-to-checkout.sample\n",
      "./llama.cpp\\.git\\hooks\\sendemail-validate.sample\n",
      "./llama.cpp\\.git\\hooks\\update.sample\n",
      "./llama.cpp\\.git\\info\\exclude\n",
      "./llama.cpp\\.git\\objects\\pack\\tmp_pack_sAgYbX\n",
      "./model\\config.json\n",
      "./model\\generation_config.json\n",
      "./model\\model.safetensors\n",
      "./model\\special_tokens_map.json\n",
      "./model\\tokenizer.json\n",
      "./model\\tokenizer_config.json\n",
      "./outputs\\checkpoint-20\\adapter_config.json\n",
      "./outputs\\checkpoint-20\\adapter_model.safetensors\n",
      "./outputs\\checkpoint-20\\optimizer.pt\n",
      "./outputs\\checkpoint-20\\README.md\n",
      "./outputs\\checkpoint-20\\rng_state.pth\n",
      "./outputs\\checkpoint-20\\scheduler.pt\n",
      "./outputs\\checkpoint-20\\special_tokens_map.json\n",
      "./outputs\\checkpoint-20\\tokenizer.json\n",
      "./outputs\\checkpoint-20\\tokenizer_config.json\n",
      "./outputs\\checkpoint-20\\trainer_state.json\n",
      "./outputs\\checkpoint-20\\training_args.bin\n",
      "./unsloth_compiled_cache\\UnslothAlignPropTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothBCOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothCPOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothDDPOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothDPOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothGKDTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothKTOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothORPOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothPPOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothPRMTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothRewardTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothRLOOTrainer.py\n",
      "./unsloth_compiled_cache\\UnslothSFTTrainer.py\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothAlignPropTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothBCOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothCPOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothDDPOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothDPOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothGKDTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothKTOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothORPOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothPPOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothPRMTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothRewardTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothRLOOTrainer.cpython-312.pyc\n",
      "./unsloth_compiled_cache\\__pycache__\\UnslothSFTTrainer.cpython-312.pyc\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# 打印当前目录下的文件\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek R1 Model Fine-Tuning (LORA) with GPT-4 Dataset [Unsloth and OLLAMA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unsloth**\n",
    "\n",
    "高效微调llm的框架，支持LoRA，fast LoRA  \n",
    "Unsloth appears to be a tool or framework designed for efficient fine-tuning of language models. From the context, it likely incorporates techniques like Low-Rank Adaptation (LoRA) and other efficiency optimizations to fine-tune large models, such as Llama derivatives, with minimal computational resources.\n",
    "\n",
    "The key features of Unsloth as implied by your description might include:\n",
    "\n",
    "1. **Efficient Fine-Tuning**: Instead of updating the entire model's weights, it leverages methods like LoRA, which fine-tune a smaller subset of the parameters, making it resource-efficient.\n",
    "\n",
    "2. **Simplified Workflow**: The process of loading models, configuring parameters, and training appears streamlined, allowing developers to focus on specific customizations rather than managing complex infrastructure.\n",
    "\n",
    "3. **Integration with Local Runtimes**: After fine-tuning, exporting to tools like Ollama for local deployment demonstrates its support for practical application.\n",
    "\n",
    "## **DeepSeek-R1**\n",
    "\n",
    "llm  \n",
    "The DeepSeek-R1 is a versatile robot for exploration and inspection in tough environments. With AI, precise sensors, and multi-terrain mobility, it handles tasks like data collection, mapping, and monitoring. Customisable for search and rescue, inspections, or research, it ensures reliable performance in hazardous areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install unsloth\n",
    "\n",
    "#  WARNING: The script shtab.exe is installed in 'C:\\Users\\Emma2\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
    "#   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
    "#   WARNING: The script diffusers-cli.exe is installed in 'C:\\Users\\Emma2\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
    "#   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
    "#   WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\Emma2\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
    "#   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
    "#   WARNING: The script trl.exe is installed in 'C:\\Users\\Emma2\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
    "#   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "用unsloth加载模型、分词器  \n",
    "Using Unsloth, you load the DeepSeek-R1 Distilled Llama-8B model, a smaller, faster version of the Llama model optimised for performance while retaining accuracy.\n",
    "Along with the model, you also load its tokeniser. The tokeniser breaks down input text into smaller units (tokens) that the model can process.AttributeError\n",
    "\n",
    "## Importance? \n",
    "Loading the model and tokenizer is the foundation for fine-tuning since they define how text inputs are processed and predictions are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:35:22.644485Z",
     "iopub.status.busy": "2025-01-28T19:35:22.644139Z",
     "iopub.status.idle": "2025-01-28T19:36:19.089957Z",
     "shell.execute_reply": "2025-01-28T19:36:19.089148Z",
     "shell.execute_reply.started": "2025-01-28T19:35:22.644457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.4.dev414+g01b3fd0a.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"\n",
    "# ] = \"0\"  # 禁用hf_transfer加速下载\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",  #自定义选用的llm，默认从huggingface下载\n",
    "    model_name = \"unsloth/deepseek-r1-distill-qwen-1.5b-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 10240,  #？更长点？2048, 32768\n",
    "    dtype = None,   #模型权重的数值精度，none为自动\n",
    "    load_in_4bit = True,  #以4bit精度微调\n",
    ")\n",
    "# 下载cpp环境以支持unsloth包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536, padding_idx=151654)\n",
      "    (layers): ModuleList(\n",
      "      (0): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "      (1-2): 2 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "      (3-25): 23 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "      (26): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "      (27): Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n",
      "\n",
      "Parameters:\n",
      "Parameter name: model.embed_tokens.weight, Shape: torch.Size([151936, 1536])\n",
      "Parameter name: model.layers.0.self_attn.q_proj.weight, Shape: torch.Size([1536, 1536])\n",
      "Parameter name: model.layers.0.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.0.self_attn.k_proj.weight, Shape: torch.Size([256, 1536])\n",
      "Parameter name: model.layers.0.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.0.self_attn.v_proj.weight, Shape: torch.Size([256, 1536])\n",
      "Parameter name: model.layers.0.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.0.self_attn.o_proj.weight, Shape: torch.Size([1536, 1536])\n",
      "Parameter name: model.layers.0.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.0.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.0.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.0.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.0.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.1.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.1.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.1.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.1.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.1.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.1.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.1.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.1.mlp.gate_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: model.layers.1.mlp.up_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: model.layers.1.mlp.down_proj.weight, Shape: torch.Size([1536, 8960])\n",
      "Parameter name: model.layers.1.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.1.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.2.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.2.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.2.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.2.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.2.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.2.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.2.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.2.mlp.gate_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: model.layers.2.mlp.up_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: model.layers.2.mlp.down_proj.weight, Shape: torch.Size([1536, 8960])\n",
      "Parameter name: model.layers.2.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.2.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.3.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.3.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.3.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.3.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.3.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.3.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.3.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.3.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.3.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.3.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.3.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.3.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.4.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.4.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.4.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.4.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.4.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.4.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.4.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.4.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.4.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.4.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.4.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.4.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.5.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.5.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.5.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.5.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.5.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.5.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.5.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.5.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.5.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.5.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.5.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.5.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.6.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.6.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.6.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.6.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.6.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.6.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.6.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.6.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.6.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.6.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.6.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.6.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.7.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.7.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.7.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.7.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.7.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.7.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.7.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.7.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.7.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.7.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.7.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.7.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.8.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.8.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.8.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.8.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.8.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.8.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.8.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.8.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.8.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.8.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.8.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.8.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.9.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.9.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.9.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.9.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.9.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.9.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.9.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.9.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.9.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.9.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.9.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.9.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.10.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.10.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.10.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.10.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.10.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.10.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.10.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.10.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.10.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.10.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.10.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.10.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.11.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.11.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.11.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.11.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.11.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.11.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.11.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.11.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.11.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.11.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.11.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.11.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.12.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.12.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.12.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.12.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.12.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.12.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.12.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.12.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.12.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.12.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.12.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.12.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.13.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.13.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.13.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.13.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.13.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.13.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.13.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.13.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.13.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.13.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.13.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.13.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.14.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.14.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.14.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.14.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.14.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.14.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.14.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.14.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.14.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.14.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.14.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.14.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.15.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.15.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.15.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.15.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.15.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.15.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.15.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.15.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.15.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.15.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.15.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.15.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.16.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.16.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.16.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.16.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.16.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.16.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.16.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.16.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.16.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.16.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.16.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.16.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.17.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.17.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.17.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.17.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.17.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.17.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.17.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.17.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.17.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.17.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.17.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.17.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.18.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.18.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.18.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.18.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.18.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.18.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.18.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.18.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.18.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.18.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.18.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.18.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.19.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.19.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.19.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.19.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.19.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.19.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.19.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.19.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.19.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.19.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.19.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.19.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.20.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.20.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.20.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.20.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.20.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.20.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.20.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.20.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.20.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.20.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.20.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.20.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.21.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.21.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.21.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.21.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.21.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.21.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.21.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.21.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.21.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.21.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.21.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.21.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.22.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.22.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.22.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.22.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.22.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.22.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.22.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.22.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.22.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.22.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.22.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.22.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.23.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.23.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.23.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.23.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.23.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.23.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.23.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.23.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.23.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.23.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.23.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.23.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.24.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.24.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.24.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.24.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.24.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.24.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.24.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.24.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.24.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.24.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.24.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.24.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.25.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.25.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.25.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.25.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.25.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.25.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.25.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.25.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.25.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.25.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.25.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.25.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.26.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.26.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.26.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.26.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.26.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.26.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.26.self_attn.o_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.26.mlp.gate_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: model.layers.26.mlp.up_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: model.layers.26.mlp.down_proj.weight, Shape: torch.Size([1536, 8960])\n",
      "Parameter name: model.layers.26.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.26.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.27.self_attn.q_proj.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: model.layers.27.self_attn.q_proj.bias, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.27.self_attn.k_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.27.self_attn.k_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.27.self_attn.v_proj.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: model.layers.27.self_attn.v_proj.bias, Shape: torch.Size([256])\n",
      "Parameter name: model.layers.27.self_attn.o_proj.weight, Shape: torch.Size([1536, 1536])\n",
      "Parameter name: model.layers.27.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.27.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.27.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: model.layers.27.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.layers.27.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: model.norm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: lm_head.weight, Shape: torch.Size([151936, 1536])\n"
     ]
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "# 方法 1: 直接打印模型\n",
    "print(\"Model structure:\")\n",
    "print(model)\n",
    "\n",
    "# 方法 2: 查看参数名称和形状\n",
    "print(\"\\nParameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameter-Efficient Fine-Tuning (PEFT) \n",
    "The **FastLanguageModel.get_peft_model** function modifies a pre-trained language model to use PEFT techniques, which allow fine-tuning of the model using fewer resources and parameters.\n",
    "\n",
    "The method introduces additional tunable parameters (like LoRA matrices) to specific layers of the model while freezing most of the original model weights.\n",
    "\n",
    "PEFT包支持的微调方法：  \n",
    "LoRA（Low-Rank Adaptation）：通过引入低秩矩阵来微调模型权重，仅需训练少量参数即可达到与全参数微调相当的性能。（LORA冻结预训练模型的权重，仅在每个Transformer层+中注入可训练的低秩分解矩阵+，大大减少了下游任务的可训练参数数量。）   \n",
    "Prefix Tuning：通过优化输入提示（prompt）来调整模型，不改变模型权重，适应性强。  \n",
    "Prompt Tuning：通过优化连续提示来微调模型，适用于零样本和少样本学习。  \n",
    "BitFit：仅对模型的偏置项进行微调，极大地减少了需要优化的参数数量。  \n",
    "QLoRA: 使用4位量化和低秩适配器，结合双量化和分页优化器。  \n",
    "等等...  \n",
    "https://huggingface.co/docs/peft/index   \n",
    "\n",
    "\n",
    "***In this Project, we use LORA to fine-tune the DeepSeek R1 LLM.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it takes the existing model. Then, it applies a technique called PEFT (Parameter-Efficient Fine-Tuning).  Think of it as adding small, adjustable \"knobs\" instead of changing the whole engine.\n",
    "\n",
    "r=4 and lora_alpha=16 are just settings for how many \"knobs\" and how sensitive they are.  target_modules specifies where these knobs are attached – specifically, parts of the model that handle questions, keys, values, and outputs.  lora_dropout=0 means no \"knobs\" are randomly turned off during training.\n",
    "\n",
    "bias=\"none\" means no extra adjustments to the model's biases. use_gradient_checkpointing=\"unsloth\" is a memory-saving trick for training. random_state=42 ensures we get the same results if we run this again.  use_rslora=False and loftq_config=None are more advanced settings that are turned off here.  \n",
    "\n",
    "调整强度 = alpha / r  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:37:14.224138Z",
     "iopub.status.busy": "2025-01-28T19:37:14.223695Z",
     "iopub.status.idle": "2025-01-28T19:37:20.051562Z",
     "shell.execute_reply": "2025-01-28T19:37:20.050877Z",
     "shell.execute_reply.started": "2025-01-28T19:37:14.224087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 4,   #指定低秩矩阵的秩（rank），r越小，参数量越小，矩阵分解，[原始维度, r] 和 [r, 原始维度]\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 333,  #seed实验时保持相同\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536, padding_idx=151654)\n",
      "        (layers): ModuleList(\n",
      "          (0): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (1-2): 2 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (3-25): 23 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (26): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (27): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Parameters:\n",
      "Parameter name: base_model.model.model.embed_tokens.weight, Shape: torch.Size([151936, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1536, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight, Shape: torch.Size([256, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight, Shape: torch.Size([256, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1536, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.0.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.0.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.0.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.0.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.0.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.1.mlp.gate_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: base_model.model.model.layers.1.mlp.up_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: base_model.model.model.layers.1.mlp.down_proj.weight, Shape: torch.Size([1536, 8960])\n",
      "Parameter name: base_model.model.model.layers.1.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.1.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.2.mlp.gate_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: base_model.model.model.layers.2.mlp.up_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: base_model.model.model.layers.2.mlp.down_proj.weight, Shape: torch.Size([1536, 8960])\n",
      "Parameter name: base_model.model.model.layers.2.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.2.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.3.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.3.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.3.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.3.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.3.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.4.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.4.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.4.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.4.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.4.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.5.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.5.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.5.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.5.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.5.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.6.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.6.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.6.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.6.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.6.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.7.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.7.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.7.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.7.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.7.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.8.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.8.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.8.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.8.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.8.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.9.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.9.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.9.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.9.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.9.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.10.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.10.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.10.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.10.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.10.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.11.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.11.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.11.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.11.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.11.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.12.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.12.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.12.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.12.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.12.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.13.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.13.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.13.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.13.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.13.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.14.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.14.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.14.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.14.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.14.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.15.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.15.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.15.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.15.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.15.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.16.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.16.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.16.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.16.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.16.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.17.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.17.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.17.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.17.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.17.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.18.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.18.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.18.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.18.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.18.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.19.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.19.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.19.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.19.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.19.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.20.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.20.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.20.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.20.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.20.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.21.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.21.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.21.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.21.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.21.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.22.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.22.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.22.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.22.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.22.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.23.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.23.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.23.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.23.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.23.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.24.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.24.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.24.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.24.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.24.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.25.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.25.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.25.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.25.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.25.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.26.mlp.gate_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: base_model.model.model.layers.26.mlp.up_proj.weight, Shape: torch.Size([8960, 1536])\n",
      "Parameter name: base_model.model.model.layers.26.mlp.down_proj.weight, Shape: torch.Size([1536, 8960])\n",
      "Parameter name: base_model.model.model.layers.26.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.26.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight, Shape: torch.Size([1179648, 1])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.q_proj.base_layer.bias, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.k_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight, Shape: torch.Size([196608, 1])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.v_proj.base_layer.bias, Shape: torch.Size([256])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([256, 4])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight, Shape: torch.Size([1536, 1536])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([4, 1536])\n",
      "Parameter name: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([1536, 4])\n",
      "Parameter name: base_model.model.model.layers.27.mlp.gate_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.27.mlp.up_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.27.mlp.down_proj.weight, Shape: torch.Size([6881280, 1])\n",
      "Parameter name: base_model.model.model.layers.27.input_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.layers.27.post_attention_layernorm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.model.norm.weight, Shape: torch.Size([1536])\n",
      "Parameter name: base_model.model.lm_head.weight, Shape: torch.Size([151936, 1536])\n"
     ]
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "# 方法 1: 直接打印模型\n",
    "print(\"Model structure:\")\n",
    "print(model)\n",
    "\n",
    "# 方法 2: 查看参数名称和形状\n",
    "print(\"\\nParameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集：vicgalle/alpaca-gpt4  \n",
    "https://huggingface.co/datasets/vicgalle/alpaca-gpt4  \n",
    "The vicgalle/alpaca-gpt4 dataset is a collection of 52,000 instruction-following instances designed to fine-tune language models (LLMs). It was created by Vic Galie and is available on Hugging Face.  \n",
    "包含GPT-4使用与Alpaca相同的提示生成的52K指令数据，该数据集的格式与Alpaca数据相同，但输出由GPT-4生成。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:44:43.524760Z",
     "iopub.status.busy": "2025-01-28T19:44:43.524338Z",
     "iopub.status.idle": "2025-01-28T19:44:46.104782Z",
     "shell.execute_reply": "2025-01-28T19:44:46.103935Z",
     "shell.execute_reply.started": "2025-01-28T19:44:43.524698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'input', 'output', 'text']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"vicgalle/alpaca-gpt4\", split = \"train\")\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:44:47.831145Z",
     "iopub.status.busy": "2025-01-28T19:44:47.830803Z",
     "iopub.status.idle": "2025-01-28T19:44:47.837464Z",
     "shell.execute_reply": "2025-01-28T19:44:47.836520Z",
     "shell.execute_reply.started": "2025-01-28T19:44:47.831121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'List the chemical elements that have an atomic number of 30',\n",
       " 'input': '',\n",
       " 'output': 'The chemical element that has an atomic number of 30 is Zinc (Zn). Atomic number refers to the number of protons in the nucleus of an atom, and since each element has a unique number of protons, there is only one element that has an atomic number of 30.',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nList the chemical elements that have an atomic number of 30\\n\\n### Response:\\nThe chemical element that has an atomic number of 30 is Zinc (Zn). Atomic number refers to the number of protons in the nucleus of an atom, and since each element has a unique number of protons, there is only one element that has an atomic number of 30.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "dataset[29689]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format the dataset in a way suitable for conversational AI training using the ShareGPT format, which is designed for multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:44:48.368943Z",
     "iopub.status.busy": "2025-01-28T19:44:48.368559Z",
     "iopub.status.idle": "2025-01-28T19:45:07.176530Z",
     "shell.execute_reply": "2025-01-28T19:45:07.175532Z",
     "shell.execute_reply.started": "2025-01-28T19:44:48.368914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"{instruction}[[\\nYour input is:\\n{input}]]\",   #合并instruction指令和input输入\n",
    "    output_column_name = \"output\",\n",
    "    conversation_extension = 3, # Select more to handle longer conversations（控制对话的历史拓展轮次？）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': 'Come up with a tagline for a new snack food.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '\"Indulge in the goodness - Happy snacking with every bite!\"'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What is an example of something that does not require input parameters?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"One example of something that doesn't require input parameters is calling the current date or time from the system's clock, using functions such as `time.time()` in Python's `time` module or `Date.now()` in JavaScript's `Date` object. These functions do not require any arguments to be passed and they simply return the current date and time.\"},\n",
       "  {'from': 'human', 'value': 'Create a new art style.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Introducing the new art style: \"Transfusionism\"\\n\\nTransfusionism is an art style that blends elements from different eras, cultures, and mediums to create a unique and harmonious fusion. This style focuses on the use of vibrant colors, intricate patterns, and bold statements.\\n\\nTransfusionism combines traditional art practices such as oil painting and sculpture, with modern techniques like digital art and graphic design. It borrows from the rich tapestry of world cultures, blending diverse elements like the intricate designs of Islamic art, the boldness of African tribal patterns, and the dreamlike quality of Surrealism.\\n\\nThe central philosophy of Transfusionism is the idea that art is constantly evolving and transcending boundaries. It embraces the concept of fluidity, and the idea that forms can merge and create something completely new and fresh.\\n\\nTransfusionist art is energetic, spontaneous, and thought-provoking. It invites the viewer to explore the rich tapestry of elements that make up the piece, and to find their own meaning in the fusion of styles.\\n\\nIn summary, Transfusionism is a dynamic new art style that provides endless creative possibilities, and celebrates the beauty of blending diverse elements to create something truly unique.'}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "dataset[29648]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:07.178493Z",
     "iopub.status.busy": "2025-01-28T19:45:07.178263Z",
     "iopub.status.idle": "2025-01-28T19:45:10.130543Z",
     "shell.execute_reply": "2025-01-28T19:45:10.129425Z",
     "shell.execute_reply.started": "2025-01-28T19:45:07.178473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:10.132983Z",
     "iopub.status.busy": "2025-01-28T19:45:10.132675Z",
     "iopub.status.idle": "2025-01-28T19:45:10.139400Z",
     "shell.execute_reply": "2025-01-28T19:45:10.138464Z",
     "shell.execute_reply.started": "2025-01-28T19:45:10.132960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations'],\n",
      "    num_rows: 52002\n",
      "})\n",
      "{'conversations': [{'content': 'Describe the structure of an atom.', 'role': 'user'}, {'content': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\", 'role': 'assistant'}, {'content': 'Describe the setting of the book \"Alice in Wonderland\".', 'role': 'user'}, {'content': 'The setting of the book \"Alice in Wonderland\" is primarily in a fantasy world located in a rabbit hole that Alice, the main character, falls into. This surreal world is called Wonderland and is filled with strange anthropomorphic creatures, magical occurrences, and illogical events. The environment changes from one scene to another, including lush gardens, forests, and the royal courtyard of the Queen of Hearts. As Alice moves through the different parts of Wonderland, she encounters unusual and puzzling situations, like changing in size, attending an unusual tea party, and partaking in a nonsensical trial. The setting of the book is dream-like and whimsical, with a touch of illogic and absurdity.', 'role': 'assistant'}, {'content': 'Rearrange the words given in the input to make a meaningful sentence.\\nYour input is:\\nsociety a modern in work importance gender of', 'role': 'user'}, {'content': 'The importance of gender in modern society work.', 'role': 'assistant'}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'Describe the structure of an atom.', 'role': 'user'},\n",
       " {'content': \"An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\",\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Describe the setting of the book \"Alice in Wonderland\".',\n",
       "  'role': 'user'},\n",
       " {'content': 'The setting of the book \"Alice in Wonderland\" is primarily in a fantasy world located in a rabbit hole that Alice, the main character, falls into. This surreal world is called Wonderland and is filled with strange anthropomorphic creatures, magical occurrences, and illogical events. The environment changes from one scene to another, including lush gardens, forests, and the royal courtyard of the Queen of Hearts. As Alice moves through the different parts of Wonderland, she encounters unusual and puzzling situations, like changing in size, attending an unusual tea party, and partaking in a nonsensical trial. The setting of the book is dream-like and whimsical, with a touch of illogic and absurdity.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Rearrange the words given in the input to make a meaningful sentence.\\nYour input is:\\nsociety a modern in work importance gender of',\n",
       "  'role': 'user'},\n",
       " {'content': 'The importance of gender in modern society work.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[2])\n",
    "dataset[2]['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizable Chat Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes a dataset, formats it into a chat-friendly format, and then applies a template so that the AI can understand the instructions and responses correctly.  \n",
    "设置对话模板  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:10.140900Z",
     "iopub.status.busy": "2025-01-28T19:45:10.140594Z",
     "iopub.status.idle": "2025-01-28T19:45:17.006478Z",
     "shell.execute_reply": "2025-01-28T19:45:17.005447Z",
     "shell.execute_reply.started": "2025-01-28T19:45:10.140876Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n"
     ]
    }
   ],
   "source": [
    "chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
    "\n",
    "### Instruction:\n",
    "{INPUT}\n",
    "\n",
    "### Response:\n",
    "{OUTPUT}\"\"\"\n",
    "\n",
    "from unsloth import apply_chat_template\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template,\n",
    "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
    ")\n",
    "\n",
    "# 这里没用上？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define and configure a fine-tuning trainer for a language model using the **SFTTrainer** class from the **trl** library (likely for fine-tuning language models) and transformers' **TrainingArguments**  \n",
    "(trl, transformers)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses SFTTrainer from the trl library, which is specifically for Supervised Fine-Tuning.  This means the model will learn from the dataset of instructions and responses you prepared earlier.\n",
    "\n",
    "It takes the model (the tuned language model), the tokenizer (for breaking down text), and the dataset. 模型、分词器、数据集。 dataset_text_field=\"text\" 文本在数据集中大位置 tells it where the actual text is in the dataset.  max_seq_length=2048 （最大序列长度：模型能够处理的最长输入序列的长度，超过此长度的序列将被截断或分块处理。） limits the length of text sequences the model processes at once. dataset_num_proc=2 用于准备数据的进程数量 uses two processes to prepare the data, and packing=False 是否使用打包技术 disables a specific data packing technique.\n",
    "\n",
    "Then, it configures the training process with TrainingArguments.  per_device_train_batch_size=2 means each training device (like a GPU) will process 2 examples at a time. gradient_accumulation_steps=4 combines the gradients from 4 batches to simulate a larger batch size. warmup_steps=5 gradually increases the learning rate at the beginning. max_steps=20 limits the total training steps. learning_rate=2e-4 sets the learning rate.  fp16 and bf16 control the precision of calculations (using either half-precision or bfloat16 if supported, for faster training). logging_steps=1 logs training progress every step. optim=\"adamw_8bit\" specifies the optimizer. weight_decay=0.01 is a regularization technique. lr_scheduler_type=\"linear\" sets how the learning rate changes over time. seed=3407 ensures reproducibility. output_dir=\"outputs\" specifies where to save the trained model. report_to=\"none\" disables reporting to services like WandB (Weights & Biases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:17.008092Z",
     "iopub.status.busy": "2025-01-28T19:45:17.007726Z",
     "iopub.status.idle": "2025-01-28T19:46:03.412875Z",
     "shell.execute_reply": "2025-01-28T19:46:03.411741Z",
     "shell.execute_reply.started": "2025-01-28T19:45:17.008053Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,    #2048，10240\n",
    "    dataset_num_proc = 1,   #1，2，... 用于准备数据的进程数量\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 6,  # batch size，2,4, 每次前向/反向传播时处理的样本数量\n",
    "        gradient_accumulation_steps = 4,  # 梯度累积步数（有效批次大小=2*4=8）\n",
    "        warmup_steps = 5,                 # 学习率预热步数（初期逐步增加学习率）\n",
    "        # num_train_epochs =1,              # epoch完整遍历整个训练集的次数，epoch = 总样本数 / (batch_size * gradient_accumulation_steps) (可不加epoch变量)\n",
    "        max_steps = 50,                   # 最大训练步数（此处20仅为示例，实际需更大; -1意为跟着epoch数量计算）\n",
    "        learning_rate = 2e-4,             # 初始学习率（常用值，可调整）\n",
    "        fp16 = not is_bfloat16_supported(), # 启用FP16混合精度（若不支持BF16）\n",
    "        bf16 = is_bfloat16_supported(),   # 优先使用BF16（兼容性更好，若GPU支持）\n",
    "        logging_steps = 1,                # 每1步记录日志（调试时可高频，生产环境可调低）\n",
    "        optim = \"adamw_8bit\",             # 使用8-bit AdamW优化器（节省显存，需安装bitsandbytes）\n",
    "        weight_decay = 0.01,              # 权重衰减（防过拟合）\n",
    "        lr_scheduler_type = \"linear\",     # 学习率调度策略（线性衰减至零）\n",
    "        seed = 3407,                      # 随机种子（确保实验可复现）\n",
    "        output_dir = \"outputs\",           # 模型和日志输出目录\n",
    "        report_to = \"none\",               # Use this for WandB etc, none禁用第三方报告（如WandB，需时改为\"wandb\"）\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9sAAALICAYAAABvvONdAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAAP+lSURBVHhe7N15XE35/wfw1607cdUNRYuMmgZZhuxFhiKVyTIhO2PPNra+5jckS3aG7EuDxhC+2cOoLIlibI0wtFjKiCJp0aa6n98f0z3fe8+95VY3bnk/H4/zR+fzOZ8+9yyfcz7nfBYBY4yBEEIIIYQQQgghaqPFX0EIIYQQQgghhJCKoco2IYQQQgghhBCiZlTZJoQQQgghhBBC1ExQWX22U1JS+KsIIYQQQgghhJDPQqVWtnV1dfmrCSGEEEIIIYSQao+akRNCCCGEEEIIIWpGlW1CCCGEEEIIIUTNqLJNCCGEEEIIIYSoGVW2CSGEEEIIIYQQNaPKNiGEEEIIIYQQomZU2SaEEEIIIYQQQtSMKtuEEEIIIYQQQoiaUWWbEEIIIYQQQghRM6psE0IIIYQQQgghakaVbUIIIYQQQgghRM2osk0IIYQQQgghhKiZQmU7PT0dycnJ/NUKkpKSkJ6ezl9NCCGEEEIIIYR89hQq20+ePMH69evx6NEjfhAnLi4OmzdvxosXL/hBhBBCCCGEEELIZ0+hst2uXTsMHToUfn5+SivccXFx2LNnD8aNG4cWLVrwgwkhhBBCCCGEkM+eQmUbpVS4Hz16xFW0mzZtKrcNIYQQQgghhBBC/iVgjDH+Sql79+4hICAAEyZMAADs2rULEyZMQOPGjflRFaSkpEBXV5e/mhBCCCGEEEIIqfZKrWyjuMK9e/duAMD06dNVqmiDKtuEEEIIIYQQQj5jH6xsA8CzZ88gEolQv359flCJqLJNCCGEEEIIIeRzpVJluzyosk0IIYQQQggh5HOldIA0QgghhBBCCCGElB9VtgkhhBBCCCGEEDWjyjYhhBBCCCGEEKJmVNmuJlJOTcM34sbos+4G8viBhFR1edHY5NoYYutpCHrGDySV4aOUKUXaOOWjg31PlN+K8u7pYNKWL5BRxA8hhBBCCNF8yp9wSBWTgvDDvyMRKQhfHI67/GBCqrp7ofC6nAI8+R3/vZ7CDyVq9xHKlCJthK4RYnSkAK+y+YEAIEBKKpAQqAX3ncLKq/ATQgghhFQSqmyXU8bNTZjm2gFrb/JDPgVjdHcfDXMYo/vi7mjND/4MVebxqcy0NUnicU+M6DENgUn8kE+glROWdzMGLEdjiI0xP7Ra0KzzqrLLFAESTwgx5Cyw4JcCeLaS8CMAYDB3KMCv8xieHdLGjLPa/AiEEEIIIRqNKtvllJdwHb9fjuWv/mSM+27F/axHOO3ZCTX5gZ+hyjw+lZm2JkmJ80PQzXT+6k+jpjVmnHmErOit6NeIH1g9aNp5VallSvwXmLgR6Di0CNOUVrSlGMx7F2GTI3B4pTYOv+KHE0IIIYRoLqpsE0II+Yi0cWq/ADdFDAvGFqpQkS+C02QJ+kIA7/1fUHNyQgghhFQZVNkui5trIRaLIRaL0XhcEADAp8e/f/9vWYsbMpukHB4BsXjEv01xU8KxaUpPfCMWQ9y4A8ZvvIEMmbgAgIxEhOzwxIge3xSn9w16jvaE32Vl/VRvYK3c/xZDvFr2v8v6N+6Iwyny+RB/g55T/BCtkBEAyEDsqbWYJs1L4w4YtCAQsUri3lj9v9+e8SAQXoM7oLFYDLH1IHgdjq3wA3LesxD4zR6EDo3//Z3f9BgBr/3R8vuvHMcHAFJuBmLtlD5c2o1t+mDa6iD531nOtPEmGoGrp6GntUzaG0OQqLSP6qf37/n6b157LgOAIIxvxvudowMhezaW+din3EDg6mnoY9P43/Qad0CfKWsR9EDxxJLNz79L8bWkzM21XHjK5U3/O2+te2LaDt65IpURiyC54/NvnhXjFl9rq2/8e10c9sKg4vx/M9ALgXEKv1I15TmvkgIxQnotF6YgfKM0/43RYdwm3HgjGxkAMpAY6gfP0dJr/t/rx3NHOFIK+XEru0wBEK+NX8IApykSdBPxA0tgVIQJA4GUE1o4XNLxJ4QQQgjRMFTZ/kjSr65FH7s+8Np/A4kAkBKLwAU9MWBLtEysFAT++A0GzfVD0M3E4nWJuHHcD56udvC6WM4HehlBh6ZhkGw+kIgb+z3R9Uf5ChSQgpC5A9BhuA9+l+YlJRYhG8ejQ/vxCFL6wJuLhOPTYGczHpvOxv6b3pMQbBrnCq+zJT15q+DuJri2HATPXSGILc5k4s0gbJrSFX4V7d96cy0a9xgPn/3hXNopD8Lx+7IR6DDu9+J9VE5vQuBp0xXjl/2OG0/+XZXyIBy/LxiE+X8oe3miqhQEjuZXyPiLkop/pVL12N/A2sY9MX7Z7wh/IN3hsQjf74MRNuPxe/F+Kr8gBM4eBDtXr/+dt09u4Pe5XTH1MG+fJ4XA060DRsgdnxBsGtcB7ScF8a6HYgUJCJpihw7jNiGkOP+JoZsw/jsvhChUcitZRjjW9rdDnwXS/Kcg9rAXerpvglypcngqvhnoCb/j0mv+3+vHb24f2C0OV3wZUkaqlyn/untdgLsA+rRTqOmXQoJuPRiMAZy7S323CSGEEFI1UGW7LDrORVZWFrKysvBoTz8AwMKL//79v2UuOvG3QxA8x/kgoctynI55/W+8v3djsDFw42C43INxTbPRWHf0Fh4lS9N7jUdH56ITUrDpVDjvi1snzOX+7wUslAsrQWgIQnRHY2vk83+3SziNua0BHA+B7CDPGWfXYNqOG+jkGYBbCcX/4+1z3DowF51SAjHnV2UP6WsxfnQILKbvRkTxNs9DF6ITUuAX+qeSr4WqiT63CTfQHevCi/ddVhayku/jwvYZqC373F2u41MD/RbsxoW/n+O1NE5CBLYOMwZC/XBSOgxzOdJOuRwIvxRg0gGZtF8/x62jy9Hmw21nPwlj9wDu91xYAAD9sDuG9zt/HwzFIcpUP/Y13BZi98X7eP66OL23zxGxeTCMEQK/P2SvBvn8SPf7h4ScDUHNkVv/l4+Tc2ENIOjcdZnKXwZCfpkGv5udMPfALTx/Kz32txDg2QkpB+dg52XFMxyrx2PEOQvM2BNRvM1zXFjQCUjxQ+iNcpzh5TivpIJmj4fPcxssP/Po3/Pr7X3sHmYM3AxEuOzw4UILjF57BLce/e/6eR1zBHM7AikbgxAul+3KK1P+pY3E+wBEDNZl7XvflMEdwKmHWkrKHkIIIYQQzUOV7Y/EeVkEIn+fge5mxbWsRt3RvweAu/nI52IZo9/KrZjkZAVjXem6mjB2coELALzJqPhDZsdJOHJqK0a3rv3v34bd0X+YNYA85HEfmjLwZ6gfUlovx/rF/WBlWLxaWBtWfX/CwplAyv4bUBzKyRqTfr+A0ysHw7p4m9qdneBiXLG81zFuAyAGoWHX/9fsVdccnUYux6R2vMhl1XEGAv5vMDo1qv2/vqOG1nDuZQMgGvn/OzhlVtvQHMYA7lwOQaK0QlOzNqycZmBuX8XqquqMMfh3foWMvyivoFUeVY99J8z4fS4GdzRHbekOF9aGdS9n2ACIzq7ADi/WafIRnNk++n/56NEfg1sDyJM5AzP+ROiuFFgvW4+Ffa1QW1i83tAK/X5eiBlIwe83Fc9wtJ6EgIunsdzdunib2ujUywXGAJLflfcMLyen5Yi4HIAZ3Yz/PXeF5ujeu7vCeWvsthxbJzvDyvh/b3hqmjnDxRkAkpHx7n9xy0WlMkVKgLi/AFj/+5W6TGowGJsASEa5X9wRQgghhHxMVNn+SGy6WKP4UbSYMfr5KakUFffx5fqzisUQi3vCRzZORTgPgTPvi5KJsbn8CiTg7ikAd73QVaF5cn302QggJQaJCl+tzGHTiZ+WNeY+KulrqGrMR65DwMiaCFncB43rfoOeU7zwe2gsMhQe5MsjT6E/q1im/2xF1Oz2Ew4s7o7EHePRoWFjdBjsiU2HbyDlI9fJPg7Vj/2//e9HcP2kxWIxxM3Go+J7/F8ug50hnxMTmHwttwJ4fBcnAUQv6KrYBL9+H2wCkBKbqNgM+msb2PC/yLabi0dZWQhwL+8ZXk6dbGEtX6jA2G03srKyMLej7NoMRB9ei2muxX3pi5d/++SrgUplioxc/gpVMRhb8NcRQgghhGguqmxrkqQgjC/u48v1Z/0k8qFYy/iUzNFv+308jz6N3cucUTfqv5g2sAMaNuuDtTcrUnPNQ/Q6V4X+rOpTE508T+PRowgEbB8Cm9QQeI3ricb1v8G04xX5b5rYZ1s1eVFri/vfB3H9pD+JIg07xStNCoImtUfXcT74/XJxX/oqLE9DZqIjhBBCCFEFVbY1SOI5PwSmAJ08j+A+12e7DH0n1cYc5m4AemzFfYXmydJlN/p95A95tS27Y/DMdThy/RGeX9+KwQiHzyw/uT7vZZJ3HYGLbwDGg7E1UqZfdRn6B6vE2Br9Ri7H1ov3kfXoCOZ2TMTvozch5LNrC5uH68d8cAPGGLw54n99trOykBWzG2rc4x/WwBz9AHTffF/JuV28+PUrd2sMjfEkBH4HU4COc3Hkb5kxD7g++R8bQyNbANGCslf887UQFwOgEar+cSGEEELIZ4Eq2+VUU88EALDz198RraZRiNPfxAAATBo34/ps5z27gcDVG7FXPmolM4aNkzNw0QfTlgUh9pO2e05B0AJP+IXGIkVmuqza5jbo3oXf5/1/VDo+GRlIBgBjc5ibFffZzstAbKgffH67zo/NUSltANFbxmPt4RtIzJDZf7Vt0b2HdQX7yn6cPts1dK0BBGHnryFqavqegYxXAGAC80YWxX2285ARFwK/Vb+j5D1eCcxs4OwEhC+bBp9T8ueWWmXfwFrXxhA37gOvs6W3ZlD1vCqT9BTEAEBDCzQzKu6znZ2IG4fXYuMBXtyPoghWrQHkChD9jB/2AU8FuAyg79cSfgghhBBCiEaiynY51e7khEnGQMrBaehqoZ7mu1bdPP4dNXnKN6hfnF79lj0xflmQ0ibO8nMQF/frXtbzf+t48yGXhfkgbyzvBoSvHoEOjesrNlMuce5d9ct75gfPgR3Q2ETm/5t0wLTjQKfFTmjN30DV42NsA2c3AHfXoo80Tv2G6DDQE78rndf8XyqlDSA/OxA+43rim4Yy+69+Q/RZHQ3jYUPQ3Uwmsgay7jYYnQDcWDcIjevL/M5yn1fGsOnVD0A01vZvWJxefTRsPwiev4UrSVN+zud/+9HLz/u9ttxTv5ljsNdydEc41g7nnVsVTvt/Uv7YCJ/LxfNQH5QdDV2RqudVmbToDo/WAI5PwzfSY2jyDXqO80GQkmb8lVmmSLXuwNAawK4w6ah0qhAUTxnG4N6liB9ICCGEEKKRqLJdXobOWP5HABaO7A4rNbVprNlxLgKOLsTojsWDCxlbwXnCOhyJvo+ACfzYlaymNWacjMTptZPQT5qfT8IYg9dewO4Fo9G9hXRHm6OT2ySsO/MIFzyt/zeKuCyVjs+/zZm3TnDm4ph3HI2Fey7gUfhyWPOjS6mUNtBp8n0ckdt/xrDqNhoLD9zC7arQRLn1DBwLXYdJbp14A46Vn7H7NkRsnwRn6bG07ITRC3bjwqMILFf21qQS1Ww3A6cjT2PdhH7oZMkPVQ/j72ZiuZM5YNwdM4bZlH7MVTyvyqRmJ8w9eAQLR0qPoTGsek/CuqP3cf/AJH7sj6NZEabbAncPaOGyqoOlvRLil92A8UCGXrxB4QghhBBCNJWAMcb4K9UhJSUFurrc/FWEEELIv+J14DRegPzBEoRML1D+woyjjVAfIYZEMvy+7z36GvHDCSGEEEI0E33ZJoQQ8nE1KcQvPwB3A7Uw46w2P1SGAIlntTHjPOA+p4gq2oQQQgipUqiyTT6epECM4PWLLW1RR59ZQogmkqD1mEL4dQEOrxRi3T1ltyIBMv78AhNXCtBoaBE2OVNfbUIIIYRULcqecAghhJDKpV0E9+WF+N2RoZGBst5MDDV0AZOhEhz2KPxAU3NCCCGEEM1DfbYJIYQQQgghhBA1oy/bhBBCCCGEEEKImlFlmxBCCCGEEEIIUTOqbBNCCCGEEEIIIWpGlW1CCCGEEEIIIUTNqLJNCCGEEEIIIYSoGVW2CSGEEEIIIYQQNaPKNiGEEEIIIYQQomZU2SaEEEIIIYQQQtSMKtuEEEIIIYQQQoiaUWWbEEIIIYQQQghRM6psE0IIIYQQQgghakaVbUIIIYQQQgghRM2osk0IIYQQQgghhKgZVbYJIYQQQgghhBA1EzDGGH+lOqSkpEBXV5e/+qPIffUSr65fQfrDaGQnPYOuWSPoNfoaJl17QtesESAQ8DchhBBCCCGEEELUplpVtt89e4I7K+ch9a/r0P3SAvXa2kJbRwcF2Vl4fesqcl78g3rtO6PdgrXQa2TJ35wQQgghhBBCCFGL6lHZZgyJQYcQvX4xvhowCs3G/ogv9GsrxMl8Go87q+YhPfY+Oi7dDNNve9FXbkIIIYQQQgghalf1K9uM4fF//fFw5y9ov2TDByvQksICxPy6AY8O+KHTiu0w+daRH6VS/Pbbb2jbti2sra35QYiOjsbu3bu5v7/77ju4uLjIxalMeXl5+O2339C7d2+Ym5vzgzl5eXnYsWMH0tPTMWvWLNSpU4cfhZOYmIitW7ciLy8PAGBgYPDBbQghhBBCCCGkulA6QFp+fj4OHz6MjRs3YuPGjTh8+DDy8/P50TRCesw9xO3divaLfWHazanUijYAaAm/QLOJs2DRfxge7FyLvNQUfhS1Sk9Px+LFixEVFcUPAgAEBwcjICAAnp6e2LRpE3x8fPDnn38iODiYH7VSJCYmYuHChXjy5Ak/SEFsbKxK8RITE7Fnzx5MmzYNmzZtwpo1a1CnTh1s2LAB6enp/OiEEEIIIYQQUu1o161bd/H169dx584dNGzYEHp6ehAKhWjZsiVsbW2hpaWFjIwMWFlZQSgU8rcvUXZ2NnR0dPir1aooPw/3Ny6Dnrklmoz0gEBLG4W5OSh4lwmhqJZ8ZMaQk/ICX+iKIdDWhvirJnh6bD+0hEIYtukkH1cN8vLysGXLFhw7dgy5ubkAgHbt2sHExISLk56ejiNHjmDw4MGwsrICANSsWRN169bF+fPn0aZNG9SsWZOLr07p6elYtWoVLl68iMLCQgiFQnTq1KnEL895eXk4cuQI3r59C5FIBFtb2xLzlp+fj44dO8LMzAwAIBQKYWpqiqtXr6JRo0Zy+4AQQgghhBBCqiOtmTNnwtXVFTo6OtDT0+OHa7TMx7F4ffsqzPu4Q0v4BQAg4fgBhI//HpmPY+TiJkdcwOXx3yPt3m0AQC3ThjDvOwRJF87gfab6v7bm5eUhPT0d48ePh4+PDwwMDPhRkJiYiJycHIUKrrQpd2Jiotx6dcrIyABjDJ6envD09IRIJOJHkRMbGwsA6NGjBz9IgbGxMYyNjeXW1a5dG7Vq1cLLly/l1hNCCCGEEEJIdaSVn5+Pv/76C998802Vq2y/ib6FWiZmqN20JbfOwm046rfvjFveM5Dz8jkAIPnKedxYMA1Nx0yDQav2XNx67WyRnfQM2c+ecuvUpU6dOli8eLHSPtqqklZMpU3Ro6Oj8dtvv2HGjBmYMWMGNmzYgLy8PERHR3PrZsyYgejoaH5SCszNzbFkyZJS+2hLpaen4/jx43BwcCjxa3Z5SH9XWfNOCCGEEEIIIZpO659//sGbN2/QqFEjfpjGy3oaD/2vmkCo+7+XBEJRLbSatRC1m7bAn57j8CTQH7cWzkDLKf8Hy0E/yPXprmXaEF/o6iE/PY1b9zFJv2jz+zFnZGQgJydHbh0A7N69G23btsWmTZvg6emJFy9ewNfXF2FhYVizZg02bdqE7777DgEBAWr9Kn7ixAlYWFhU6MWB9Ct+8+bNgeLfvGHDBtja2mLTpk3YtGkTxo8fz9+MEEIIIYQQQqokrfj4eJibm1fZfrRaNUTQ0pbvSy7U1YP13GWoYVgfd1Z7wWrcDFgOHqMweJq2Tg1ofaGDd/8kyK3/WMzNzdGiRQu5ynFeXh7Onj2rtFl3u3btuAqvdNu3b9/Czc2N++Jsa2uLWrVq4eHDh7ytyyc6OhoJCQn4/vvv+UEqS0xMREBAAHr06MF9SZc2Y5dWvgHA2tq6QhV6QgghhBBCCNEUWi9evEDbtm3566sMJikClMxelh57H5mPYqAtqoXn54KQ9zqZH4UjMpLvX/wxjRkzBi1atMC6deswY8YMLFy4EPb29qhbty4/qsJxMjExQa1atVC7Nm9OcTWRVpLd3NwU+pWrKjg4GOvWrUOLFi3kpjOrXbs2BAIB/P39Fb7sE0IIIYQQQkhVp9WgQYMq+1W7dtPmyIj7G++zMuTWp/19Bze9puHroePQ+8xN1DCsj5sLpitM85WbmoKi/DzUMv531OxPZcyYMVxT6jVr1sDExATp6ekwNTXlR/1o8vLycPz4cbRo0aLcX5t/++03/PHHHxg/fjzGjBkjF1anTh38/PPPqFOnDhYuXEj9tQkhhBBCCCHVilaTJk3466qMui3aICc5CRlxf3Pr0mPu4fp/xqOhUz80HjEJOrXror33OmjXFCFq2VwUZGVycV/9GQ6dOnVRy0yz+qtLm5SrMnhZZZHOqR0VFSU3gNkff/yBtLQ0LFy4EL/99ht/M6C4or5hwwY8ePAAnp6eJVbWa9asiVmzZnF9zXfv3v3R5hcnhBBCCCGEkMqk9eWXX/LXVRn6lk1Rt3lrxO/bgaK8f+eyfp+eBvO+Q9Dyx/ncdGA1DOqh3YK1qFG7Lgre/VvZzn31Ev8En0DDXv1Qo66hXLqfUl5eHsLCwmBhYVHuptvqYG1tzX1tl12+++47GBgYwMfHR+FrtVRsbCxevHiBadOmqfzCwMXFBe3atUNycsnN/QkhhBBCCCGkqtCqUaMGf12VoV1TBMsh45Aa9ScSThwEGIORbXe0mPoTV9GWEhk3QPslG1HLtCEkhQWI/307BNpaaOQ6SC7ex3by5Em5PsuHDh1Cenp6hQYk+9iCg4Px008/cV/k//rrL7Ro0aLUinZiYiIuXbrE/Z2eno6EhIQq26WBEEIIIYQQQmRp8VdUNSad7dFy2jzc37QM8ft3QlJYwI8ipyg/Dw/91uOfkBNoNWshatb7dIOjAcDbt2+5PsszZswAACxevPiTftVWB37zc+myePFi7uXCH3/8wa1fuHAhbG1t5QZRI4QQQgghhJCqSsCYkqG8Zdy4cQOJiYno168fyvIVPCUlBbq6uvzVlUJSWIBHAX74e+sqGLbphDY/r4T+V03kp/piDJlP43Fn1TxkPo5Fp+VbYWTbXTYZQgghhBBCCCFELZRWtvPz8xEUFIQXL14AABo0aKDRlW3g38r0m+ibuLNyHjIePUStBl+ifocu+EJXjILsLLy6Fo68tNf40qk/Ws5cAFF9aq5MCCGEEEIIIaRyKK1sq8NHr2wXYxIJMh89RMrVMGTEP0Dem1TUbtwMdVq2gbFtd40aDI0QQgghhBBCSPVU7SrbhBBCCCGEEELIp1blB0gjhBBCCCGEEEI0DVW2CSGEEEIIIYQQNaPKNiGEEEIIIYQQomZU2SaEEEIIIYQQQtSMKtuEEEIIIYQQQoiaUWWbEEIIIYQQQghRs0qd+osQQgghhBBCCPkcVVplmxBCCCGEEEII+VxRM3JCCCGEEEIIIUTNqLJNCCGEEEIIIYSoGVW2CSGEEEIIIYQQNaPKNiGEEEIIIYQQomZU2SaEEEIIIYQQQtSMKtuEEEIIIYQQQoiaUWWbEEIIIYQQQghRM6psE0IIIYQQQgghakaVbUIIIYQQQgghRM2osk0IIYQQQgghhKgZVbYJIYQQQgghhBA1o8o2IYQQQgghhBCiZlTZJoQQQgghhBBC1Iwq24QQQgghhBBCiJpRZZsQQgghhBBCCFEzqmwTQgghhBBCCCFqRpVtQgghhBBCCCFEzaiyTQghhBCigZYuXYrJkycjNzeXH0QIIaQKoMo2IYQQQgghhBCiZp99ZbuwsBAbN25EcHAwGGP8YAXZ2dlYvHgxbt++zQ+qshhjCAwMxN69e1FYWMgPVlAd9wEhhFQHhw8fRoMGDbB27VqVyvPPQWRkJJo1a4ZZs2YhOzubH0wIIYRUms+6sl1YWIgdO3Zg9erVSE9P5wcryM7OxooVK3Do0KFqdcMuKirCq1ev8NNPPyEwMPCDLx1ycnLw5MkTTJs2DQ8fPuQHEzULCAiAQCDgljZt2iA2NpYfrdJFRkZCIBAgMjKSH0TKgTGG8PBwDB48GIaGhtzxXbp0KT8qKUFubi527NgBe3t7uWukup2jaWlpWLVqFdq1a1dqOZCbm4sLFy7g5cuXuHDhAjIyMuTCP1cXL15EbGwsLl26hOfPn/ODCSEqevPmDVxcXKhrAyFl8FlXti9evIilS5fil19+wZAhQyAQCPhROIwxHDhwAAEBAdi5cye6devGj1JlCYVCTJ48GXPmzMH8+fNx+fJlfhQ59evXx9q1a2FiYoJ58+YhJSWFH6VKk95MSqr0BAQEKH3QraqSk5MxZcoUrF69mh9UbWRnZ2P79u2wtbWFQCBAs2bNsGrVKrx+/Zof9aM5f/48BgwYgMOHDyMtLY0f/FG8e/cO3t7emD17dpV7cCosLMSqVaswZcoUhIeH84OrjczMTPz000+YN28e/vrrL36wHJFIhJ49e8LU1BQ9e/ZE7dq1+VE+Ck07r3r06AErKyvY29ujYcOG/ODPhvSFqb6+Pi5cuCAXlpubi8mTJ2t0JUoikeDatWuYMGECLCwsuLJ82bJlePbsGRdP+jtlFwsLCwwZMgRhYWGQSCRc3M/tfv+5yM3NxZkzZ9CtWzc6fuST+2wr2wkJCfDx8cGgQYPQv3//UivaAHD79m0sX74cU6ZMgZ2dHT+4yhMKhZg0aRI6deqE9evX49WrV/wocoyNjbFgwQLcuXMHe/bsQVFRET8KUZMRI0aAMQbGGPbv388PrrCMjAxcu3YN79+/5wdVC0+ePMGAAQMwdepUXL9+HQAQGxuLefPmwcPD45NUuHNzc3H06FEYGxsjLCwMBQUF3DH29vbmR680+fn5uHnzpsY+XJfm8ePHOHnyJIYOHYrExERIJBJuH1anMvrevXsIDAyEp6cnUlNTud94584dWFlZ8aPD3d0dL168wNy5cyEUCvnBH4WmnVd2dnaIiYnBhg0boKuryw/+7GRlZWH//v1VqoVednY2vL290aVLF+zevRuJiYlAcVnu7e2NFStWlHq+JSYmIjAwED169KAuFtVYdnY29u7dCxsbG/Tp0wdXrlzhRyHko/ssK9uMMRw7dgxpaWnw8PD44M03Pz8f+/btg6WlJUaOHPnJHmAqW926dTF9+nTcvHkTJ0+e/GBz8rZt28LDwwP79u3D33//zQ8mRCMYGxujQ4cOOH36NHJyciCRSBAXFwdXV1ccP34c165d429S6XJycpCQkAAXFxd06dKl2pYplSk1NRXR0dEYNmwYGjVq9MEXplVVQkIC9PT0MGzYMBgaGvKDCSmz0aNH49q1a1Xmvl1UVIRt27ZhxYoV6NixI06fPo13796BMYb8/HxcuXIFzZo142+GiIgI7uVUTk4Ozp49C2trayxfvpx78Uqql5CQEIwZMwbv37/Hzp07MWrUKH4UQj66z7Ky/ezZMxw9ehSDBg1Cy5Yt+cEK7t+/j8OHD2PEiBEwMzPjB1cr7du3h7OzMwIDA5GcnMwPlqOtrY1+/foBAI4cOfLZft2WNkMLCAhAQkICJk2aBENDQxgaGmLKlClK92Nubi4OHjwIR0dHrombh4eHWpo68ZtMS9NOSEjg4kjzLG2GFx0djYULFyo0vSup72tUVBQGDBgAgUCA1q1bw9/fX2O/FOjq6mL58uVwdXWFSCSCQCBAkyZNMHPmTABAdHQ0Fzc7OxvTpk2Dvr4+QkNDZVIBUlJS8P3338PW1lZtYxXUrVsXOjo6/NWc/Px8BAcHy/Xrtre3x/bt25V+lVLlvJI2FxUIBKhXrx5CQkKwc+dO1KpVS+7YBwQEyKUtkUgQFhYGNzc36Ovrw9DQEJMmTZI7r6Rkm17KXhMWFhbYuHFjqV+gyqq0CqhsE9H8/HwcPHiQuy4GDBiAO3fu8DdBfn4+jh49KrcPFyxYUGILiISEBMyZM4dr1uro6Ijz58/LNVWtKCMjI+jp6fFXc2SvZ+lSUrNY6fFfunQpXr9+jQULFsDCwgL6+voYPHgwYmJi+JuodOzLc16VNA6EsibNZc23bH6kS2lNpAMCAuDi4oIXL17InSe2trY4duyYwvFkjCEqKgrDhw+XG3NBumhy09XOnTujc+fOCAwMRH5+Pj+YU5bjI52i7O7du+jbty8MDQ1x6NAhJCcnY+TIkTA0NMTOnTvL9ZwQExMDf39/2NjYYO/evXB1deU+kujo6KBr166YNWsWRCIRf1OOSCSCi4sL5syZg6ysLKXlVlnExsbKXff29vbYu3dvieeXKkpr0l5Sc3ZVynxZr1+/xqpVq9CsWTPu/D548KDS80B63cvef7799lvcvHmTH1VjtGvXDitXrsSVK1cwatQo1KpVix+FkI+PaRgfHx8GoMTF2tqaxcTEMMYYi4mJYdbW1gpxZBcfHx/+v2AnT55kAFhoaCg/SKn169ezJk2asOjoaH4QJyIiQuF/85f9+/czxhjLyclhHh4eCuGyi7OzM0tNTa30tJXZv38/E4vFLCIigh+kQPr/XFxc2OvXr+XCynp8ynLsK1NqaipzdnZWeu6w4v0jmxdp/G+//Vbp7/Xw8GA5OTnc9nl5eczT01MhnrK4fPz/zZecnMz69++vkC4AZm5uzi5dusSYTJ75cfiL7DkgPQ9/+OEHZmBgIBdPLBazEydOyORE84WGhjIAbO/evXLrnz59yuzs7JidnR17+vQpY4yxgoICtmrVqgr9TlWuY/6x3b9/v0Ic6TJ//nxWUFDAxVX1vFKljIBMmcIYY4WFhWzbtm1MLBYrxDM3N1coK6Tn6datW5mVlZXCNhs2bGASiURuG1V8qIwA77dKz/M5c+awmTNnKsSVPcaseN/Mnz9fIR4AZmNjw+Li4mRyw9iNGzeUXvMA2Jo1a1hhYaFcfFWoenxkyydl13NJ5Zc0/Y4dOzJbW1uFdPv168dSUlK4+Koee1XzLXteSa8J/vkjTUvZeatqvpXlp7Tydf/+/czU1JS5uLgopG1qasrCw8Pl4l+6dImZm5srxJUu/Gu5vHx8fErNd1lI9/f+/fvZ0aNHWfPmzdn9+/cZK2Gfl+X4+Pj4MAcHB9avXz9uHzg5ObGRI0dyf3fp0oUlJCTIpaWKHTt2MABs165d/CClSso3k0nr0KFDjJXjfs8YYzdv3izx2Cv7n6oqLS/K8qFqmS/1+PFj5uTkpBAXAJs+fbpc/IKCArZmzRql131J6avqQ+U4/3eWl/Q8VVd6hJTXZ/ll+++//0b79u3x1Vdf8YMU5OXl4dGjR2jatCm+/PJLfnC11KpVK5iYmOD+/fv8IAUikQht27bF48eP8eLFC37wZ+XKlSvQ09PDn3/+iaKiIiQkJMDV1RW3bt1CUlISF++ff/7BpUuX4OnpyTWFKygowN27d/HNN9/IpVkWRUVF2LNnD06ePIlFixYhLS0NjDG8e/cOfn5+SEtLw/bt25GZmQlDQ0NuuruYmBhYW1vDx8eHa3JXWt/XvXv3YuDAgXjx4gWKiooQEhICPT09hIaGIi8vjx9dIxUWFuLSpUto3rw52rdvLxdmYWGB5cuX4/nz51i3bh1yc3Nx+fJlbNy4EV5eXnB1dZWLX5lq1qyJTZs2ISkpCUVFRXJN4C9dulSu80okEmHHjh1gjCE1NRXOzs7w8PBATk6O3LEfMWIEt81ff/2F1atXw93dHU+fPoVEIkFBQQHCwsJgYmKCXbt2KXxpj46OxrRp02BjY4PHjx9DIpHg3r17sLGxQUREBDIzM+XiV6b169cjODgY+/btQ05ODvLy8rBixQpERkbi7t27XLzQ0FBs3rwZXl5eSElJ4ZqfHj58GMnJydi3bx/3Ze7t27dYu3YtatasyfW7l0gkePr0KcaNGwd/f3+Fr62VRfZ6lh7TD7l58ybevn2L06dPo6CgAGlpaZgwYQLCwsLw5MkTLp6qx74851V5qJpv2fzk5OTAw8NDLh1lXr58icjISPj4+CAjIwMFBQXw9/fn1ktlZ2fjt99+Q506dRASEoKCggIUFBTg6tWrsLGxwfTp03Ht2jWlfeo1RdeuXdGkSRMEBQWV62uzMmFhYdxX40WLFiE0NBTx8fF48OABtm3bhnv37uHly5f8zUpVVFSExMREmJqaok2bNtz6pUuXqtxqAcXH7NSpU9i+fTtsbGzQqVMnfhSVnTt3Djo6Orh27RqKiorAGENaWhr27NmDmjVr8qNXGlXLfBS32Nm2bRtiY2Nx+PBh7rpMSUmBl5cX9u7di6tXr3Lxo6Ki4OvrC0dHR8TExHD3n6dPn6Jnz55yaRNCSqdxlW1vb2+5GzN/kR0UxsrKCnfu3FGII7vwBxvKy8vD8+fPUa9ePdStW1cuTJnc3FwkJiaiYcOGpRaidnZ2Cv+bv0gfMmQfAkpagoODueaRlZm2MrVr10b9+vU/OEialKmpKeLj45GVlSW3vqzHpyzHXhM5OTlxTd20tLRgbm6Onj17orCwUO5hRldXF3Xr1sW9e/cQGxsLxhiEQiFatWqF6dOnl9oUrjQpKSkIDQ3F6NGjMWfOHO781tXVxdixY/Hjjz8iKiqqws3nZs6cCV9fX5iamkJLSws2Njaws7PjHjrKS9kIsqo8RJUVK55XfteuXViwYAFatGjBj4Ju3bph2bJl2Lt3LzZu3Ihly5bBzc0N06dPL3f/atnrWFoZ4b/g4J/jAwcOxI8//ogGDRpAS0sLguIm8DY2NsjOzpZ7uVFZ5xWKXySZmJhwTXcFAgGEQiHs7e0xduzYEh+ifX194efnB0tLSwgEAjRu3Bjt27dHVlZWubodyJYRERERAK9fJmMMO3bsUPitDg4OOH78OEaOHAmRSIQaNWrAwcEBYrGYK7fev3+P8PBwODo6wtPTE0ZGRkBxmerm5obhw4cjKiqKe0nw5MkTRERE4KeffoK9vT2EQiHXjHPmzJkoLCxU6YUlH78M379/P6ytrRETEyP3O/n3trKytraGv78/XF1dIRQKUbduXfTt2xdZWVly5VV5j31lUTXf5SEWi7FixQrMmzcP+vr6EAqFcHBwQJcuXeTO17y8PLx8+RL29vbo3r07hEIhhEIhbGxs0KtXL8THxyMnJ0cubU1jZGSEfv364eTJk3IjeVfU+PHjYW5ujiZNmnB/N2vWDPr6+uU6Ru/fv0daWtoHu1Io07VrV+4+oqenh379+iE9PR0//fQTLCws+NFVZmRkhOTkZERERHD3prp162Ls2LEKL28rU1nK/OTkZFy7dg0zZszAwIEDuTAjIyNMnToV7du3x40bN4Die2RoaCjq1KmDpUuXwsrKirv/iMXict8Dpar6sx4hZaVxle3KxhgrU2EvkUjK9VBYlenp6UEsFvNXl6i0ivvnRPqlQJb0gV2WqakpvL29kZiYiPbt26Nz585Ys2YNEhISwCpQWc3KysLbt2/Rpk0b6Ovry4UJhUK0aNFC6UuRsnJ3d5cbVFBHR6fKnAOs+GXT/PnzMWfOHAwePBgCJQNrCQQCuLm5YdSoUZg3bx5ycnIwffr0Dw6mqG7S/ni9e/eGvr4+9+C4cOFCftRKO6+krXuuX7/OVZr5L0Pi4uKQmprK3xQdO3ZEjRo1uL9r1qyJrVu3fvCFn7o1bdpU4eHa1tYWmZmZ3IvK7OxsxMXF4fjx4zAwMJD7jUKhECtXrkRycjLevHkDAHj06BFevnyJgQMHKuwTa2trxMfHy31p1TS2trZyXwqhpCyvyLGvLKrku7wsLS3Rq1cvucqEsvuhUCiEWCzGpUuX8Oeff6KwsBCFhYW4cuUKTp06BSMjo1LHYtAULi4uqFWrFoKCghT6pJeHtbU1OnToIPd3t27dIFBSxqpKIBBAW1sbOTk5cv2KpRU2VVtytG3bFkuXLsXly5e58UbKa+jQoRg3bhzmzp2LRo0aYfTo0QgNDVXrS2FVlKXMf/nyJe7duwdPT0+u4ixdzMzMuDno8/LyuI9SlpaWaNCggVw6hJCy07jKNr9pEH+RHSAiNjYWbdq0UYgjuygbaKIylPRVTnaRDgyjbOAW/uLi4sI91FVm2srk5+crHSyjJCW9wS/r8SnLsf8Y3r59q3Q6rLK8rClJt27dcPv2bZw+fRpNmzbF6tWr8dVXX2H27NkKzXHLqqoOCFJSCw5lXyrLQyKR4ODBgxg/fjymTp2KWbNmffANfUFBAVD8VUDVlh7qkpubi59++gnDhw9HcHCwSi9JKuO8KusLyqqqrC9WyxK3qvpcjn1Z1a5dG5MnT+a+bn/xxRf44osvYG9vj/T0dIwfP16hgq6JGjRogD59+uD06dNISUnhB2uEmjVromHDhoiPj8ejR4/4waWSbfUSFRWFBQsWoFGjRvxoQBnv97q6uvD19UVMTAz+7//+D3FxcXB2dkbbtm1x+fJlfvRKpWqZX1RUpNI9RJZIJIKWlvqrCZr2rEdIZVP/VaThRCIRzMzMkJqairdv3/KDFdSqVQsWFhbcG7/PQWpqKpKSkmBubs4PUurJkydo1aqV2r4uaIqEhASFCkp+fj5u374NoVAIbW1tubCy0tXVhaurK37//Xc8e/YMixYtwsaNG3HhwgV+VJXUrFkTurq6+PPPPxXyXVhYiAcPHsDa2hr16tWTC/sc5Ofn45dffsHs2bO5r9qyX1z5CgsLsWXLFoSGhiIkJASdOnXC2rVrlY4sX1ni4+MREhICV1dXrs+c9MHRx8eHH52j7vNKWmba29sjKSlJ4WUIYwyZmZmwtbXlb1qlSMv6UaNGITMzU+E3MsZw69YtNG7cGCju24/ift78eNJl3rx5vP9StXysY//8+XO5vx88eKB0pHhNkZubixMnTqBx48b4/vvvYWBgAAMDAwwbNgzHjh1Dt27d+JtoJIFAAFdXV7x+/VphxHFZn/r4dOrUCWKxGAcPHkRaWho/WC3Ker8XCASwsrLCTz/9hGvXruHGjRuoU6cONm/ejIyMDLm4ZZWcnCz3vJmWlobw8HC5OLJUKfPr1asHa2vrUrsZbt26FTVr1oS2tjZq1aqF1NRUhX1y//79jzYWBSHVhcZVtsvSl6OsfYKlrK2tcfv2bTx9+pQfpEAkEsHKygpxcXH4559/+MGckr7KyS7l7VddmWkr8+jRIyQnJ+Prr7/mBynIz8/HgwcPYGZmplCJK+vxKcuxr0xisRgtWrTA+fPnsWXLFq6PZnZ2Nn7//Xf4+/ujVatWMDU15W+qkps3b2Lp0qV4/Pgx93WsRo0a6Ny5M0xNTcv89lnKxMQEHTp0wNGjR7F+/XruZVJmZiY2bNiAzZs3o3v37grNaaWV9DNnzuDPP/9US3NCTZKVlYXFixdj9erV8PX1xdSpU0v9os2K+3QvX74cs2fPRq9evfDTTz/h/v37WLp0qcLDR2XJyclBcnIyTE1NYWRkBC0tLbx9+xYHDhzAqVOn+NHLdV5Jm8MGBwfj/PnzJX6t7dKlC27fvo0lS5bIpV+diEQidOzYESdOnMAvv/yCFy9elHotWFpawtHREUuWLEFYWNhHb0L6sZTn2Kt6XtWsWROmpqYIDg7G69evUVhYiNDQUIwfP16j50F+9uwZrly5gjFjxmDXrl14/vw53rx5gwMHDqBdu3YQVKCJ8sfWuHFj9OvXD8eOHVNo8aYpx6djx45wc3PDkSNHMH36dNy7d487p/Lz80s8v1RR1vt9bm4uVq5cibNnz3KVaoFAAEtLS7Rq1arc41FA5rqJjIxEdHQ0JBIJnjx5gunTp+PXX3/lRy9Tmd+wYUO0bdsWvr6+OHbsWKkvBHR0dNCyZUtcunQJfn5+yM7ORm5uLvbv348ffvgBiYmJ/E3KRFOe9Qj5aPjDk38Onjx5wmxsbNiMGTNYXl4eP1jB9evXmampKVu3bl25pqupSvLy8tiMGTOYm5sbS0tL4wcrkO7L6rZvHjx4wGxsbBSmpEDxFEAPHjzg4pZ1uo7SpoDiTy+kbEof/iI71Uhp+bazs1M6/UVp04com/qLP7WJsmlgNElp02cp+53R0dGsefPmbPTo0SwjI4MxxphEImEbNmxgANiqVavkptwqj9LOGamUlBS5aXT4S0XOKynZ38VfZKdoysnJYbNmzVKII134x166z/nnijqVdD7Kku5nfv5K8urVKzZkyBCF3ydd+McrMDBQYRo86cI/PuWlrAzh+9A5rmwKLWX7RNk+LeuxZ2U4r9LS0pi7u7tCHDs7O+bq6lqhfJd2PUDJFJgl7Wdl12piYiLr3r27QpoAmJWVFVu5ciV78+aNXDrlVVlTf8mSlnngHc+yHB8fHx+5/cffnxUtE+Li4kqctoqfb2XnQ2lKu2/y7/fS85AfT7qUd8o/Vsp1Y2BgwH744QeF87O0c1xZmR8REaF0KkbpIru/pFNg8uOMHTuWubu7q+2cVLcYFaacVfW8IERdNO7L9sfQqFEj9O/fH+fOnVOpX0irVq3Qr18/nD59utpPb3X9+nUcPnwYrq6uHxytvaioCIcOHUJmZiacnZ2r1Nv8D2nevDkOHTqE2bNnc83praysMGfOHBw6dAjNmzfnb6KyDh064OzZs3B3d4eBgQFQPHjLypUrcerUKYVB1sqiefPmOHz4MObMmSPXAmTlypU4fvy40rfFNWrUwPz587Fy5Uq0bduWH/xZSUlJ4QYf8/T05AaaEwgEGD16NNzd3bF9+/ZSm1yqi5GREX755ReMHTsWBgYGEIvF+P7773H27Fn897//5Ucv13klEAgwadIkbN++Hd27d+cHc0QiEVatWoUjR47AxcWlSvRHLY/69etj9+7d2LZtW6n7Q2rQoEG4ePEiNwJzdVSeY6/qeVW3bl2sWrUKo0ePhlgshrm5Oby8vHD8+HHY2Njwo2uML7/8EqNGjQKKB2yTLTdjY2Mxb948eHp6frRWMBVlZWWFXr168Vdr1PFp0qQJjh07hoCAALnz0NbWFgsWLMDMmTNLnTGmNGW534tEIsyfPx9Lly7ljruBgQHc3d1x8eJFeHp6KjQ5V5VAIMCECROwevVqmJubQywWw93dHefOncPEiRP50ctc5tvZ2SE4OFju+aAkFhYW2LVrF9zd3SEWi2FjY4MDBw5g/fr13P8ihKhGwFgFhqmtwhISEjBy5Ei0aNECvr6+Hxxl+NatWxg0aBAmTJiAn3/+udRmqFXV27dv4eHhgcLCQuzcuRP169fnR5Ej3SfTpk3DnDlzyn2DIYQQQqqKyMhIjBgxAkuWLMGoUaPkBpFKTEzEtGnT8Pz5c/z3v//9YKXmQ5YuXYqkpCT4+vqqZaBIQgghH9dn+WUbxW/tpk+fjkOHDmHLli0f7GPTtm1bTJs2DWvWrEFgYCCq2zuKwsJC+Pn54caNG/D09PxgRTslJQXLli1Du3btMGbMGKpoE0II+SxcvHgRiYmJyMvLk+sXm5ubi4cPH+LJkycwNTUtdWwUQgghn4fPtrKN4iaAXl5eWL58+Qcr0Nra2pg6dSpGjRqF+fPnf/TpHSpTYWEh9u7di/Xr12PFihXo0qULP4qctLQ0eHt7Izk5GcuXL/9gxZwQQgipLlq3bg2xWIzJkyejTp063JRFtWrVQu/evZGTkwMPDw+FQUMJIYR8frQXL168mL/yc6GlpQVbW1vo6emhSZMmsLCwKLXfsY6ODuzt7ZGTk4MOHTrAyMiIH6VKEggEiI2NRa9evTBs2LAPzqsokUgQFRUFLy8vtGzZkh9MCCGEVFtNmzaFg4MDBAIBsrOzuVG827ZtiwkTJmDTpk0Vng5N6vLly8jKyoKLiwu++OILfjAhhBAN99n22SaEEEIIIYQQQipL6Z8wCSGEEEIIIYQQUmZU2SaEEEIIIYQQQtSMKtuEEEIIIYQQQoiaUWWbEEIIIYQQQghRM6psE0IIIYQQQgghakaVbUIIIYQQQgghRM2osk3K7c2bN3BxcUFAQAA/iBBCqrTY2Fi0adMGkZGR/CBCPjt0vyeEkPKhyjYhhBBCCCGEEKJmVNkmhBBCCCGEEELUjCrbpbhx4wYsLCxw4cIFfpCC/Px8zJw5E2PHjkV2djY/uMphjGH9+vXo1asXXr58yQ8m5JOKjIyEQCCQW6i5LyFESlpGULlA1C0gIEDu3tOmTRvExsbyoxFCCECV7dK1bNkSDg4OCAoKQn5+Pj9YTo0aNdCvXz+EhYXh77//5gdXOS9evMDp06fRu3dvmJiY8IOrNWnfNIFAgIULF6KoqEguPCAggG6uRONI+xjzX0KU9DDIGEN0dDTGjRsHQ0ND6Ovrw83NDWFhYZBIJHJxy+P169dYtWoVmjVrBoFAAFtbW+zYsQO5ubn8qJBIJAgLC4Obmxv09fVhaGiIcePG4d69e/yolZ7vylCVyxTZvCtbli5dyt+EfAKZmZnYu3cvWrduDRcXF7x584YfRWOUVlap89y6cuUKevfujWvXrvGDqpXAwEAMGDBAI8sPTSeRSHDt2jXufiIQCGBvb4/t27er5cOZsvtgSWkvXbpU4Roo7VrIz8/H0aNH4ejoCIFAAAsLC8yZMwf//PMPPyr5xKiyXQpdXV307dsXhw8fRnR0ND9YQbt27dCuXTsEBAR8sHKu6YKDg5GcnAxnZ2cIBAJ+8GfjyJEjiImJ4a8mn5idnR0YY2CMISIigh9MPqCoqAg7duzAt99+C39/f6SlpSErKwsnTpxA//79ERgYyN+kTP755x+MGTMG8+bN4x4Ar1+/jilTpuA///mP3IMGYwz+/v7o378/Tpw4gaysLKSlpcHf3x99+/ZFeHg4F7ey8/0xUJlC1OnNmzdYt24dWrdujTFjxih9QfW5evbsGYKDg9X+Em7EiBHc/Wf//v384I8uNjYWT5484a8mKoiPj8eUKVO4+wkAhIeHY+rUqfj555+VvhxW1c2bN9GrVy+F+6A60s7Pz8fq1asxaNAgrvVtYmIifH194e7ujocPH/I3IZ8QVbY/oGvXrujYsSNOnz6t8DWCr27duhg4cCDOnTuHR48e8YOrjLdv3+LMmTNwc3ODlZUVP/iz8d1338HIyAghISFgjPGDCdFI+/fv5x4EpcudO3fkrmVtbW20bNkSP/74I54+fQqJRIKcnBz4+fkBAI4ePYqMjAyZVFVXWFiIHTt24MqVK/Dx8UFGRgYkEgni4uLg6uqKffv24fz581z8e/fucZWFP//8E0VFRVxe0tLSsHXrVrx9+xao5Hx/DFW5TPHw8EBOTo7CueXt7c2PSj4Sxhj27t2L//znPzAyMsKBAwfQs2dPfjSNY2VlhTt37nDnUExMDKytreHj40PnFvmotLW1MWjQIDx+/BhFRUUoKipCTEwMXF1dceXKFTx79oy/icoaNWqEnj174vLlyygoKEBRURGuXbsGGxsbHD16FA8ePOBvAmdnZ6Smpn6wnA0PD8cvv/wCNzc3xMXFQSKRICMjA4sWLcL169exe/fuKv/RrzqhyvYHGBkZoV+/fggNDVXpouvRowfMzMyq5MOUVFRUFKKiotC/f38IhUJ+8Gfjyy+/xIABA3Ds2DEkJibygznSZpbKmvnwm4dGRkaiTZs2uH37Nry9vaGvr49Zs2YhPT0d69evh6GhISZMmFChJoDKmhYtWLAAr1+/losXHh4OCwsL/PDDD8jMzOTWM8Zw4MAB6OvrY/Xq1SgsLOTC3rx5gy1btsDe3h4CgQCGhoYYPHiwQhPepUuXYvLkybh79y769u0LQ0NDHDp0CMnJyRg5ciQMDQ2xc+dO7gVWZGQkGjRogFu3biE8PJxrTtysWTNs2rSpQm+AUUIz5UmTJiEhIYEfFQCQm5uLgwcPyu1DDw+PatVMr1u3bli+fDksLCwgEAggEokwdOhQfP/994iPj0dycjJ/E5U8fvwYZ86cwdChQzFnzhzo6+tDIBCgSZMmWLhwIfT09BAeHo7379+DMYagoCCkp6fDx8cHNjY20NLSgkgkwujRozF27FhERETIfbWprHx/DFW1TFFVbm4uJk+ejDlz5iA5OZlrPqmvr4/Bgwfjzp07/E0UyitDQ0MMHz4cUVFRSu+hrLgbwYQJE7hzwNHREefPny/xC2ZUVBQGDBgAgUCA1q1bw9/fX65ck6pq171AIEC3bt2wadMmhIWFwcnJqVrfsxMSEjBnzhzuuLdr1w4bN26UaykjO57HyJEjgeKPJtJ1AoFAaTP7Z8+eYdmyZWjXrh137CdMmIDo6Gil52Flio2Nlfud9vb22Lt3r9x9ULbf+MKFCxEdHc01VZYukydPVrh3KmvWfPDgQYWKWXnuyS9evMCyZcu4tNu1a4f169dr9AvQxo0bY8GCBbC0tISWlha0tLRgZWWF3r17QygUQltbm7+JyoyNjbFu3Tp8++23EAqF0NLSgq2tLcaOHYuXL1+Wu4VTbm4ujh07BktLSyxfvhxNmjSBQCCAvr4+Zs6cCTc3N1y7dk2j74WfHUY+6MmTJ8zGxoZt2LCBSSQSfrAciUTC1q1bx+zs7NjTp0/5wRrv3bt3bMyYMWzMmDHs3bt3/GA5qampzNnZme3fv58fVKVJf5eHhwe7d+8es7GxYbt27eLC9+/fz6ytrVlMTIxcfB8fH5lUlMeNiIhgpqambPz48QwAA8BMTU3Z7NmzmVgs5tadPHmSn5RKcnJy2Pz587l0ZBcbGxsWFxfHxZVIJGzDhg0MgNy5/eDBA2ZjY8Pc3d1ZWlqaXNoeHh4K6QJgYrGYhYSEcHF9fHyYg4MD69evHxfHycmJjRw5kvu7S5cuLCEhgbHi/QKAjRw5Um4/SJfSrj3pthEREfwgxhhjhYWFbNu2bUrTNTc3V9guLy+PeXp6KsQFwDw8PFhOTo5cfFWVtv+ki7OzM0tNTeVvqpKYmBhmbW1doesxMzOTjRo1ijk6OrJXr17xg1USGhrKwDuHJRIJe/z4MRs7diwTi8Xc75T+v0GDBrH09HQufk5ODgsICGCtWrViAD74m9SRbz7p/uSfH+VRlcsU2bx/6NyXnuMODg6se/fuCuc3/7747t07NnXqVIV4KC5TAgIC5K57iUTCAgIClF7LsvuEyZQLP/zwAzMwMFBI+8SJE1xcVoWve1nSY6Wu9GRV5v1eeq0pO9+lLl26xMzNzRX2HQDWv39/lpyczJjMcS9t4e8f6W/jxwPAmjdvzqKjo2VyIo9/PVbUzZs3S/ydsmXR/v37FcL5C/+8ffz4MXNyclKIB4BNnz5dLm5Z78lPnz5ldnZ2CvGgQvldGum5wU9TdintvCmrgoICdvXqVWZjY8NWrVrFCgoK+FEqbMeOHQwACw8Pl1vv4+OjcG4qk5SUxOzt7dns2bNZfn4+tz4jI4OtX7+eO3/Uce8i6kFftlXQqFEjODk54fTp0x98UyQQCODs7Iy0tDSVRjHXNH///TfCwsLw/fffQ1dXlx/82TE3N0f//v0RFBSEV69e8YPL5eXLlwgMDMQff/yBS5cu4eXLl9i1axd27NiB27dvo0mTJuUeZC80NBSbN2+Gl5cXUlJSwBhDTk4ODh8+jOTkZOzbt4/7miwQCDBhwgRMnToVvr6+uHr1KrKzs+Hr6wsAWLJkCerWrSuXvpmZGf744w+kp6eDMYaCggIEBQXBwMAAwcHBeP/+PRc3LCwMWVlZSEhIwKJFixAaGor4+Hg8ePAA27Ztw7179xRGut+/fz88PDzw/PlzFBUVITo6GjY2Njhx4oRCXFX99ddfWL16Ndzd3bmmxwUFBQgLC4OJiQl27dol92Xkn3/+waVLl+Dp6Yl3795xv/Pu3bv45ptv5NLWRCNHjuTecjs6OmLLli0KX3FK8vTpU9y6dQudO3eGgYEBP1glr169gqmpKczMzIDiL0Y//fQT2rRpA39/f2RlZSE5ORmpqal4//49Xr16hS+//BIikQj5+fk4c+YMunfvjhEjRnD9Tz/UH1Ed+f5YqlqZIrVz507UqlVL7suZQCBAQEAAPyrCwsKQnp6Os2fPIj8/Hzk5OVi+fDkiIyMREhLCxTtx4gS2bduGiRMn4vnz55BIJMjPz8fJkydhYGAAPz8/ucF+bt++jfnz58PS0hJBQUHIycmBRCLBy5cv4eHhofQr1N69ezFw4EC8ePECRUVFCAkJgZ6eHkJDQ5GXl8fFq+rXfXX26tUrrF+/HgBw8uRJ5OfnQyKR4Pnz55g4cSJOnjyJw4cPA7zxPKT9qSMiIuSa5AYHB8PQ0FDuf7Rp0wZXrlzhukpIu6c8f/78oz7LnTt3Djo6Orh27RqKiorAGENaWhr27NmDmjVrcvFk+437+PjA2toaMTExcr9zx44dEIlEQHELkm3btiE2NhaHDx/mfmdKSgq8vLywd+9eXL16VSYn/1L1nnz9+nVERkbixIkTKCgoAGMM7969w+nTpyEWi+XS1DTSFjkCgQBffPEFvv/+e4wdOxazZs1Se0uRzMxMXL16FY6OjmjSpAk/GCEhIahXrx4EAgGaNWuGiRMn4tq1a3KtdrKysvD27Vt8+eWX0NHRQXZ2Nvz9/dGpUyfMmTOHazVVUss98vFRZVsF2traGDJkCJKSklQa1bJZs2YYNGgQzpw5w/U1rAqKiopw+vRptGvXDt26deMHf5aEQiH69euH+Ph4tQ7ENXz4cNjb28PExATW1tZwc3NDnz59oKuri1q1ailt4vgh79+/R3h4OBwdHeHp6QkjIyMAgEgkgpubG9c0U7bJuK6uLry9vfHNN99g7dq1WLt2LUJDQ7F69Wo0b95cJvV/0/H29kbv3r1Ru3ZtoHj/dOnSBc2aNUNOTo7CuAbjx4+Hubk5d1MZP34816w0KytLIf7GjRuxcuVKmJmZQUtLC61bt8bYsWNx+/ZtlbpxKHPlyhWYmJhgwYIFXLM8oVAIe3t7jB07VqHSr6uri7p16+LevXuIjY0FYwxCoRCtWrXC9OnTuYeXshKJRNixY4fcwxB/UfYQWF5ZWVm4cOECfvzxR/Tv3/+DTWFTUlKwcOFCNG3aFNOmTVNacVGVkZERcnJysGzZMrRt2xa//PILmjVrhtDQUCxYsIAfHfr6+oiMjET//v3Rp08fxMTE4P/+7/8QFhYGa2trfnQ56sz3x1CVypTycnBwQGBgIFxcXKCjowORSIQxY8bA3t4ed+/eRV5eHrKyshASEgIHBwcsWrQIZmZmEAgE0NHRQd++feHl5YXw8HDuvJVeH+/fv8f69evRt29fiEQiCAQCmJiYYMqUKWjcuDE/K5g5cyZ8fX1hamoKLS0t2NjYwM7OjqvISFWX6746io6ORlBQELy8vNC3b1/o6OhAIBDAzMwMixYtgoODA27cuIGsrCz+pioxNDTEqlWr0LVrV+44i0QidOvWDZaWlnj37h1/k0pjZGSE5ORkREREcE2169ati7Fjx6J9+/b86CpLTk7GtWvXMGPGDAwcOJD7nUZGRpg6dSrat2+PGzdu8DdT+Z5cr149iMViREREcM+9urq6cHV1Rb9+/WRSLBt+/35lC78/c0W9evUKc+fOxdq1a5U2ly+vwsJCbN++HeHh4ViwYAFMTU35UeTExsZi165dcHZ2xrp16xTK8Nq1a+Po0aNwcHDAuHHj8Pr1a6xfvx5nzpyRi0c+Papsq6hx48bo1asXTp48KVdZUUZbWxt9+vRBVFQUbt68yQ/WWI8ePUJQUBBcXV0Vvmh+zqysrODm5oZDhw6p7eXJt99+K/fw5uTkBH19fbk4ZZWdnY24uDgcP34cBgYGcl+fhEIhVq5cieTkZIWvnCYmJpg7dy7u3LmDJUuWYMqUKbCzs5OLIyXtkyXt1yYQCFCvXj25r1VS1tbW6NChg9zf3bp1g6CU0e3bt2+v8Ca5efPmSivmqsjLy8OjR49w/fp1WFpaKnyVmzx5MuLi4pCamsptY2pqCm9vbyQmJqJ9+/bo3Lkz1qxZg4SEBLmHc03DfygpKCjAo0ePMGnSJERGRiIgIKDEfZiWlgZvb28kJydj5cqVMDY25kcpk+joaHTr1g3e3t74+uuvceTIEVy5cgW9evWCjo6OQl+4JUuWoEePHrh69SqmT5+Ou3fvYtWqVdzDCP+ckFJ3vj+WqlKmyCppgLQRI0bwo6Jp06b48ssv5dbVqVMHjRs35iq50lYNbdq0Qf369eXiCgQCtGjRAih+8EXxtfz8+XO0a9cObdu2lYtfGnd3d7lWWjo6OkortlX1uv8cSFvLSO87sgwMDNC0aVO8evVKrmVVWcXGxsLT01Ou33OzZs1UmolGnYYOHYpx48Zh7ty5aNSoEUaPHo3Q0NAKV/pevnyJe/fuwdPTE1paWnL3QTMzM1y6dAnPnz+Xa+2BMtyTu3fvDm9vb+zcuRNff/01V7596HlZE8i+ECsoKEBsbCzc3Nzg7e2NAwcO8KOXS2FhIfbu3Yv169djxYoVSj9oeXt7y5Wt6enpOHr0KBo0aAB/f3+FF+bjx4/nBndbunQpYmJiMHv2bLmPIUQzUGVbRdJ5tIODg1WqQEvn6D548KDS+fQ0DWMMZ86cgZ6eXpUYzfRjEgqF6N+/P27cuIGoqCh+sMaQSCQKbz5VVVRUxD2oPHv2TGGwFBRPkTFgwAB4e3vjr7/+4gdXGmV5URVjrMQKZmm6deuG27dv4/Tp02jatClWr16Nr776CrNnzy739SzbVK2kRdnAPeUlFArx9ddfY/bs2WjVqlWJD6NJSUmYPHkyHjx4gN27dyu0aCgraZNBKysr7NmzB2FhYRg4cCBq1KiBnJwcPHv2DCYmJjA0NIRQKOTijx49GtevX8emTZtgYWEBFH+1jo6OhqWlpdz/QCXk+2OqKmWKOhUVFSm9lmvWrFmm1ggikQhaWpXz6FIdrvvqSk9PD1988QV/tVpERkbC2dkZ69evV6jQfGy6urrw9fXlWvfExcXB2dkZbdu2xeXLl/nRVVZUVFTuL/98yq5joVCIuXPn4uHDh1i9ejWysrIwbNgwfPXVVzh06FC5X1jFqjAnu7KBJMtLKBSiadOmWLlyJezt7XHz5s0Kv+jIz8/H+vXr4e3tDV9fXwwdOhSCUj46SNWuXRtubm4YOnQoHj58iPT0dKC4zNTV1YVYLMZ//vMf/PXXX1iwYAH30vLx48cQi8UwNzfnpUg+lcq5Y1VTHTt2hIuLC4KCgpQWNrJ0dXUxbNgwhIWFVbiv3MeQnJyMs2fPYsCAAXSBKtGqVSvu2PPf/EolJyfLhaWlpcnNEVzZatWqBQsLC4waNQqZmZkKX6AYY7h165ZCU8uHDx9i/vz5GD58OA4ePIh9+/bh+PHjCjfHixcv4vr161i0aBHS0tK4NFNTU+Hs7CwXV12Kiopw5coVNGnSpFz9vkQiEczMzGBvb4+kpCSF/cEYQ2ZmJmxtbfmbck3gfv/9dzx79gyLFi3Cxo0bP2r/PXXIzc1FXl4eatWqpVCp+fvvvzFkyBBkZGTA398fLVu2lAsvj6+//hrNmzeHvb09hg4dKvdV8dGjR4iMjETTpk2hq6sLPT09NG/eHKamphg/fjyaN2/OPYQUFhYiJCQEpqamCudsZeT7Y6sKZYo6PXv2DLdu3YKBgQHXukEsFuPmzZsKfdcZY3jw4AHEYjH34kVHRwcGBgb4+++/Sx3JvaKqy3VfnYjFYsTHxyv90JGWloa4uDhYWFigVq1a/OAPKioqwpkzZ5CWlgY/Pz+uvz6TmZLsYxMIBLCyssJPP/2Ea9eu4caNG6hTpw42b95c7pG969WrB2tr61K7NGzdulWuX7gyH7onm5mZYcqUKTh//jweP36Mzp07Y/PmzeXuBvapfPHFF1wLLFUqxiXJysrC3LlzsW3bNmzZsgXDhg0r08vCoqIi5ObmwtTUlDs29erVQ9OmTWFpaYkJEyagUaNGXPzMzExcuHABrVq1+mAzdfLxqH7ECfT19dG/f3+V59Hu2LEj2rVrp9Ic3Z/atWvXkJSUBGdn5woVLNWVSCSCu7s7wsLCcPfuXbkw6UNjZGQkoqOjIZFI8OTJE0yfPh2//vqrXNzKJBKJ0LFjR5w4cQK//PILXrx4UeJUOFIpKSmYN28ehEIhpk+fDjc3N0yaNAkLFixQeIuelpYGALCwsIBYLOZ+59atW5U+BJXH69evubfImZmZ+PXXX7F161b06NFD6WAiqujSpQtu376NJUuW4PHjxx/8+n/z5k0sXbpULm6NGjXQuXNnmJqalvvrwMfuu/n+/XtcvnwZP//8M5KTk7n+syiuzERGRmLEiBFo0KAB9uzZo9L+ZYzh0KFDaNCgATw9PZV+7bO0tES3bt1w4MABbN26lXvxEx8fj/nz5yM9PR3ff/89dHR0oK2tDUdHRwDA8uXLcevWLUgkEuTm5sLf3x+bN29Gv379uCbF5c23JqoKZUp5ZWZm4s2bN5BIJJBIJLh79y68vb2Rnp4OFxcXaGtrQ19fH507d8b58+exbNky7mVYbm4uAgICsHz5cjg6OnLHXltbGzY2Nnj+/Dnmzp2Ly5cvcy013r59Cz8/P5XuyyWpLtd9ddSqVSvY2Nhg7dq1OHXqFDdt4JMnTzB37lxu2jN+v3ppZXDv3r0lDrBZUFCAjIwM6OnpwdzcHCKRCIWFhbh37x62bNnywcEZ1Sk3NxcrV67E2bNnuUq1QCCApaUlWrVqhaysLKX3Lz09PURHR+PgwYMlNttu2LAh2rZtC19fXxw7dkzlSruq9+StW7fiwIEDePPmDVjxi3pjY2N06tQJ2dnZJb5Q/BB+9yhlizr7bBcWFuLZs2dYt24d7t69CycnJ6UvIGJjY9G1a1c4OjqW2BoiOTkZ06ZNQ2hoKH777Te4ubmp/HzNGENycjLWrVuH7du3o1evXtz+1tXVhaOjI6Kjo7FkyRI8efIErPjDwfr16/H777+jT58+aNiwIT9Z8qnwhycnpXvx4gVzdHRk3t7erLCwkB+sYNeuXax58+bs/v37/CCNkZaWxtzc3NiMGTNYXl4eP7hElTkVyKdU0lQ3GRkZbPTo0Qy8aWZkp9CSXQwMDNgPP/ygME0PZKbC4E/XpMoUKKV59eoVGzJkiEJepItsugUFBWz+/PkK03ZJp/CwsbFhDx484NaHh4czU1NThTSli+z+8vHxkfvd/OlRpNOWSKemKG26Fn4+VJlKR/aczMnJYbNmzVKII134x/lDeZGdPk2TlJRvsVjMNm/eLDeFSWlT3UgX/n5hjLH8/Hw2e/ZsLk5J00lJp4/jpwlAYTqVgoICtmrVKoV4UHLsy5vv8qisqb+qWpnyoX0um+6Hrk3+sU9OTmb9+/dXiAeAWVlZKUyN8+7dOzZz5kyFuChl6i/+8ZPmkX8sSrp+UEWv+5L2S3lV5v3+Q+doaVO+AWAzZ85UOlVpSdNR8adX+tA0WrL5+tD1ACXnnKo+dP2sWbNG6XNnSdOFKTvHraysFOIpy3dp5xW/XGbF93x+POnCn1ZMk5R07MViMVuzZk2JU3+dPHmSi8ufgkuqtH0C3rVZ2rF3cnJSKH9KmzZRdio8ohnoy3YZmZiYYPDgwQgNDVWpWYyLiwtMTEwQEhLCve3TNFFRUYiKisKIESNQo0YNfjApJm3ZwCconkJr9erVMDc3h1gshru7O86dO4eJEyfyo1eq+vXrY/fu3di2bRu6d+/OD+YwxhAYGIjNmzfjxx9/RI8ePbgwCwsLzJ07Fw8ePMCWLVu4r5ddu3bFzp07uT795ubmmD17Nv766y/MmjWL215d2rZti5UrV+LUqVMV6o8rEomwatUqHDlyBC4uLkqbvsnq0KEDzp49C3d3d24aKdm8VJUvqW3btsWCBQtw584dTJs2TS2Dpejo6GDixIlc08qYmBh+FKB4AJ1Dhw5h9uzZXLeUnj174ujRo/D09JTLi1AoxKxZs3DkyBGFc+vw4cMVOvaariqUKRVlYGAAd3d3XLx4EXPnzpU79sbGxtizZw9WrlzJDXomPfbBwcEKgwjp6upi9erVcteyWCzG999/D19f3wpdm9Xluq+OBAIBhg0bhlOnTskdn549e+LIkSNYvXq1XHcVKQsLC2zfvp2bFaMkgwYNwu7du2FjYwMUf01dunQpoqOjMWjQIH70SiMSiTB//nwsXbqUux5krx9PT0+FrkAoHsRs7969cvtGGTs7OwQHB2POnDmwsrLiB39QaffkqVOnYvPmzdxzh1gshouLC44cOYJffvlFodWBprKyssKMGTO4aQBLum/27NmTK4ufPHmitJVXRUiP+9GjR3HixAmF8kdXVxdr1qzBtm3b5M7blStXYs+ePVVmsNDPBr/2TT7syZMnzMbGhu3atYsfpEAikbB169YxR0dH9uLFC37wJ5eXl8dmzJjBxowZo/TNcGkq8003+byU9BWKaJ7s7Gw2YcIEBoAdOnSIH1xtqPPL9ueipK/GpOqj+/3nhe7JH7Z3714GgE2dOpXl5ubygwnh0JftcmjUqBGcnJwQFBSkMLALn0AggLOzM5KSknDx4kV+8Cd37949nDx5En379lX6ZpgQQqSys7Nx7NgxnDt3DnZ2dtwbdUIIIeRzUFhYiJs3byIgIABisbjEft2ESFFluxy0tbXh4+ODkydPwsjIiB+soGXLlnjw4IHS+Ug/tQ4dOiAhIQEDBgzgBxFCCCcgIAB6enoYNWoUAGDOnDmlNs0khBBCqpPY2Fh06NABnTp1QmhoKH788UdukE9CSkKVbUIIISqxsrLCnDlzcOnSJQwYMEDlkVUJIYSQ6kBHR4frR7906VJqFUo+SMA0ddQuQgghhBBCCCGkiqIv24QQQgghhBBCiJpRZZsQQgghhBBCCFEzqmwTQgghhBBCCCFqRpVtQgghhBBCCCFEzaiyTQghhBBCCCGEqBlVtgkhhBBCCCGEEDWjyjYhhBBCCCGEEKJmVNkmhBBCqonY2Fi0adMGkZGR/CBCCCGEfGRU2SaEEEIIIYQQQtSMKtuEkEqVm5uL2bNno1mzZrh8+TI/mFRjdOyJJoiMjESzZs0wa9YsZGdn84MJ+SgOHz6MBg0aYO3atSgsLOQHE0KqKapsl8PTp09ha2uL3bt384NUduPGDVhYWODChQv8oEqjjnyrIw1SeQICAtCmTRvExsbygz6ZZ8+eISwsDLGxsQgPD+cHk0ombVa8dOlSflClo2NPPoaAgAAIBIISm85fvHgRsbGxuHTpEp4/f84PJtVMbm4uJk+eDIFAwC2TJ09Gbm4uP+pHk5ubiwsXLuDly5e4cOECMjIy+FEIIdWURla28/PzcfToUfTu3Rv6+vowNDTE8OHDce3aNTDG+NGB4m3+85//oEWLFrh79y4/mFOetGUxxnD+/HkAQI8ePfjBKmvZsiUcHBwQFBSE/Px8frDaqSvfjRo1gpOTE86cOYO3b9/yg6sN6cOb7NKsWTNMnDgR0dHRKp0rHxIYGIgBAwZoVMW4MjRq1AgODg6wsrJC9+7d+cGkGqNjX7KLFy+iQYMGEAgECAgI4AdrnDdv3mDLli2wt7eHQCCAvr4+3NzcEBwc/FHuYRXRo0cPWFlZwd7eHg0bNuQHfzSfS5n/OUhOTsaUKVOwevVqfpBSIpEIPXv2hKmpKXr27InatWvzo1R57969g7e3N2bPnv1JX2xUB9nZ2Zg0aRIEAgFcXFzw5s0bfhRShWhcZTs7Oxs+Pj4YNGgQgoODkZWVhbS0NBw8eBDOzs44ePCg0orO9evXcenSJRgYGOD3339XevMvb9qykpOTERgYCCcnJzRq1IgfrDJdXV307dsXhw8fRnR0ND9Y7dSVb21tbfTp0wd//vknwsLC+MHVWmxsLHbt2oX+/fvj9u3b/OAyi42NxZMnT/irqx2RSARfX1/ExMSgW7du/GBSjdGxVy4hIQELFy7kr9ZY165dQ8+ePfHjjz9yLRSysrJw4sQJ9O7dG7du3eJvolHs7OwQExODDRs2QFdXlx/80XwuZf6nJhKJsGPHDjDGkJOTAw8PD36UCsvIyMC1a9fw/v17flCJ3N3d8eLFC8ydOxdCoZAfXOXl5+fj5s2bVNGuIMYYdu3ahUOHDsHIyIgfTKogjatsnzhxAitWrMCwYcPw+PFjSCQS5Ofn4+zZs7C0tMSyZcvw4MEDuW1yc3Nx4MABDBw4ED4+Pjhy5IjSJovlSZvv2rVrSEpKwqBBg6Ctrc0PLpOuXbuiY8eOOH36NIqKivjBaqXOfLdq1Qr9+vXDqVOnqnX/N2tra8TExIAxBsYY3r17Bz8/P6SlpeHKlSv86IQQ8kHZ2dlYu3YtAGDlypX8YI0THx+P2bNn48mTJ/jPf/6Dx48fo6ioCBKJBC9fvsSmTZtQs2ZN/maEEELK4fLly/D19cX//d//oX///vxgUgVpVGX77du3OHr0KBwcHLB27VpYWlpCIBBAR0cHLi4uWLJkCZ4/f47Q0FC57a5cuYJ79+5h6NChsLOzw6BBg7Br1y65Zs7lTVtWdnY2Tp06hV69esHKyoofXGZGRkbo168fQkND8ezZM36w2qg73yKRCO7u7ggLC8Pff//ND662dHV10a1bN1haWvKDkJ+fj+DgYAwePBiGhoYQCASwt7fH9u3b5V5IyDZPX7hwIaKjo9GsWbMP9i2TSCQICwvj0tfX18fgwYMRFRWltDWGRCLB+fPn0a1bNwgEAnTr1g1nzpxRGrcsCgsLERISAjc3tw92w4iMjFRoil9Sn0ppv2J+fOmirL/x69evsWrVKm7/2dra4uDBg0pbtZRFQEAA7Ozs8OjRIxw7dgyOjo5c+ocOHVI6sE1CQgLmzJkDCwsLCAQCODo64vz585BIJHLxpL8zICAA2dnZ2LhxI5o1awZ9fX2MGzcO//zzj1z8suJ3kxEIBHB3dy/xa5oq+1A6TsPYsWOVvly7ceMGGjRogPXr13PnQFmOvVR2dja2b9/ONVO2sLDAggUL8Pr1a35UlfKtiRhjOHnyJM6dO4fly5ejcePG/CgapaioCPv27cP169fh5eWFlStXwtLSElpaWhAIBDAxMcGPP/6I9u3bc9vIjhuRkJCASZMmwdDQEBYWFti4caNC2cZvnm5oaIjBgwcjLCxM4fqBzLFv164dF3/58uX8aOXqtyu9fqTXfEnn4Js3b+Di4oKAgAC532hoaIgpU6YgOTmZi1veMr8snj17hmXLlnH7xMLCAhMmTCixyxNjDNHR0ZgwYcIHyyyoeG1K97ey3yItD6RlgLQcDA8Px/bt27ljLm2BZ2Fhgb59+3LlVlnSLi9V9qH0uAuKu5VFR0dj4cKFcseSnxfZbaSLsvuZlPTYjBs3Tu5ZQln5FhAQABcXF7x48QIHDx6Era0tBMXl4bFjx5Qey7JQ5X4ve53Vq1cPISEh2LlzJ2rVqiX3m/ldZaTPNLJpT5o0CQkJCXLxpOnPmTMHycnJXLkvfQa6c+eOXHyomG9NlZKSAl9fX7i6umLSpEnQ0tKoahopL6ZBoqOjWZMmTdj69ev5QYwxxl69esUcHR2Zh4cHy8nJYYwxlpaWxtzd3dm6deuYRCJhrDid5s2bs/3793PblidtvvPnzzMjIyN2/vx5flC5PXnyhNnY2LANGzZw+Ve3ysh3Wloac3NzY56eniwvL48fXOXt37+fWVtbs5iYGMYYYxKJhL18+ZJ5eXkxc3NzdvPmTYX4AJQu8+fPZwUFBR+MJ13452BBQQFbtWqVQjwAzNnZmaWmpsrlo0mTJmzIkCFMLBbLxVWW77KQSCRsw4YNCnlQlg/GGIuIiFCIFxERIRdHKiYmhllbWyvEB8DEYjE7ePCgXPzHjx8zJycnhbgA2PTp00u8hlUh3YcDBgxQSFssFrMTJ07Ixb9x40aJeV+zZg0rLCzk4kp/56pVq5i7u7tCfHd3d5aWliaXvqrevXvHZs6cqZCmdPHx8ZGLr+o+LCwsZN7e3qx58+bs/v37cmlIJBK2bt06hbCyHHvGGEtOTmb9+/dX2KYi+S6rnJwc5uHhoZCm7KLsPC8L6b1Jel5I95PsvaqipOdYaftbVUlJScze3p45OjqyFy9e8IOVkpadW7duZVZWVgr7UPZeV9o+F4vFLCQkRC7tmJgYZmdnpxBXusj+ZmVp88tWWTk5OWz+/PkKaQJgNjY2LC4ujoubmprKnJ2d2bfffqv02pf9P+Up88tCmhd+mgBY8+bNWXR0tFx8iUTCAgICFO4PAOTueVKqXpvS/a3st0jPc+nxkZ6j48ePZ6amplx6s2fPZubm5tzf0ue1sqTNV9q2Uqruw9LiyS6yeVG2Db9Mkyrt2ABgU6dOZe/evePi79+/n5mamjIXFxeFuKampiw8PFwu/bJQ9X6v7DpTtsiWcYWFhWzbtm1Kf6e5ubnS69jBwYF1795dIb6dnR17+vQpF1/VfJeHsvsaf6lIWV5QUMDmz5/PlTfS317RfJNPT6NemaSnpyM+Ph7ffPMNPwgAoKenh6+//hoJCQnIyckBAISFheHZs2dwc3ODQCAAigcfGzVqFHbv3o2kpCSgnGnLys/PR1BQELp374527drxg8tNOuDY6dOn5d6Gq0tl5btu3bpwdXXFH3/8gUePHvGDqwXZLxBaWlowNTXFrVu3cPLkSXTo0EEubs2aNbFp0yYkJSVxTSzj4uLg6uqKS5cucefhiBEjuGbpPj4+Ck3VGWPYsWMHRCIRl/aZM2ewfPlyODk54fLlyygoKEBRUREeP34MJycnhTef8fHx+O9//4s5c+YgIyMDBQUF8Pf3R2JiIs6dOycXtyzS09Nx7tw5DBs2DMnJyWCMlZoPOzs77jdFRETIhfFZWVnhzp07cvshOTkZ/fv3h5eXFwYNGsTFzc/Px7Zt2xAbG4vDhw8jJycHjDGkpKTAy8sLe/fuxdWrV+XSL6v4+HicO3cOa9euRVpaGoqKinDu3DkYGBjg6NGjyMrKAopbzKxduxY1a9ZEWFgYCgoKIJFI8PTpU4wbNw7+/v6IiYnhJ4+ff/4ZCQkJCA0NRUFBATIzMzFlyhRERESU+BX6Q0JCQrBx40ZMnToVz58/h0QiQVFREW7duoVWrVrJxS3LPpSO05Cenq5wHN+8eYNz586hV69ecl9py3Ls8/PzsXbtWpw8eRKzZ8/G06dPuS4+4eHhMDMzk4urar41zevXr7F48WJ07doVU6dOrXB3no8hNTUVSUlJaNWqFQwNDYESWqHwvypGR0dj2rRpsLGx4bps3bt3DzY2NoiIiEBmZiYX18zMDH/88QfS09PBGENBQQGCgoJgYGCA4OBgrk+s9Ng/fPgQu3fvxrt378AYQ35+Pvz8/GT++7/K2m83NDQUmzdvhpeXF1JSUrjtDh8+jOTkZOzbt0+hu9eVK1egp6eHP//8E0VFRUhISICrqytu3bpVoTK/rNq0aYMrV65w10NOTg78/Pzw/PlzhRlPbt++jfnz58PS0hJBQUHIycnhugR4eHjInZdluTbLY/fu3Vi0aBH+/vtvtGrVCr6+vpgyZQqePn2Kb7/9Fo8ePUJeXh5/s0qhyj40NDREcHAwGGOIiYmBtbU1fHx85I4lYwx2dnZcurLbpKamwtnZWea/youLi8OaNWvQokULXL16lbvXx8TEwNXVFfv27cOlS5fktnn58iUiIyPh4+Mjd7+Xri8vVe/3steZ9Pd5eHhw+1G6jBgxgkv7r7/+wurVq+Hu7s6dUwUFBQgLC4OJiQl27dql0IoqLCwM6enpOHv2LPLz85GTk4Ply5cjMjISISEhXDxV862Jjhw5goCAAKxYsQJNmjThB5MqTKPOOmkTylq1avGDlHr16hX27t2LYcOGwcLCgluvra2NAQMGIC0tDUeOHAFjrMxp8z169Ajnzp3D0KFDUbduXX5wuWlra2PIkCFISkrCtWvX+MEVVln5BoDvvvsOZmZmCAkJAdPwpjnqEhISgiVLliAxMVFu/cCBA/Hjjz+iQYMGXBPLJk2awMbGBtnZ2eV+YMjNzeXGFNiwYQO+/fZbCIVCaGlpwdLSEnPmzFE4rmKxGGvWrMGCBQugr68PoVAIBwcHdOnSRWkTaFXp6Oigfv36ePDgAe7evQuJRFJqPipCOpihmZkZpk+fLjeYTHJyMq5du4YZM2Zg4MCB3EOqkZERpk6divbt2+PGjRsyqZWdqakpdu/eDU9PT9StWxdaWlpwcHDA8OHD8eDBA6SkpAAAnjx5goiICPz000+wt7eHUCiEoLgJ4syZM1FYWIj79+/zk8fQoUMRFBSEXr16QSgUQiwWo3v37nj58mW5zpXc3FyEhobC0dERCxYsgJmZGfeSSE9PT+EBo6z7sEmTJrC1tcX58+flpqyJjY1FZGQk+vTpgxo1anDryyIhIQHnz5/HxIkTsXTpUq5Zq46ODrp164Zx48Zxccua77KQfWgsaQkODuYqnWVRWFiIPXv24MWLF5g9e/YnHaSrLLKyshAfH4+6detCR0eHH1wqX19f+Pn5cV22GjdujPbt2yMrK4srh0QiEby9vdG7d29udGahUIguXbqgWbNmyMnJ4Sq4CQkJCA8Px7Rp0/DDDz9w+1BHR6fc93Wp9+/fIzw8HI6OjvD09OQGJRKJRHBzc8Pw4cMRFRUl95IAAJycnODv7w8bGxtoaWnB3NwcPXv2RGFhoULFvLIYGhpi1apV6Nq1K3c9iEQirsvTu3fvuLjSc/j9+/dYv349+vbtC5FIBEFxl4ApU6bIvTQry7VZHv369YObmxuMjY3RoEEDODg4YOTIkRCLxahVqxaKioo+yrNFWfZhZbp06RKePHkCHx8fdO7cmbvXW1lZYcWKFWjYsCEiIyPlzi2xWIwVK1Zg3rx5VeZ+f+XKFZiYmGDBggXcOSUUCmFvb4+xY8fi3r17ePnypdw2Dg4OCAwMhIuLC3R0dCASiTBmzBjY29vj7t273H2zMvMt+xK5pEX2pUJZPHz4EBs2bMCUKVNoMNFqSKMq29IbnLIvy7KkBZCRkRFOnjyJmTNncl+1paRfyqRhZU1bVlFREYKCgmBmZobOnTvLhalD48aN0atXL5w8eVLhZl4RlZ1vExMT9O7dG6dPn8aLFy/4wVUe/wtEWloadu/ejaioKKxfv16u/1Rubi4OHjwo11dWUNxHryJycnKQkJAAOzs7fPXVV/xgpSwtLdGvXz+5Cqqenh7EYrFcvLLS1dXFzz//DD09PTg5OaFFixbw9vbG/fv3K9w3TFZhYSG2bNmCpKQkLFy4UKFi8vLlS9y7dw+enp7ciw3pYmZmxs2lW55Kq5SRkRFat24tV65oa2ujWbNmcg/Sjx49wsuXLzFw4EC5fAgEAlhbWyM+Pl7pl+oWLVrAxMREbt2QIUPAeF9FVCU9T5o2barSw0RZ96G0JUtkZCTXkqWoqAghISGwsbEpscWQKlJTUxEdHY2ePXsqHGu+suZbU1y+fBnbt2/HvHnz0Lx5c36wxtLW1oZYLEZubi53zsu2Qtm/fz9/E07Hjh3lXsDUrFkTW7duVXhh8eLFC7m+sgKZvp+ypOdJy5Yt1d4qIDs7G3FxcTh+/DgMDAzkziuhUIiVK1ciOTlZYfqdrl27KnyB+hSjB8fGxsLT01OuL7i0T7GsvLw8PH/+HO3atUPbtm3lwpQpy7VZHh06dJDbXw4ODhX+Wl5equ7DyvTq1Su0atVK6dg6ZmZmaNSoEdLS0uRGQLe0tORe2kpp8v0+Ly8Pjx49wvXr17kXcbLL5MmTERcXh9TUVLntmjZtii+//FJuXZ06ddC4cWO5lzKVle/KlJ2dDV9fXzRq1AiTJk2qliPVf+40qrItFoshFouVfglCcZPN2NhYmJubl7nJVUXSfvbsGU6ePIk+ffooPCCrQ40aNdCvXz8EBwfj5s2b/OByq+x8CwQCODs7Izk5GcHBwfzgaqdu3br44YcfMHz4cNy6dYtr9p+bm4uffvoJw4cP56aUU7caNWoovAT6FKysrHDhwgWEhYXByckJ+/btQ6tWrTB06FDua29FMMYQGBiIiIgIbNy4EcbGxvwoKCoqqpR9rAr+4DwV+XJQGVQ9T8qzD3v06IFGjRohPDwcjDG8evUK165dQ+/evdVSvvDLXWXKk29VKRtQi7+Ud77TyMhIJCYm4vvvv5dLr2vXrgCAkSNHcg+a/HPsU6pXrx6aNm2KmJgYtb4IloqPj8eAAQPg7e2Nv/76ix+slCrnSVlJJBKNu5ZVFRkZCWdnZ6xfvx6xKs7fLRKJVConpCpjn2uS8uzDyqKjo4MvvviCv/qTqIz7PStu1q0ORUVFCoPGoZLyjRIG/uQv/IHgVPH8+XPcuHEDhw8flnvZV6tWLezcuRMhISGoV68eBEq67JCqQfXS9iNo1KgR2rdvjwsXLij0X2aM4eLFi7h06RLatWtX5qlGKpL2xYsXgeImTwLeF3R16dixI1xcXBAUFKS08CiPj5FvKysruLm54cyZM3Kjv1d3sjfE+Ph4hISEwNXVFTExMdxbVlbcR68idHR0YGRkhDt37iiMiPup1KhRA/b29ti0aRNii+ceDw4OxoEDB/hRy+zy5cv49ddfsWbNGpibm/ODgeIKgLW1dalNfrdu3apwHVdUdnY2/vzzT9StW5f7aiDtvhIaGqqQB+kyb948XkrqJ22KnpqaKld+MMZw+/Ztha/r5dmH0vElIiIikJ6ejvv37yMpKQnOzs4VKl/EYjGaNGmC69evf7DCU558k/IzNTVFq1atcP78eYSFhYGpuUnvxYsXcf36dSxatAhpaWncMVTWt7VmzZowNTVVaF6anZ1d4X76tWrVgoWFBUaNGoXMzEyFc4oxhlu3bmnc6PFFRUU4c+YM0tLS4Ofnx/VjZzJ9imXp6OjAwMAAf//9t0JXKGXKcm1K8VuW5Obm4vz583Jxyqsy0i7rPqxMenp6uH37Nh4+fMgPQlJSEp49e4aGDRt+1PJN3fd7kUgEMzMz2NvbIykpSeE6Y4whMzMTtra2/E0VPHv2DLdu3YKBgYFCNxd155uQitCoyraxsTGcnJxw5swZeHl5ISEhAYwx5ObmIiAgAAsWLICDgwNcXFz4m35QedN++/Ytzpw5AycnJzRq1EguTJ309fXRv39/nDt3Ti0Djn2sfAuFQvTv3x9RUVGIioriB1cr2dnZOHjwIA4cOIDWrVtzTSFzcnKQnJwMU1NTGBkZQUtLC2/fvsWBAwdw6tQpfjIcPT09REdH4+DBgyV+NdLT00P79u0RFhaGuXPn4t69eygsLARjDElJSdi4ceNHe8nx6NEjeHt74+7du1wzti+++ALt2rVTS7+2y5cvY+7cuVi8eHGpTW0bNmyItm3bwtfXF8eOHZPrQ6wuOTk5SE1N5X7nq1evsGbNGhw9ehROTk7cF3dLS0s4OjpiyZIlCAsL+2RfJfX09NC8eXOcOHECgYGByM/PR2ZmJtatW4fJkycrfA0uzz6UDpQWFRWFu3fvIiwsTGFgtPL48ssv0aZNG2zevBnr1q3jBrUpLCzEzZs3sWfPHi5uefKtqsrss+3t7a2QFpMZPG7//v1gahgoS910dXUxbNgwAMCcOXPg7+/PlTcSiaTCzfXT0tKA4pdWYrEYEokET548wdatWxVaeTVo0AAtW7aEv78/bt68ycWdOHEitm3bJhe3rEQiETp27IgTJ07gl19+wYsXLyqlyakqZX5ZFBQUICMjA3p6elyrvMLCQty7dw9btmxReMmmra0NGxsbPH/+HHPnzsXly5e5Mu7t27fw8/OTe/4oy7Wpra2NWrVqISoqCleuXOEGXVu4cCEWL17MxSuPyky7rPtQqmbNmtDV1cWZM2fw559/quV8sbOzg56eHhYtWoQ///wThYWFkEgkuHXrFubOnYv09HQ4OTlV6OWmqspzv5e+9A0ODsb58+dLfEHTpUsX3L59G0uWLMHjx49LjCcrMzMTb968gUQigUQiwd27d+Ht7Y309HS4uLhwXUvKk29VVVafbWUDxDKZgR2dnZ2RmpoKVs5uZkQD8Icn/9RKm2bCysqqQlMZlCfto0ePMlNTU3b9+nV+kNq9ePGCOTo6Mm9vb7npgsrjY+b73bt3bMyYMWzMmDFy01JUZaVN12JnZyc3PUpKSgrr16+fQjzpomw6FcYYu3nzptw0J9KFP0VJcnKy0mmioGQqC/6UZVLS6UdKmnJEFaVNz1XSdB38eLKL7BQZcXFxzMbGRiGOdOH/zoiICKXTCkmXkqaBUUVpx75///4sOTlZLn5gYCAzMDBQiAslx166DytyHEqi7HwSi8Vs9uzZ7Ntvv1X4n+XZh9JrfcqUKcze3l7pdIJlPfaMMRYeHl5iXtSRb00lnUqGvz8qQp1Tf7Hi6Wg2b96sdJoeZftcev2o8v/Dw8Plpn7iL7JlYUlT+lhZWTFvb2+F//mhaXr4ZcqrV6/YkCFDFOJJF9nzsLTytKQymJVwjUJJmV8WpZVX/HyzD0wRqCzfZbk2T5w4oXCeiMViNmTIENakSRPu+PDLQf7+lP4tu19UTZuV49iXdR8yxlheXh7z9PRUiIsSroeSFtnfWNo0n2KxmK1Zs4abRlSatrJjxt+f5VGW+71USdcoeGVcTk4OmzVrlkIc6SK7Tz50P1m1apXcPilPvjUVTf1VfWjUl20Uf4EOCAjAtm3bYGNjAxS/9ZkzZw6Cg4MrNEpfWdPOzs7GqVOn4O7u/lGaEpmYmGDw4MEIDQ3Fs2fP+MEq+9j5ln79CAsLw99//80Prja6d++OzZs34+TJk3IDmBgZGeGXX37B2LFjYWBgALFYjO+//x5nz57Ff//7X7k0ZLVv3x579+6Fu7s7DAwM+MEcY2Nj+Pv7Y9u2bejevTsAwMDAAGPHjsXq1atL3VadmjRpgu3bt2P8+PFcE2/p9XPp0qUKvXF99eoVrl+/zl9dIjs7OwQHB2POnDlKB5NRt+7du2Pbtm0ICAhQ6Ec+aNAgXLx4UW6/fArt27fHgQMH0LNnTwBAz549cezYMcybN0/paM3l2Ye6urro27cvtm/fjrp166ptOsFu3bop5MXKygorV67E5MmT5eKWJ9+k/IRCIaZNm4br16/L7XNzc3MMHjwYBw8eRJs2bfibqaRr167YuXMnd86am5tj9uzZ+OuvvzBr1iy5uNI+7Tt27ICVlRUMDAwwceJEBAcHKzQ5L4/69etj9+7dcuWsuqla5pfFoEGDsHv3brlnmqVLlyI6OlpuykQpXV1drF69GkeOHIGLiws3ns33338PX19fhQHfynJturq6YuPGjdxUg9IyaNGiRUrLoLKozLTLug9R3Ex5/vz5WLlypUqDzalCKBTC09MTR48eVTg2J0+ehKen50cbPKs893uBQIBJkyZh+/btpV5DIpEIq1atkjsHy8LAwADu7u64ePEi5s6dK7dPypNvQiqbgDE1d8KqRm7cuIHBgwdj9+7d3MNAZXv69CmGDRuGiRMnYvz48fxglXyKfL99+xbjx4/HN998g0WLFql9tFhCPpaAgACsXbsW//3vf6kyR6qc2NhYDBkyBFu3bqUHS0JIlZabm4vZs2cDxdMJalI3G0JUpXFftjVFfn4+AgMD0a5dO7V9vVGFdBCioKAgvHr1ih/8QZ8q39KpgYKCgtTS55wQQgghhBBCqjKqbJegRo0a+OWXX3Ds2DGV5q1VF21tbfj4+ODkyZMwMjLC0qVLFaYWKGmZPHkyJBLJJ8k3AIwfPx537tyhr4GEEEIIIYSQzx5VtgkhhBBCCCGEEDWjPtuEEEIIIYQQQoia0ZdtQgghhBBCCCFEzaiyTQghhBBCCCGEqBlVtgkhhBBCCCGEEDWjyjYhhBBCCCGEEKJmVNkmhBBCCCGEEELUjCrbhBBCCCGEEEKImlFlmxBCCKmgpUuXYvLkycjNzeUHEUIIIeQzRZVtQgghhBBCCCFEzaiyTQghhBDyGcnNzcXs2bPRrFkzXL58mR9cJcTGxsLLywtpaWn8IKLhsrOzsXjxYty+fZsfREi1o3GV7adPn8LW1ha7d+/mBxEN9vbtWwwYMAALFy5EUVERP5gQtYqMjIRAIJBbIiMj+dGA4ofKyZMny8VVZ3PfgIAAubTbtGmD2NhYfjRCPgtluTbLQnodq/Pa/VgCAgI0rlx49uwZwsLCEBsbi/DwcH6wxouPj8eMGTMQHx+PgoICfjDRcDk5OXjy5AmmTZuGhw8f8oMJqVY0rrLdqFEjODk54cyZM3j79i0/mGiomzdvIioqCn369IG2tjY/uEp58+YNXFxc5B4WDQ0NMXjwYBw6dAjZ2dn8TcolMDAQAwYM+GQPYMp+p76+PhwdHbF9+3a1/U5SMfn5+QgODsbgwYNhaGgIgUAAW1tbbNmyBW/evOHi8Sv9AoEAzZo1w8SJExEdHQ3GGBdXWiFSVgmqypWakuTm5uLMmTPo1q2bxlV6+JS9HFK2uLi4yB3/snr37h28vb0xe/bsanOc1eFTl8sfS6NGjeDg4AArKyt0796dH6zRMjMzsWzZMohEImzevBnGxsb8KHIePnyINm3aQCAQYOnSpfxgjZWdnY1Jkyap5Xr/GBhjSEhIwM8//wxDQ8NS93X9+vWxdu1amJiYYN68eUhJSeFH0VgSiQT379/HuHHjoK+vj4CAAH4UORKJBNeuXcPEiRPRrFkzCOil/GdH4yrb2tra6NOnD/7880+EhYXxg4kGys7OxsGDB+Hg4ICWLVvyg6uFtLQ0HD58GMOGDcOQIUOQmJjIj1JmsbGxePLkCX/1J5WVlYULFy5g6tSpGDFihMbeAO3s7MAYA2MMERER/GA5IpEIO3bsAGMMOTk58PDw4EepkBEjRnB52b9/Pz+4Ql6/fo2JEyeid+/eOHz4MNdc8vr16/jxxx+xbds2/iZyYmNjsWvXLnz77bc4ePCgXIX7c5CdnY29e/fCxsYGffr0wZUrV/hRPlv5+fm4efOm2ivaZbk2NZEmlsuVQSQSwdfXFzExMejWrRs/WGMxxuDv74/w8HDMnj37gxXtt2/fYtGiRXj58iXEYjE/WGMxxrBr1y4cOnQIRkZG/GCNwhjDw4cPMX78eLRu3RqrV69WqWm/sbExFixYgDt37mDPnj0a3ypSIpEgIiICgwYNQqtWreDv74+srCx+NDlpaWn48ccf0aVLF+zatYsq2J8pjatsA0CrVq3Qr18/nDp1ir6uVQF///03wsLCMGzYMOjq6vKDqywfHx/uoTE/Px/R0dEYNmwYzpw5g19++UXtD6mfiuzvLCgoQGxsLIYNG4aTJ0/i119//ewqaJoiNzcXPj4+2LdvH/r06YPw8HDk5+eDMYZ3797hjz/+gImJidw21tbWiImJ4Y5neno69u3bBwMDA6xZswZxcXFy8au7kJAQjBkzBu/fv8fOnTsxatQofhSNI/tyiMm8IHJ2dkZqaiq3Pjg4GIaGhvzNCanWEhIScPDgQYwYMQJ2dnb8YDmFhYXw8/PDjRs3sGbNGlhaWvKjaKzLly/D19cX//d//4f+/fvzgzVKeno6vLy84O/vDzc3N+zdu5cfpURt27aFh4cH9u3bh7///psfrFESExPxn//8B+fPn8f//d//Ye3atfwocvLz87FixQrs27cP06dPR3R0NHcPv3PnDqysrPibkGpKIyvbIpEI7u7uCAsL0/iL73NXVFSE06dPo127dujYsSM/uNrQ0dFB69at4evri/79++Po0aN48OABF66sqa+9vb1Cc2zZpr4LFy5EdHQ016xIuvCb76qatjoIhUI0bdoUCxcuRKtWrfD8+XPk5eVx4W/evMGWLVtgb28PgUzz+rCwMEgkErm0ZPspJiQkYNKkSTA0NISFhQU2btyo8LKiLGlXtvz8fBw9ehSOjo4QCASwsLDAggUL8Pr1a37USnP16lXs3bsX/fv3x65du9CtWzfo6OgAAHR1ddG7d29MnDiRv5mc2rVrY8SIERgzZgyio6ORmprKj6KywsJChISEwM3NDfr6+jA0NMTw4cNx7do1jX0h065dO6xcuRJXrlzBqFGjUKtWLX6UaiM7Oxvbt2+Hra0td856eHggISGBiyPbRL1evXoICQnBzp07UatWLbkyiN8sUpOuTRT/1o0bN6JZs2bQ19fH4MGDcefOHX40lfNdnnIZxV+6wsLCuLJZmpeoqCil14REIsH58+fRrVs3CAQCdOvWDWfOnFEatyxyc3Nx8OBBufLKw8ND6Ve0svSpj42N5ZpfK1uUNRN+/fo1Vq1axe0/W1tbHDx4EPn5+fyo5XLixAlkZmZi+PDhEAqF/GA5kZGR2L17N1asWAFbW1t+cLmEh4fDwsICP/zwAzIzM7n1jDEcOHAA+vr6WL16NQoLC+W2K4uUlBT4+vrC1dUVkyZNgpZWxR/VHz58CFtbW/To0QNJSUlyYdLfNG3atHI9U9SpUwe9e/fGuXPn4O/vj6+//pofpUTa2tro168fAODIkSMa/XW7QYMGGDRoEC5duoSVK1fC1NSUH0VOdHQ0Dhw4AC8vL/j6+qJ169bcPZx8ZpiGSktLY25ubszT05Pl5eXxg4mGuH//PmvevDnbtWsXP6jKSk1NZc7OzszHx4cfxBhjbMeOHQwA27t3L7du//79DIDSZf78+aygoOCD8aSLh4cHy8nJKXPaZVXa74yOjmZNmjRhU6dOZbm5uYwxxnJycpiHh4dCHgAwsVjMQkJC5NLYv38/s7a2Zlu3bmVWVlYK22zYsIFJJJJypS0rIiKCAWARERH8IAXS/8Pfx7JycnLY/PnzFfIBgNnY2LC4uDj+Jhzpb46JieEHlUlhYSGbN28eE4vF7Pz58/xgpUr637JpXbt2jbEP7DNl+0gikbANGzYo7A8AzNnZmaWmpvKTUZmPj49CmrKLst9UHtLfpa70+Hx8fEo9r8pLmu/S9nNycjLr37+/wr4DwMzNzdmlS5cY+8B1Jrvs37+fS7u0bdR5bX6INB8ODg7MyclJIS92dnbs6dOnCvH58ZTlu7QyVrrwj21BQQFbtWqVQjwouSb279/PmjRpwoYMGcLEYrFcXHNzc3bz5k0ublnl5eUxT09PhTwoyzOTOSayS0nHJyYmhllbWyvER/E+PHjwoFz8x48fKz02ANj06dMV8lJW6enpbNCgQWzUqFEsMzOTHyzn+fPnzMHBgfu/0t+i7H5XFrJloew97MGDB8zGxoa5u7uztLQ0/mYqKygoYPPnz+fuNapc/6o6ceIEE4vFcs/V0rKDf/1UhPQcU3VfS3+ji4sLe/36NT+4TD7W/YTJlBuy5aWs9evXsy5durCEhAR+EPnMVPx1WSWpW7cuXF1d8ccff+DRo0f8YKIBGGMICQmBgYEBevbsyQ+utr755huguEmRVM2aNbFp0yYkJSWhqKgIEokEcXFxcHV1xaVLl7g3ybL9e318fBSa/TLGsGPHDohEojKnrQ6FhYW4d+8eVq1ahfj4ePTs2RM1a9bkws3MzPDHH38gPT0drLjZeVBQEAwMDBAcHIz379/LpRcdHY1p06bBxsYGjx8/hkQiwb1792BjY4OIiAi5LwNlTbuyhIaGYvPmzfDy8kJKSgqkTXkPHz6M5ORk7Nu3r9Lfvufk5OD58+do1aoVGjduDJQwcJayL0uy3r59i7179+LAgQNwdHQsd7O19PR0nDt3DsOGDUNycjIYYygqKsLjx4/h5OSkli8vpHyKioqwZ88enDx5EosWLUJaWhpYcVcDPz8/pKWlYfv27cjMzJRrop6amgpnZ2d4eHggJydHrgwaMWKE3P/QlGsTAMLCwpCSkoKzZ88iPz8fOTk5WL58OSIjIxESEiIXV9V8l6dcPnPmDJYvXw4nJydcvnwZBQUFpV4T8fHx+O9//4s5c+YgIyMDBQUF8Pf3R2JiIs6dOycXtyz++ecfXLp0CZ6ennj37h33O+/evcvdq2SVpU+9lZUV7ty5I7cfkpOT0b9/f3h5eWHQoEFc3Pz8fGzbtg2xsbE4fPgwd06lpKTAy8sLe/fuxdWrV+XSL6vXr1/j6dOnaN68ean9r3Nzc7Fq1SrUqlULXl5ecsetogQCASZMmICpU6fC19cXV69eRXZ2Nnx9fQEAS5YsQd26dfmbqezIkSMICAjAihUr0KRJE35whbi6usLLywt+fn44fvw4CgsL8euvv+LOnTtYvnw5LCws+Jt8FCKRCG3btsXjx4/x4sULfnCVlJeXh0ePHsHKygpxcXEYN24c1zLR0dERx44dq1DrB1K1aPQT0nfffQczMzOEhISAVbCZFVG/5ORknD17FgMGDIC5uTk/+LMycOBA/Pjjj2jQoAG0tLQgEAjQpEkT2NjYIDs7W64pdllVZtoAsHDhQq7y9sUXX6B169Y4ePAg5s+fD2dnZy6eSCSCt7c3evfujdq1awPFzc67dOmCZs2aIScnR2kl1NfXF35+frC0tIRAIEDjxo3Rvn17ZGVlcTeb8qatbu/fv0d4eDgcHR3h6enJDUwjEong5uaG4cOHIyoqSu4lQWV4//49Xr16BbFYDD09PX5wifjNXw0MDDB+/HjUqVMHc+fOLfdDoI6ODurXr48HDx7g7t27kEgk0NLSgqWlJebMmVPudAHA29tb7mGev1DfttKlpKQgNDQUo0ePljsWurq6GDt2LH788UdERUXJNScvC025NqUcHBwQGBgIFxcX6OjoQCQSYcyYMbC3t8fdu3e58rAy852bm4uzZ8/C0tISGzZswLfffguhUFjqNSEWi7FmzRosWLAA+vr6EAqFcHBwQJcuXSr00K2rq4u6devi3r17iI2NBWMMQqEQrVq1wvTp09Va0czOzoaPjw/MzMwwffp0uWbcycnJuHbtGmbMmIGBAwdy/9fIyAhTp05F+/btcePGDZnUyi41NRVxcXFo1KgRP4jDGMPx48cREhKCuXPnKoxroQ66urrw9vbGN998g7Vr12Lt2rUIDQ3F6tWr0bx5c350lT18+BAbNmzAlClTKmXQOqFQiOnTp2Po0KHYsGEDNm3ahM2bN2PFihWV8v/KwtTUFPHx8R8ccOxDNOV+wopfSPv7+8PJyQn+/v7coHEXLlzAwIEDsXXrVjCq23wWNLqybWJigt69e+P06dPV5m1XdXLx4kUkJSXB2dkZAoGAH1xtKXtAk/aZ6927N/T19bnKzsKFC/lRy6wy0+YzNzfH+PHjcfXqVSxdulRhwLsXL15g2bJlaNeuHZcPad/PknTs2BE1atTg/q5Zsya2bt2qMMBTedJWt+zsbMTFxeH48eMwMDDg8iEQCCAUCrFy5UokJydX+hQsWlpaEAqFyM/P5/o6yn6VjImJgbW1NX8zBd27d8fmzZtx4cIFdO7cmR+sMl1dXfz888/Q09ODk5MTWrRoAW9vb9y/f/+T9Nkl/5OVlYW3b9+iTZs20NfXlwsTCoVo0aJFhR9iNeHalGratCm+/PJLuXV16tRB48aNUVRUJPfwWln5zsnJQUJCAuzs7PDVV1/xg5WytLREv3795Cqoenp6pX6hVYWpqSm8vb2RmJiI9u3bo3PnzlizZg0SEhLU+iBfWFiILVu2ICkpCQsXLlS4N7x8+RL37t2Dp6cn91JYupiZmeHSpUsKY4CUVVFR0QfP45iYGKxZswbTpk2r1AqkiYkJ5s6dizt37mDJkiWYMmXKBwdsK43063ijRo0wadKkD/ZHLy9dXV3Mnj0bAODp6YlBgwahf//+EHziZ7jqOtijgYEBli5dimfPnqGoqAgFBQW4evUqbGxscPr0aSQnJ/M3IdWQRle2BQIBnJ2dkZycjODgYH4w+YTevn2Lo0eP4rvvvuOauX4OGGO4desWUPzQh+LK8E8//YThw4cjODj4gw8DZVGZaUvJjkaekJCAXbt2oXPnzkqbQQ4YMADe3t7466+/5MIqqjLTLguJRFKhr0zqIhKJYG5ujtjYWDx//pwfXCJ+89dLly5h+vTpJT7IKHtpIH0j///s3XlcjOv/P/DXVEpqOrQpW7Zk+RBZihyJKCcH2XfHEo5zLHHs4pAlji37vmffCoeIikL2LClUtrRS2lMz1++PX3N/m3tmqqmJOO/n43E/HtzXNVf33Nvc7/va+CwsLHD16lUEBASge/fuOHjwIJo3b47BgweXaYo4Dw8PqQdz/kLzkZZMeQ3+VlGuzaKIRCKZAbi+xnZraWnJ3Ce/hU6dOuH+/fs4f/48GjVqhJUrV6JevXpwc3Mr1YBXfIwxHD9+HMHBwfDy8pI73VZJAuGv4cGDBwgLC8O0adOkgv7GjRsjLCyMa8mlinmrRSIR1xXh7du3MuegMt6/f487d+7gxIkTUi96q1Spgu3bt8PPzw+GhoYQFDGgnTIkLz1iY2ORkZHBT/7qsrKy+KtKpaL9nvTt2xfTp09H7dq1uZfo7du3x+jRo5GUlFTureRIxfDtfyWKYWFhARcXF1y4cAEpKSn8ZPKNPHjwAA8ePMDAgQOlai1/dBERETh+/Disra3Rrl07oOChzs/PD87OzoiIiOBqV1hB/7+yKM+ylXXt2jWEhoZK9Qtlhfp+lkV5lq2MKlWqoG7duhgxYgTS0tK47Si83Lt3r9xfMFWuXBlWVlaIi4vD0aNHZUZBVpVXr16B8Wq/EhIS8OTJE6irq0PAq+3Q0tJC586dsWHDBm4O70uXLuHw4cNS+cjXU7lyZejo6OD27dsygVV+fj7Cw8NhaWkJQ0NDqbSSqijXZlHevn2Le/fuQV9fnxvttzy3W1NTE8bGxnj06NFXnaGgKDo6OnB2dsaBAwfw9u1bLFq0CF5eXrh69So/q9KuX7+OnTt3YtWqVQq7jBkaGsLS0lJq2jr+snnzZqkxQJQl+Rtv377lJ311z58/x7x58zB06FAcOXIEBw8exJkzZ2TupxVNQkIC5s6di0aNGsHPzw+PHj3Czp07v/lL5ujoaDRv3lzhi+HvTeXKlWFiYoLIyEiZOccZYzL3avJjq/DBtoaGBnr37s0Fd+Tby83Nha+vL+zt7dGsWTN+8g/p8+fPuHDhAkaNGoXQ0FD069eP6zeWlZWF+Ph4mJqawtjYGGpqakhJScHhw4dx7tw5flEcXV1dhIWF4ciRIwrfbpa27PIg+cGoW7cuhEIhxGIxoqOjsXnzZty9e5efXSnlWbYytLW10bZtW5w9exarV6/Ghw8fvlkzaScnJ9jb22PdunWYP38+oqOjuW3Jyckp03bVqlULHTp0wI4dO/Dvv/8iPz8fjDHExsZixYoVCA0NhZWVFfdg/OrVK7i7u+Px48dcTU6lSpVgZWWF+vXrl6lmpKL0sftemZiYoE2bNjh16hTWrl3LvZROS0vD+vXrsXHjRtjZ2ckMfqShoQGhUIhLly7B399f4cN2Rbk2JdLS0vDx40eIxWKIxWJERkbC3d0dqampcHJygrq6OlCG7S7JfVlXVxetW7dGQEAAZs6ciSdPnkhdQ15eXl+tcuDu3bvw8PBAVFQUdwy1tLTQvn17mJqalrm2+fr165g5cyb+/vvvIvsj16pVC61atcK6detw+vRpfP78mZ+lzExMTGBubo7nz58r/F6FB7srvEi63khacvG7MSlDErBK+kC7uLhg/PjxWLBgAa5fv87PXiLyBqNjBYNzTpgwAY6OjkhOTgZjrNTN1SV97h89eoRZs2ahW7ducHNzw6pVq3D8+HGwb/SiIDc3F+Hh4ahZs2apXwpKVJTfE4FAgJ9//hmPHj2Cl5cX4uLigILxWM6dO4cNGzagVatWqFWrFv+j5EfEH568IsrIyGC//fYb++2331hGRgY/mXxloaGhzNTUlJ06dYqf9EOQTInFnzICBdOdLFiwQOo8TEhIYL169ZLJK1kUTTVx9+5dZmZmJpO/8HQtpS27JIqa+kueoKAgZmpqKrMN8rabFZoWQ9G0MoUpU3ZRU/pIlsJTccib6qbwwp9SJTExkQ0aNEgmn2QpvL+KOlckS0m+vyI3b95UOPUOf1sUTf0lj1gsZt7e3jLTEEmWSZMmSZ3jRU0BZGZmVqbvWJ6K2m7Joqpt/5ZTf0mmHeJ/NxRMiSXvnChqOrfC1095XpvKKK5sT09PqWkQldnuwkpyX2YFUyYNGDBAJh/k3FMUXZvK3oPlKer+xp+qsLh9CN7xefHihcLzCnK+Z3BwsNxpHiVLWa81sVjM1qxZw0xNTVloaCg/uUiqmvpLMjUXf/q4mJgYZmtry6ytrVl4eLjUZ8qiJNd/SYjFYrZjxw4GgO3YsYObsuzTp09swIABUlMElkZxU27xr5/CoqOjmbW1NVuzZg23XRVRSa6fwudXRkYGmzRpkkweFFybqjxPSMVW4Wu2UdA8asiQIQgICMCzZ8/4yeQrEolEOH/+PGxsbGBvb89P/mHZ2NhgwYIFuHnzJhYvXiw1OIyxsTFWr16N0aNHQ19fH0KhEH369MHFixdx7NgxqXIKa926Nfbv348BAwZAX1+fnwyUoezy0LFjR2zfvp2b5s3MzAxubm54+PAhpk2bxs+ulPIsW1lGRkbYvXs3tmzZAjs7O37yV9W+fXtcvXqVq51EwajGXbt2xcqVKzFq1Cj+R0pEIBBg8ODB8PHxQZ8+fbhBmuzs7LBlyxasWrVK6hw3NzfH1q1bMXbsWK4ZqYWFBaZPn47AwMBS17QQ1WjSpAlOnDiB6dOnc7U2FhYWWLFiBc6cOSO3JkcgEGD8+PHYunVrked5Rbk2NTU1MXz4cEyZMgWtWrUCCgYfGjBgAK5du4aZM2dKDSpV2u0uyX0ZAKpXr469e/dK3Sf09fUxevRorFy5ssjPqlKbNm1w8eJFqe1t1aoVVqxYgXPnzpVp+qjExESEhobyVytka2uLS5cuSZ2HqiQQCODs7AxjY2P4+PgobI1RXlhB3/WNGzdi8uTJ6NKlC5dWt25dzJw5E+Hh4di0aVOFayZ8/fp1LFu2DCNHjsSgQYO4LkLVqlXDrFmzgIKZQ8oy/kZpiEQiHD16FGlpaT/cYLs6OjpYtWoVtmzZAmtra6Dgvjxv3jycPn26yJYi5MciYN+q3YiSUlJSMHbsWPzvf//DokWLuKZi5OuKiYnBkCFD4OrqirFjx/KTCSHkP8nDwwOxsbFYt26dSqdbIoT8H8YYNmzYgJUrV+LQoUNSAS/5/ty7dw/9+/fHH3/8genTp9OzPfkhfRc12yh4++bs7AxfX1+8evWKn0y+AsYYfH19AYB+4AghhBDyVQkEAgwdOhQ2NjaYN28enj9/zs9CvhMJCQnc1Hy//fYbBdrkh/Xd1GwTQgghFRXVbBPy9Tx//hyjR4+GiYkJtm/fLnc6MlJxffr0CXPmzMHjx4+xd+9ealJNfmjfTc02IYQQQgghTZo0wf79+9G8efNym2OelB8tLS0YGRlh165dFGiTHx7VbBNCCCGEEEIIISpGNduEEEIIIYQQQoiKUbBNCCGEEEIIIYSoGAXbhBBCCCGEEEKIilGwTQghhBBCCCGEqBgF24QQQgghhBBCiIpRsE0IIYQQQgghhKgYBduEEEIIIYQQQoiKUbBNCCGEEEIIIYSoGAXbhBBCCCGEEEKIilGwTch3IikpCYMHD0bHjh0RGRnJTyaEEELID+b69etYtWoVsrOz+UnkO3HixAnUqFED//zzD/Lz8/nJFU5mZib+/vtv3L9/n59ESuG7D7Y/fvwIJycnCAQCXLlyBdnZ2Zg4cSIEAgE8PDz42YkckZGRaNmyJRo1aoTHjx9L7VNvb29+dvKNvHjxAseOHUNISAju3bvHTy53ISEhEAgECAkJ4SfJJclfeCnpZwkhpDykpKSgb9++WLhwIUQiET+ZkArl1q1bmDJlCj58+ACxWMxP/u4wxnD27FnY2NhAT08P48ePR1JSEpeen5+PpUuXok+fPkhISJD67PcqOzsbV69eRVxcHK5evYrPnz/zs3wz3t7ecp/NsrKyEB0djT/++APPnz+XSiPK++6DbTU1NWhoaAAAqlSpAm1tbdSsWRMAYGZmxstN5FFXV4eGhgaqVKkCLS0tVKlSBXXr1gUA1K5dm5/9hycvSJS3fO0XEY0aNcKgQYNga2uLNm3a8JMJUVpGRgbc3d3h5uZGtSalJBaL8fTpU4wZMwZ6enoqvS+8fv0a06dPR926dSEQCODg4IDTp0/LrRlJSkqCp6cnrKysIBAIoKenhx49esjNzxhDWFgYxowZAwMDAy6vv79/mR/oJQ9v8paJEydKnWeMMTx//lzqO1pZWcHT01PqAZwvMzMThw8fxq+//goDAwO5ZSty9+5dPHjwAD179oS6ujq3XvLSmb/NAoEALVu2LHNrotzcXJw6dQoODg4QCASoW7cupk+fjnfv3vGzAkoee2XLVoZYLEZAQABcXFygp6cHAwMDjBkzBk+ePOFnBQqdh40bN4ZAIICNjQ22bdsm99goW7YylC1b2X2ozPEprdjYWMyfPx+2trZYtmwZdHR0+FmU3m5lKLMP5d1TXFxcEBAQIHVPuX//PqZNm4bQ0FCkp6dj586dWL16Nb58+QIAuHbtGg4cOAA3NzdUr1690F9Qzo0bN9CjRw/cunWLn/TVaWtro2vXrjA1NUXXrl3x008/8bNUOEZGRvjnn39gYmKCuXPn/jAvPr4Z9p3LyspiEyZMYABYcHAwY4yxJUuWMADs0KFD/OxEjuTkZObo6MgsLS1ZRESE3H36XxIcHMwAFLv8184vyX4pzTlRls+S8iO59idMmMCysrL4yaQIIpGI3bhxg7m4uJTLfSEsLIxZWlrK3HcAME9PT5aXl8flff36NevcubNMPsmyfv16JhaLufxnzpxhZmZmMvmEQiHz9vaWyqusQ4cOyZQrWfjnmeT84+cDwFxcXFhiYqJU2Ywxdv/+fWZrayuTn1+2PBkZGey3335jv/32G8vIyJBKi4iIULi/Jb+NpZWTk8MWL14sUy4AZm1tzcLDw6XyK3PslS1bGWKxmO3atYsJhUKZss3MzFhgYKBU/rdv37JffvlFJi8ANmnSJKl9rmzZylC2bGX3oTLHp7RycnLYjBkz5P59CWW3WxnK7MP8/Hy2ZcsWuXmFQiE7cuQIl3ft2rXM0tKSPX78mOXk5LC//vqLOTo6suTkZPb+/Xtmb2/PVq1axfLz87nPlIbkPkTPHEUrbj/dvXuXmZmZseXLl5f5mPyXffc12xKWlpYwNDQEABgbGwMAV+NNSsbExASGhobQ1NSEvr4+hEKh1Jv//wpbW1swxrglODgYAHDo0CGp9cOGDeN/lBDyH/HmzRv89ddf8Pf3x+zZs/HPP//ws5RaWloa1qxZg3fv3mH37t3IysqCSCTCrVu3YG1tja1bt+LRo0dc/qCgINy/fx87duxARkYGGGPIy8vDzZs3YW1tjeDgYKSlpQEA4uLisHnzZtSqVQu3bt1CXl4eRCIRIiIi0KlTJ2zYsAGvX78utDXKs7S0REREhNT9kjGGbdu2QVtbm8unpqaGdu3a4fHjx8jLy4NYLMb79+/h6uqKM2fO4MWLF1Llvn79GlOmTEFycjI2bNiA2NhYiEQiuWXL8+zZMwQEBGDIkCFyawkh5z7PGMOjR49gYWHBz1piQUFBWL16NVxcXPDixQuIxWJ8/vwZixYtQmhoKHbv3o3c3FyglMe+pGUr68mTJ1izZg1atGiB27dvQyQSISsrCzt27MCnT5+wefNmpKSkAAXNf7dt24YbN25gyZIl+Pz5M8RiMV68eAFnZ2ccPHgQ/v7+pSpbWcqWrcw+VPb4lFZYWBgOHz6MsWPHonHjxvxkQMntVpYy+1BdXR3NmjXD5MmTERMTA7FYzOUFgFOnTsltOp2fn4/Pnz9DQ0MDIpEIW7ZsQdWqVfHbb7/9J589K6JWrVphwoQJOHjwIJ49e8ZPJiXFj76/N2KxmC1atEjqzXNxb2qItMzMTDZu3Dju7SIraB1Q1rf5PwpJrWxRNVZLlixhjo6OLCkpid2/f58NGDCACYVC1rx5c3bkyBGZN91v3rxhHh4erFWrVtyb4rFjx7JHjx7J1Crxa4qKOi6HDh1ijo6OLDY2lh0+fJhZW1szFLzlPnXqFBOJRFL5c3Jy2MWLF9mAAQOYvr4+A8Ds7OzYli1bZGp+CtdOF/6OFhYWzMvLq8hapZLUbItEInbt2jXWp08fJhQKmb6+PnN1dWUxMTH8rEoJDg5mpqam7O7duywwMJArv6jtVub4lMexl7QuWbJkCbt16xZr27Yts7CwYEFBQSwiIoJ17dqVWVhYMB8fH6myc3Jy2MmTJ1nXrl25sufPny9VQ1i45UpRC/98V+b4HDp0iDtPY2JimKurK9PX12dmZmZs/fr1Mvs8Ly+PXbp0SarsIUOGsJs3b8rs74oiJyeH/fPPP+z+/ftMLBZz1yl/v5VGcHAwEwqFzMPDQ+b8OXXqFAPA1q5dy63btm0bMzc3Zw8ePJDKm5qayoYNG8ZGjBjB0tLSGGOMBQUFMQDs1KlTUnkZYyw0NJSZmpqyo0eP8pNKrPCxL61///2XCYVCduvWLW6dWCxma9askalVK6n8/Hzm7u7OXFxc2KdPn/jJXM22Ko5fYZLrzdLSUqam8dOnT8zFxYV16NCBvX79mjElj72yZStDLBYzDw8PZmpqyq5evSqVlpOTw6ZMmcJMTU3ZvXv3GCu0/1xdXWV+OyTnlZubG8vNzVW6bGUoW7ay+1CZ41Naubm5zM3NjTk4OLAPHz7wkxkrxXYrQ9l9qEhaWhobMWKE1P3A399fpgZ8+fLl7MKFC8zc3LzIZ4TiSJ4ziloKP+cy3nWfkZHB1q9fzywsLJhQKGSjR49mb9++lfobyjwzyWu5s2TJEqk8EpK8hw4dkvrN1NfXZxMnTmRxcXH8j5To976wxMREtmLFCu7ZQ19fnzVp0oShmGezp0+fsiZNmjB3d3eq3S6l7z7YJqS8lTTYdnBwYF5eXtwNWLIIhUJ29uxZLq+8G7BkadKkCQsLC5MqW9lg29TUlDk5OcmUbWpqyoKCgmTy8/NJlnnz5kk9TEj2w6hRo2S+I+Q0VS2suGC7qGZoZmZmCj9XEpK/PXz4cLnl87db2eNTHsde8iDVq1cvZm9vz+UZNGiQVDPN/v37s9TUVO4z8+bNkykXBS9bXrx4IVU2Pw9/KXy+K3t8JAHX5s2bmYWFhcxnCu9zsVjM1q9fL5MHch6MlCXpUqRoKepaUpYqg+1t27bJPMyKRCJ29+5drtl64WbTL168YNbW1szCwoLt2bOHpaamsqioKPb777/LBKc+Pj4MALt8+TK3TiI2NpZ17txZ4QNhSZQl2BaJRCwiIoI5OzvLNDtOTU1l/fv35wI2ZUkeGHft2sVPYqwcg23JPuVv9+fPn9natWu55vySa0iZY69s2cqQBEqF7zGs4P7h7e3NmjdvLnW+X758mQGQegEoFotZVFQUGz16NBMKhdz1rGzZylC2bGX3oTLHp7Rev37NOnTowObOnaswuFF2u5Wh7D5URFKOg4MDFwDm5eWxbdu2MQsLC6avr8/c3d1ZeHg4s7e3l/ktVlZZgm1PT082YMAAmfwDBgyQejmnzDOTvN97RfdWSd6ff/6ZWcrposA/p0r6ey8REREht/uNZCnqPJE8Mzg5ObGkpCR+MimBH6YZOSHfmr+/PxYsWABXV1du5NCrV69CX18fQUFB3AAgANCyZUvcuHEDWVlZYIxxTa7ev3+Pq1evSpU7bNgwrjnjoUOHpNLkiYuLQ0hICNeULy8vD3v37uXWF1a5cmWp5piFm/0FBgYiNjZWKj8A7N+/H2PGjMH79+8hEokQFhYGa2trnD17FnFxcfzsJfLw4UOsXLkSAwYM4Jqh5eXlISAgACYmJti1axcyMzP5H1PKoUOHMGHChBJttzLHB+V47H19fVGrVi3ExsZi3LhxOHbsGCpVqoR3795h1qxZiImJ4QaSunz5MjZu3Ij58+cjISGBK/vEiROIj4/HwYMHIRKJoK2tjW3btoExhuTkZDg6OmLChAnc9kiWwt0kSnN8wsLC8Mcff8Da2hpRUVEQi8V48uSJTLPm1NRUXLlyBUOGDEF8fDwYYxCJRIiKikL37t2hpvbf+5lKTExEvXr1YGhoCFYwiNjo0aPRtm1bnDlzBihoUp2VlQUAMDc3x759+2Bubo4xY8agatWqaNCgAd69e4fTp0+jU6dOXNk1a9aEqakpvLy8EBkZCbFYjPz8fDx58gQrVqxQyVQvYWFh3ABZdevWxaBBg3Dp0iWFTVo9PDwgEAigrq6Oxo0bw8rKCitXrpRq6p2UlISYmBg0aNAAJ0+e5AaEKmrQJgnGGPz8/KCvr4+uXbvyk6UMHz4cgoIB5hwcHLBp0yZ8/PiRn63E0tPTkZKSgtq1a0NTUxOZmZnYu3cv2rVrh+nTp+PNmzdAwfGEksde2bKV8eXLFyQmJqJ27drQ1tZGbm4uLly4ADs7OwwbNozb39HR0UDBdpuamnID1L59+xazZs1Cy5YtsXfvXqSnpyM+Ph7JyclKl60MZctWdh8qc3xKKy4uDk+ePIGlpaXC5tTKbrcylN2HisTExODevXto37499PX1gYLunRMmTEBERAQ+fvyI+fPnY/fu3TA0NMTIkSMhEAj4xZRY4S6Akmel4OBgqd+1S5cuwcDAgP9RzJkzB69fv8bly5eRl5eHtLQ0/P777wgODpb6nso8MxkYGODSpUtSv7XFuXHjBnR1dbmm+69fv4azszPu3bsnVXZJf+9RMIjeli1b8Pz5c+zevZvrapSbm8s19S+KtrY2WrVqhaioKHz48IGfTErgP/cUo2ik6ZKOZPq9UDQiLE2HVn5MTU2xc+dOLF++HKamphAIBGjVqhX+97//cf26UHAD9vT0RMeOHbk+htra2ujUqRPq16+PjIwMXsnKEQqFWL58OebOnQs9PT1oaGjA3t4eHTp0kBkptV+/fpg8eTJq1KgBNTU1CAQCmJubw9raGpmZmcjJyZHKDwBeXl5YsWIFatasCTU1NbRo0QKjR4/G/fv38fbtW372Erlx4wZMTEywYMECbnRXDQ0NdO7cGaNHj8aTJ09kAmJllXS7S3N8yuvYm5qaYty4cahRowbq1KkDoVCISZMmoVatWtDV1UV+fj5EIhG+fPmCoKAgODg4YMaMGdy4Fdra2nBxccHQoUPx4MEDLsBVVmmPz7p167Bjxw7Ur18fAoEADRs2ROvWrZGens6di5qamjAyMkJ4eDgeP34MsVgMNTU11K9fH9OnT0e1atX4xZaYu7u71IMWfylrX9zyJBQKkZSUhClTpsDa2hoHDhxAz549ERQUhHHjxvGzw8DAAM2aNYNQKOTWvXz5EtHR0WCMcev+97//YejQobhw4QIaN24MdXV1VKpUCS1atMCmTZuQnp7O5VWFN2/e4Pjx4+jRowdmz54t81JGHg8PD8yePRufPn3i1olEIuTn5+PPP//EsGHDuBdTnz59wt69e/Hrr78qnBIxPj4eFy9eRN++fUs8S0l6ejquXr2KyZMno3fv3mUejfynn37CqVOnYG9vjzFjxiApKQlr167FhQsX+FmVPvbKlK0sPT09hISEoHfv3ujZsyciIiIwe/ZsBAQEwNLSUiqvsbExsrKysHTpUrRq1QqrV69G48aNcfnyZSxYsEAqL5QsW1nKlq3MPlT2+CjrzZs3SE9PR61atfhJMpTZbmUpuw8LS0hIwMKFC9GoUSP88ccfCl8aXLp0Cb6+vpg1a1aZ7vVlNXjwYPj6+qJbt27Q0NCAUCiEnZ0d4uLipJ6DSvPMpIzu3btj7969sLa2hpqaGszMzNC1a1futx4FL0OU+b1//fo1goKC8Mcff2DUqFHcS0xNTU1UqVKl0F9XzNTUFC9fvlT578N/xX8u2CakvBgbG6Nly5ZSNXHVqlXD+fPnZQbviYyMxIwZM7jaH4FAgMaNGyMsLIzLU1r169fnfjAkdHV1pR7CJbKzs3HkyBH06NEDenp63LYsXLiQn5XTunVrmcEHmzRpgvT0dO7HQBk5OTl49eoVQkNDuaCs8DJx4kS8ePECycnJ/I8qRZntVvb4lNexb9GiBZo1a8b9v0OHDmjdurVUHhRMhfTixQucOXMG+vr6UvtPQ0MDK1asQHx8fKlq6MpyfNq2bQstLS3u/5UrV8bmzZulahd0dHQwZ84c6Orqonv37mjatCnc3d3x9OlTiMs4DdX3zM/PD23btsWmTZvQoUMHXLt2DT4+Pmjbti03XaPkfEtISICrqyt27tyJxYsXIzk5GQcPHgQAjBkzBkePHuUCbi0tLXh4eGDr1q2wtrYGCqbJdHNzw+nTp2FpaSlznSijcEscSW3LtWvXYGtriz179uD27dv8j3AvRUQiEd6+fYvZs2fj4MGDWL9+vcy12bZtWxw/fhypqalgjCEjI4MbtMnX11cmPwqmE4qNjYWjoyMECmrOLCws8OjRI2678/Ly8OrVK4wfPx4hISHw9vaWW3ZJjR07Fv3790dUVBQ8PDwQEREBNzc3bhqgwvtcmWOvbNnKWrx4Mbp06YKbN2/izz//xOPHj+Hp6QlTU1OAV3ZYWBg6deoEd3d3rgXCjRs30K1bN2hqakJDQ0Mq6FKmbGUpW7Yy+1DZ46Ms/kvxoiiz3cpSdh9KfPr0Ce7u7oiPj8eKFSsUTuH1+vVr/PPPP/jjjz+gp6eHvn37QlAwjZq8+0R5atq0KUxMTKTWDRo0CIwx2NracutK88ykjI4dO8Lc3FxqnSSYllD29z45ORlhYWFo1qyZwpcexZHXGoCUXOnvBt8p/kjTkoX/QPy94z/wSBZ3d3d+VvKVhYSEwNHREWvXri1zbUlZZGdnY9asWRg6dCguXbpUpjeWipqHloTkIftbkLfd5Xl8yqtsSVPg8vA1jo+FhQWuXr2KgIAAdO/eHQcPHkTz5s0xePDgMs3vKWmerGhRxfzJ5UFXVxcoCCzPnz8PHx8f2NvbQ01NDRkZGYiKioKZmRm0tbUhEomwefNm+Pj4wMvLC9OmTYOBgQGGDx+OEydOoH79+ti8ebNU6w1tbW1MnDgRt2/fBmMMr1+/xtq1a1GzZk0kJiZKveApK21tbdjb22P8+PFcU2JF1NTUULt2bcyfPx99+vTBnTt3kJqaKpXnzz//xIABA7hgQkdHB4MHD0afPn2QmJgo1WUDAFJSUnDq1Cn88ssvaNiwoVRaUTQ0NNCgQQO4ubmhefPmcssuicqVK0NHRwdCoRB//fUXHj58iAULFsDIyAgAEBUVBaFQyNW4K3PslS1bGRoFtXsAMHLkSISGhmLDhg2oW7cuUPCCJywsDPXr1wcKantRcC3v2bMHAQEB6NevH7S0tJCVlYW3b9/CxMQEBgYGSpetDGXLVnYfKnN8ypOy260MZfdhYbGxsZg4cSLCw8Oxe/duNGnShJ8FKPjt3bRpExo0aAAXFxcsWLCAa4Z/9epVuLu7y20t9S2p8pmpLEr7e1+Wc7Is3SLIfzDYJuRbEolEuHDhAj59+iQ1TQ9jDBEREcU2zVKlly9fws/PD87OzoiIiICoYAodxhiWLFnCz66QSCTCjRs3YG5uLrf2vDja2tqoWbMmOnfujNjYWJkXRIwxpKWlwcbGhv/RMpG33eV5fMqz7CpVqqBu3boYMWIE0tLSZPYfYwz37t1TKtiQ+FrHR0tLC507d8aGDRsQGRmJXbt24dKlSzh8+DA/6w+vRYsWEAqFcHFxgZOTk1TrgIcPHyI0NBQNGzZE5cqVuf6VHTp0wM8//yxVc2thYYEuXbogNja22GmUMjMzsX//fjRq1AgtW7bkJ5cJY4xrPl6Se4S6ujq0tLSkagj19fVhZGSEZ8+eyTxo5ufny3TBkHjw4AEePHiAgQMHSu3HksrOzkZOTg6qVKlSqlohQ0NDNGrUCPXr18e4ceNQp04dLi0tLQ1Xr15F8+bNudpCZY69smUrQ1dXF02aNIGpqSnGjh2LJk2acOdWfn4+/Pz8YGpqyt1TGjRogCZNmqBz584YPHiwVH/7V69eISQkBI0aNYKOjo7SZStD2bKV3YfKHJ/SkgS179+/5ydxlN1uZSi7DyWePXuGQYMG4fPnz9i7d2+RL+3OnDmDy5cvY8aMGUhKSkJwcDD27duH/Px8nD9/HqGhoVy/84pCVc9MZaXs733lypVhamoq8/IiMzMTN2/elFqnSHR0NJo3b0413KVEwTYhX1FeXh4+f/4MXV1d7u23ZHCiTZs2FTvgiCplZWUhPj4epqamMDY2hpqaGlJSUnD48GGcO3eOn52TlJTEjW8geUDfvHkzunTpItP8qaQ6dOiA+/fvY/HixYiKipJ5mFaFwtudlpaGnTt3ymx3eR6f8ixbW1sbbdu2xdmzZ7F69WpukLbiSGowLl26BH9/f4X7vTyPz6tXr+Du7o7Hjx9zNYeVKlWClZWV3H7syqiIfbbz8/Pxzz//oEaNGli7dq3cfdm8eXPY29tj+/btOHLkCLKzsyEWi3H79m0sXLgQ9evXxy+//AIAEBQMLBYTE4PAwEAuqM3Pz8ejR4/w6NEj1KxZU25/SMYYUlJScPXqVQwePBgHDx7E+PHjUbt2bX5WMMZw9OhR1KhRAzNmzChR32sU1Czv3bsXK1euhL29vdxuEBJisRhJSUnYvn07Tp8+jW7duqFq1apAQbDdvn17HDt2DEePHuWu5ZSUFHh5eeHMmTNo27atVO1Nbm4ufH19YW9vX+SDvzxfvnzB9evXMWfOHMTHx8PJyQmampr8bIiMjETHjh3h4OAgt5WEjo4OHBwcEBYWhsWLF3N96NPS0rB27Vquv6+kf64yx17ZsgtLSEjAwIED0aJFC9y4cYOfDHV1dTg4OAAAli1bhnv37kEsFiM7Oxt79+7Fxo0b0atXLzRt2hQo6L7UqVMnHD58GJs3b+aCgJcvX2LevHlITU1Fnz59oKmpqXTZhal6u5Xdh8ocn9KqVasWOnTogLCwMIWtipTd7sIk+7Bx48Y4c+YMWKExHVCKfcgYQ0hICIYNG4YaNWpgz549RT4LvHz5EuvXr8fkyZPRvHlz5OXlISMjg+s/LK9LkrIkL/X2798vE2SWVmmfmVRN2d/7GjVqoFmzZti7dy/u3r0LsViM6OhouLq6YsuWLfzsMnJzcxEeHo6aNWvC0NCQn0xKgj88OSFEWkmn/irpdDdFTR0B3tQQ8qaO4C+Fp2xQNO2OpJzCZSckJLBevXrJlCdZ+OUUNa2GtbW11FyfJZleqvD+zMrKYtOmTZPJI1n4014oQ5ntZkoeH1ZOx16y/wpPUyKZz7vw/wv/3cTERDZo0CCZMvllF1bUtFtlOT6S71nUdCISkqlX+GVCwbRiFUVJznF5+1wyhRUKpuNTNFdtYGAgN31P4UUoFDJvb2+pKXIU5VWUX96UaEKhkG3cuFFm7mAJydy/kvz8Od4l5JWNgvvJzZs3pfIWdW1OnTpVZt7a8PBwZm1tLZMXgMxUYazQ/M7y5hTnU7Qtxe0XyVRqAGSmYZLIyMhgkyZNkikbAOvduzeLj4+Xyq/oeMo7lsqWLXHv3j1mamrKwJtCsLC8vDzm6ekpUy4U3DuLOj6enp5S+1DZsiXKY7uV3YfKHJ/SkMxlbW1tzaKjo/nJHGW3W6Lw7xD/3i2hzD4syXOK5O9kZWWxP//8k40cOZJ9/vyZMcbYhw8fmIODg1R+/pRbyoqJiZE71ZWiqb/k3av5lH1mKu73vvC+l/eMVrgcftnK/N4r+p23sLBg7u7uDMX8VkdHRzNra2u2Zs2aMp/b/1VUs03IV9a/f3/s3r2bG5zIwsICHh4eCAsLQ//+/fnZy42xsTFWr16N0aNHQ19fH0KhEH369MHFixdx7NgxfnZYWFhgy5Yt6NmzJzeNR6tWrbBixQqcO3dOYd+sktDW1oanpydOnjwJJyenEjU1LYuitrs8j095lm1kZITdu3djy5YtsLOz4yfLJRAIMH78eGzdurXIz5Tn8TE3N8fWrVsxduxYrn+hhYUFpk+fjsDAQKnBaX4EP/30E37//XeuWZ+iqXk6deoEHx8fmevz3LlzGDJkiFRzcTs7O/j5+WH69OlcTb2ZmRnGjh0LPz8/DB48WCq/hFAoRNeuXbFy5Uo8fvwYf/zxh8JBjzQ1NeHq6sp1d4iIiOBnkSEUCuHk5IRdu3bh+vXraN++PT+LFDMzM4waNQoBAQFYvXq1VFNkFAxoePToUbi5uXHnip2dHfbt2yeTXyQS4fz587CxsYG9vX2hUkqmVatWWLBgAR49elTkfunatStcXV2BgqaW8mr8dXR0sGrVKmzZskXq2l+xYgX27NkjM4CUMsde2bIlLC0tMWnSJADAu3fv5A6eqKGhgWnTpuHkyZPclGmSwfROnDghc++Ud3y6du2KU6dOYcaMGVL7UNmyJcpju5Xdh8ocn9LQ0tJCv3798PbtW/j7+8vUPEsou90SDg4OGDZsGCwsLODo6Ci3ybuy+7CkfHx8cOvWLcyZMwd6enpAwUjXHh4e3N9xcXHBsmXL5LbGKam6devK/K6UlbLPTOVJmd97QcFAptu2bYOFhQX09fXh6uqKS5cuFTsdmUgkwtGjR5GWllbkAJOkGPzomxBCfiSSGqui3twS8jUlJiYyBwcHJhQK2a1bt/jJFVZmZiYbN24cA8COHj3KT65QJLUxu3bt4iep3P79+xkKatezs7P5yRVWUFAQA8CcnZ3LVIv4tX2v262MnJwcNmPGDGZpaSlTE0/I13L37l1mZmbGVq1axfLz8/nJpISoZpsQQgj5SlJSUrB//36EhobCycnpq/cZL63MzEycPn0aV65cga2tLVeTVhExxuDr6wsA6NKlCz9ZZfLz83H37l14e3tDKBSie/fucmsJKxqxWIyXL19i27ZtACDVN74i+163uzS0tLQwYcIEVK5cGXPnzi3TrAyElEZCQgKWLl0KKysr/Pbbb6UaIJIU4EffpOg+hPIWyZv+kiyDBg1izZs3l1mvaFGmbEV9bwj5L6OabVJRFO7PLK8Pc0VVuO+hmZkZO3Xq1H++7x7/OWHevHkyfcYrIn7/2hEjRrDExER+tgrne93uspL0D5c3JgEh5eXjx4/M1dVVpn8+KR2q2SaEEEK+Esl4AVeuXCm2D3NFUrgffd++fanvXkE/9gEDBuDatWvw8PCQ6WNeUQkEAjg5OeHkyZPYuXMnNzdzRfe9bndZ2NnZ4cCBA2jYsKHCcQMIUTUtLS0YGRlh165dpe6fT/6PgCkaeYEQQgghhBBCCCGlQjXbhBBCCCGEEEKIilGwTQghhBBCCCGEqBgF24QQQgghhBBCiIpRsE0IIYQQQgghhKgYBduEEEIIIYQQQoiKUbBNCCGEEEIIIYSoGAXbhBBCCCGEEEKIilGwTQghhBBCCCGEqBgF2+SryM7OhpubGxo3bozr16/zkwkhRCl0TyGEEEJIRUfBdhE+fvwIJycnCAQCbvHw8OBnK3eS7fgWf1tV3r59i4CAAERGRiIoKIifDBQ8PE+cOFFqf0+cOBHZ2dn8rACAkJAQqbwCgQAhISH8bKQI3t7eUvuvZcuWiIyM5Ger0LKzs7Ft2zZ07ty5ROfCkydPMG7cONStW1fueVZRrvuieHh4fJfHShmS46DoHlCSewohRD7J722NGjVw//59fjIhhBAVqVDBtuTmr+jhShJcKXqI/h4dP34cffv2/S4emiWBWVH7f/v27ejWrRuSkpKk1tepUwf29vawsLCAnZ2dVBpRve/pvCqL/Px8eHp64vfffy9RwPX48WOMGDECu3fvxps3b/jJ5DtSUe4p8fHx+P3337Fy5Up+0lf3X7nuv6UrV65AIBBg6dKlYIzxk4GC+0yjRo2wcOFCiEQimZeaAoEAVlZW8PT0lPmtLOzOnTuoUaMGBAIBdu/ezU/mXkjp6enh6tWrUmkxMTGwsbEp9jfb2NgYurq6/NWEEEJUpEIF2xWNgYEBLl26BMYYkpOT4ejoyM9SZpGRkYiOjuavrpCEQiF/lYzExESoq6tDTU361NLW1sa6desQERGBTp06SaVJaGtrY9u2bWCMISsrCxMmTOBnkWJrawvGGBhjCA4O5if/p5X0vBo2bBi3Dw8dOsRPrvCioqLg4+ODwYMH482bNxCLxdz3sbW1lcrLGIOvry8SExNx+PBh5Obmcnm3bdsGbW1t4Ctd96TsSnJP+Ro+f/6MW7du4cuXL/ykr66k1z0pvWbNmqFz58548OABUlNT+ckAgFu3biE+Ph6Ojo5QV1fnJwMAHj58iLlz58LV1RUJCQn8ZO53rX379hg5ciSCgoKQnp7OzwYASE9Px61bt6SC/5s3byI0NFQqX2GVK1eGiYkJfzUhhBAVo2CblJiBgQEAQCQS8ZOkCIVCaGho8FcTonLJyckICwvDkCFDUKdOHQgEAn4WTk5ODt6/fw9bW1v88ssv0NTU5GchhJAiVa9eHe3bt8ft27fx8uVLfjLS09MREhICW1tbWFhYSKUFBweDMQaxWIyoqCgMGTIEPj4+8Pf3l8qHglrrK1euoHv37ujduzdu376NmJgYqTzJycmIj4/H7Nmzcfv2bXz8+BEoaCV448YNLFq0CJaWllKfkRAIBApfBBBCCFGd7zrYljQ79/DwQFJSEhYsWIC6detCT08PAwcOREREBP8jyM/Ph5+fH1xcXKCnpwcDAwMMHTpU5q1waTDGEBYWhjFjxsDAwAACgQCdO3fGkSNHkJuby+Ur3KRs4cKFCAsLQ+PGjaWamClqSp+UlIT58+fDwMAABgYG+OuvvxQ2Q2OMYe/evahTpw6OHj1a5u8n8e7dO/4qoCAIz87OhoGBARfIfM/9qrOzs3HkyBE4ODhAIBCgbt26mDBhglQTzcLn4O3bt9GuXTtuwKbIyEg4ODigcePG8PX1lSr77du3WLp0KaysrLiyx40bh7CwMJnj5OHhAScnJyQnJ+PBgwcYOHAg9PT00KJFCxw9ehT5+flAGc+r4kiaJI4ePRqZmZn8ZK6549q1a2W2v6Ryc3Nx6tQpbn9Lrs0HDx4UW6bkRVBJFD4/VaGk172EMsceAF6/fo0ZM2Zwx7Ju3brYt28fP1upvH79GhMmTJDqvy7vWi08bkRubi6OHDnCNVHt27cvHj16JFWuWCzGrVu3pPrGF9VsViwWIyAgAAMHDuT24c8//4y7d+/ysyp9T5GUXfieP378eLx+/ZqfFd7e3nBycsKHDx+kvqONjQ1Onz4NsVgM8Pr1N27cGGFhYVi4cGGx21WSe4qySnvdv379GtOnT5c6Pl5eXnKvb2V9+PABS5cu5bbDysoKa9euxefPn/lZ8fHjR2zatIkbc8HAwAADBw5EQEAAt78ZY1i7dq3c5tIouHdMnToVNjY2UgGpMse+pNTV1dG1a1dkZGRwwXNhMTExuH37Nrp166bwviQQCFC/fn1MnDgRAOS2RoiMjMTz58/Rpk0bNGvWDFWqVMGtW7f42QAAVlZWyMvL486j6OhoJCYmct0r3r9/z/sEIYSQr4ZVIFlZWWzChAlswoQJLCsri5/MgoODGQAWHBwslb9t27bMxsaGAZBaevXqxRISErjPi8Vitn79epl8AJijoyNLTk4u9NekJScnM0dHR7ZkyRJ+EmMFZXt7ezOhUChTNgA2adIklpGRwRhj7NChQzLp/KXwPpD8bWdnZ2ZrayuTd8aMGSwnJ4e3Rf/3OQCsd+/e7NOnT/wsSomIiGCWlpbs0KFDjDHGPn36xFxcXNi2bdsYU3D8JMes8CI5fkWRV1ZR+OdGWeXk5LAZM2bIbDv/2Ei2s1evXsze3p7LM2jQIPbLL79w/+/fvz9LTU1ljHdc+EuTJk1YWFiY1LYsWbKEOTg4MC8vL6avry+VXygUsrNnzzJWivOK79ChQ8zS0pJFRETwk1h+fj5zd3dn5ubmMtsnFouZh4cHa9KkCXv69KlUWkllZGSwSZMmyWyv5Dt6e3szsVjMWMH+4OfhL5LvKTln+en8RdF5o8rrnpXi2AcFBTELCwuZvAAUHquSCg8PZ9bW1jLlFl4k+0Wy3dOnT2dTp06VyWdra8tiYmK4suVd95Jl5MiR7PPnz1zevLw8tmrVKoX7kH/Oyitb0fHLz89nW7ZskVu2mZmZzOcOHTrETE1NmZOTk0x+U1NTFhQUxFgxx1HRdpX0nqKs0lz3gYGBzMzMTCYfCn4r4uPjpf6GMmJiYuT+TgHgfjskJPdPfj4UXPd+fn5c3qdPn7ImTZowd3d3lp+fL1VOdHQ0s7a2lkpT9tgr48OHD8zBwYG5uLjI/K5u27aNmZqastDQUG6d5Bjx/+bly5cZALZ8+XKp9ZJ7qqR8yX4aMWIES0tL4/JJ7m/BwcHMw8ODeXh4MLFYzHbt2sXc3d3Zs2fPpH6z+ZYsWVLm+wghhJCifdc12xJ3795FSkoKzp8/j7y8PHz69Anjxo1DQECA1Bvj1NRUXLlyBUOGDEF8fDwYYxCJRIiKikL37t1l+hkr48WLF1i1ahWaNm2KmzdvIi8vDyKRCBEREXB2dsbBgwcRGBgI8PrJLlmyBJaWloiIiODWMV4fUokLFy4gPz8ft27dgkgkwuvXr+Hs7IzAwEC5tc1Vq1aFi4sLateujcGDB6Nq1ar8LKWSmJgIFLw9v337Nm7fvi1VG1KzZk1u27/XftXv3r1DYGAgZsyYgYyMDDDGkJeXh8ePH+N///sfPzt8fX1Rq1YtxMbGYty4cTh27BgqVaqEd+/eYdasWYiJiZGq0WvZsiVu3LiBrKwssII+6jt27MD79+/l1tz4+/tjwYIFcHV1xYcPHyAWi3H16lXo6+sjKCgIX758KfV5VRLq6uro2bMnMjIy4O/vj8K1OfHx8QgKCsIvv/yChg0bSn2upM6ePYstW7bA1dUV79+/h1gsRm5uLnx8fKCvr48dO3bIPce/NWWue4mSHvuUlBSsX78eKDi/JH3Ms7KyMG/evEIlKk8kEuHIkSOIj4/HwYMHkZWVJbXdLi4uSExMlOn3vnbtWly6dIn7TE5ODpYvX46QkBA8fvyYy6euro7Fixfj1atXyMvLA2MMHz58gKurK65cuSLV/PbBgwdYt24dHBwcEBERAZFIBLFYjJiYGHTt2pXLJ6HMPeXhw4dYuXIlBgwYgJiYGIjFYuTl5SEgIAAmJibYtWuXTE1uXFwcQkJCsGTJEnz+/Bl5eXnYu3cvtx68fv0RERGwtLTEkiVLpK41xhs3QNl7Skkpe90nJiZi7dq1AAAfHx/k5uZCLBbj/fv3cHV1hY+PD06cOMH7KyUXGhqKkJAQnD17ljv2GRkZOH/+vNxxP2rWrIl///0Xqamp3D7x9fWFvr4+Ll26xPWFb9iwIbp164YbN27I9HG+efMm3r59i549e3JNo0tz7EvK2NhYblPyopqQF5afn4+7d+9i9erVMDMzQ7du3aTSP378iJCQEFhZWaFq1arQ1tbGzz//DH9/f7x48YLLl5KSwj3j2NvbIywsDO/evcP169eL7C9emIaGRonyEUIIKZ3SR5cViKWlJfbu3QtnZ2doaGigWrVq+PXXX5Geni7Vv1hTUxNGRkYIDw/H48ePIRaLoaamhvr162P69OmoVq2aVLnKCAwMRHR0NJYsWYL27dtDQ0MDampqsLCwwPLly1GrVi2EhIQU29+5KLa2tti/fz9sbGygpqYGMzMzdO3aFfn5+XLLVVdXx4QJE/D27VsMHjwYgiL6s5aEUChEtWrVkJGRAQC4d+8eOnfujA8fPuD9+/f48uUL12fse6ejo4Nq1arhyZMniIyMBGMMGhoaaN68Of7880+ZgNXU1BTjxo1DjRo1UKdOHQiFQkyaNAm1atWCrq6u1DEyMDCAp6cnOnbsyJWjra2NTp06oX79+tz+LczU1BQ7d+7E8uXLYWpqCoFAgFatWuF///sfFyiVt2bNmsHR0RHBwcFSAwM9ffoUz549w8CBA6GlpSX1mZJIT0+Hn58f7O3tsWjRItSsWRMCgQCampr49ddfMX/+fAQFBXFNJN3d3WWCLUlzTn5wYWFhgUePHoEVGnRvwoQJXKArWfhBZUkpe90rc+zDw8Ph7++PmTNn4tdff+Wavmtra6Ny5cpcvtL48uULEhMTYWVlhZ49e0JbW5vb7r59+yI6OhqfPn3ifwz29vY4c+YMhg8fDm1tbWhpacHe3h5CoVBq8CYbGxssXLgQDRo04MZvMDU1hZ2dHeLi4pCTkwPg/zcPvnz5MqpWrQoPDw9YWFhATU0NAoFAJWM/3LhxAyYmJlwXI4FAAA0NDXTu3BmjR4/GkydPEBcXJ/UZoVCI5cuXY+7cudDT04OGhgbs7e3RoUMHrstGaSh7TykvYWFh8PX1xfz587nzSiAQoGbNmli0aBHs7e1x584dhYNxFcfQ0BBCoRDBwcFISUkBCr67s7MzevXqJZVXW1sb7u7u6NGjB3766SegIPjr0KEDGjduLHVv09LSgp2dHQIDA3Hv3j2uDEn/ZAcHBzRp0oRbX5pjX1Lq6uqws7OTaUouaULeoUMH7vsU1rFjRwgEAlSqVAnt2rXDvXv34OnpidatW0vlCwsLw4MHD2Bvb8/9brds2RJVq1aVmlNeJBLBxMQEQqEQFhYWyMvLw5UrV5CTkwMLCwuZ32w+DQ0NGBoalunZhxBCSNF+iGDbxsYGLVu2lFonr6+Ujo4O5syZA11dXXTv3h1NmzaFu7s7nj59yvUNK63ExEQ0b95c7tvsmjVrok6dOvj06VOZRqx1dHSUKd/Y2Fjq/+VJS0uLC6ays7Px8OFD9O7dG3p6erh37x7y8/ORnp7+VbepvJiamsLd3R1v3rxB69at0b59e6xatQqvX7/mHqwKa9GiBZo1a8b9v0OHDjIPUIVFRkZK9cMVFOr7KY+xsTFatmwp1fqiWrVqOH/+fKlrq5Wlo6ODX3/9Ff7+/njw4AFQ0Ffy/PnzcHR0lPr+ypAEfi1btoSRkZFUmkAgQNOmTYFCLSoqktJc9yU99pJ+paXdr0VRV1dHlSpV8ODBA/j5+eHLly8Qi8V4+vQpjh07hmrVqsmthWzUqBHq1q0rtc7GxgZpaWkYNmwYt04yNkbhPtgCgQDDhw+X+qxk0Lr69eujRo0aUmlllZOTg1evXiE0NBT169fntkGyTJw4ES9evEBycrLU5+rXr49u3bpJBfq6urpy94cylL2nlJfExESYmppyYwYUpq+vj0aNGiExMbHUv1V2dnZwd3fH9u3b0aBBA7i4uODo0aNIS0vjZwUK9e+WbI9AIIChoSH8/Pz4WdG+fXs4ODggMDCQ277o6Ghcv34ddnZ23DEq7bFXRtOmTWFtbY1bt25x303Sp1peiwy+Zs2a4cKFCzIvwkUiEYKCgtCiRQvUr1+fW1+nTh20adMGN2/elOr7XqVKFWhpacHAwABt2rSBu7s7rK2tYWBgwP1mK7p31qlTh7+KEEKIilXIYPvjx49yf+hVUXtnYWGBq1evIiAgAN27d8fBgwfRvHlzDB48WKZpmrI0NTVRqVIl/uofhpqaGjQ0NBAfH483b97g48ePsLa2Rvv27XH//n3umP0oc3Z26tQJ9+/fx/nz59GoUSOsXLkS9erVg5ubW6mbH6JggCdHR0esXbu2TAMjfQvt27eHtbU1goKCIBKJ8OHDB4SGhuLXX3+Fjo4OP7tSKleu/F02Z1Tmulf22Ovq6pa4bGVoampizJgx0NPTw+DBg6GlpQV1dXU0b94ct27dgqurK0xNTfkfKxGRSIR169bByckJJ06ckFtDziepWVclVtBNqCIpr3uKssrrvEJBbenMmTPx/PlzrFy5Eunp6RgyZAjq1asnM1Dny5cv0bdvX7i7u+Phw4dS5chjYmKCHj164ObNm4iNjQUA3L59G3p6eujSpQuX72scexMTE9jZ2SEkJASvXr3impDb2NigXr16/OxAQQscsViMFy9eoG7dupg2bRqeP38ulScxMRG3bt2Cv78/N8e2QCCAnp4eDh48yP098AY+EwgEsLe3h6amJleDTggh5NtT7dONirx7907mAY0xhnv37kEoFJb5gVxLSwudO3fGhg0bEBkZiV27duHSpUs4fPgwP2uJ6erq4v79+zI/nAAQGxuLt2/folatWmVu/vktVa5cGbVq1eL6dhoYGKB69eqwtrbGnTt3EBkZifT09DI3/axIJM0fDxw4gLdv32LRokXw8vKS26+6JEQiES5cuIBPnz5hx44dXN9NVqjvZ0Umedi9evUq3r9/j2vXrkEoFKJ9+/b8rCWmoaEBoVCIu3fvytTAMMYQHh4OoVAoU6NaEShz3St77IVCIeLi4mTuhQkJCbh//77UOmWJRCJcvHgRampqGDBgAMzMzCAUCuHk5ITTp0/L1LYpIyEhAf/++y+sra25fuyS78mfy11Sw56cnCwTbD59+lTujBIlpa2tjZo1a6Jz586IjY3ltqHwkpaWBhsbG/5Hy5Wq7ynKEgqFePnypdyR3j99+sQFglWqVOEnK6VmzZr4/fff4e/vj6ioKLRv3x4bN27E27dvuTzXrl1DaGgoFi1ahE+fPnHHRdH89gKBAI6OjkhLS8PDhw+Rnp6OoKAgdO/eXaqW9msce0lwm5GRgevXr3NNyAvXsMsjEAhgbm6OWbNm4e3bt/Dy8pIaKT48PLzI+bHj4uK4puT8bg22trZ4/fo12rVrJ7VekWHDhuHSpUtyWwISQghRjQoVbEv6WIaGhmL16tVcf6ovX77g3Llz2LBhA6ytrWFmZsb/aIm8evUK7u7uePz4MVcLW6lSJVhZWcn0l1SWra0tdHV1sWjRIty+fRv5+fkQi8W4d+8eZs6cidTUVHTv3l3mAVZXVxdhYWE4cuSIwmZ2pcXKaeqvhIQEnDx5Eq1atYK2tjbq168PbW1t/Pvvv8jMzEStWrX4H/nu3L17Fx4eHoiKiuIeaLS0tNC+fXuYmpqWuj9jXl4ePn/+DF1dXZiZmUFbWxv5+fl48uQJNm3aJHcKmNIor/NK8rCbkpKCixcvIjg4GAMHDoSJiQk/a4np6emhffv28Pf3x9KlS7mH4+zsbHh7e2PZsmVwcHDgmpNXJMpc98oe+wYNGqBJkybw8vJCdHQ0V+7IkSPh4+MjlVdZqampuHr1Knr16oXNmzfj6dOnSEtLw8WLF+Hg4FCmWuasrCykp6fD0NAQNWrUgIaGBjIzM3Hx4kUcPHhQKq+mpiaaNWuGwMBA7NixA5mZmcjOzsahQ4cwatQovHnzRiq/sjp06ID79+9j8eLFUteyKlWuXBk6Ojq4cOECbt++rbBLUnndUworyXXfvHlzWFtb459//sG5c+fw5csXMMYQHR2NmTNncq2+Sts1ZfPmzTh8+DA+fvzI/eZUr14d7dq1Q2ZmJtdfHwXBPQDUrVsXQqEQYrEY0dHR2Lx5s9yXASg0UNrly5fx7NkzPH78WGpgNImvcewtLCxga2uLhw8f4tKlS9DQ0ChxAG9tbY2hQ4fi8OHDuHnzJlCoCXnz5s257gWFl0+fPsHFxQU3btzg+sOXRGxsrMzUb7m5uZg7dy46duxYolY2hBBCSok/PPm3Fh8fz3r37i0zVYdkuo7AwEAub1HTQ8mbCqqoaYDkTQVS3JQqhf9uXl4e8/T0lMmDgilMVq1axfLy8qTKZ4yxu3fvyp2CpXDZRU0/VNR0Taqe+kssFrNFixYxFEyDc+/ePcYKpliZO3cut+2S/VjUtC6SpfCUJPKm9Cm8FJ6eTdmylVXUtlhbW7MXL14wVmg7Cm/bkiVLZP5f+BgVd17xjzP/8yWhzHnFz1N44V8TrGAKoylTpjCoYPopiaKuewsLC27KJT5517kiRd0vCivu+JTlui+u7MLHXtFUUba2tuyvv/4q077//PkzGzRokEzZKLgXurm5sbdv33L5JedKcfuOlfDaLHy8FE0VNXr0aDZgwACpv1mSsgtf91lZWWzatGkyeSQL//soup8WdQ9WdJzA+54lvaeURUmu++Kmq5s6darUdHXKKmpqvj///FNqfwcFBTFTU1OZfJKFf3wk/P39WYcOHdjkyZPZb7/9Jnd7lT32pSGZoksoFDJjY2OFZSqa+issLIw1adKEmw4vNjaWde7cmU2ZMkXudJ6SvyeZWmzt2rVyz1eJoq7bqKgobuq/svxWEkIIKVrpqy/KSfXq1bFz506sWLGCG3TIzMwMY8eOhY+PDzp16sT/SImZm5tj69atGDt2LFc7bmFhgenTpyMwMLDUIxKjoCnsjBkzcOrUKTg5OUEoFEIoFKJPnz7w8fHBjBkz5Davbt26Nfbv348BAwZAX1+fn1wmqp76SyAQcLUHlpaW3D5UV1fn3uY3b978h2iS1qZNG1y8eFHquLRq1QorVqzAuXPnYG5uzv9IifXv3x+7d++GtbU1UHAOenh4ICwsDP379+dnL5XyPK+0tLTQq1cvCIVC2NnZqaR5d/Xq1bFnzx6sWLECrVq1Agquezc3N1y6dKlM1315Uva6V+bYa2lpYdGiRVi0aBHMzMxgZmaG+fPn48yZMzIDQipLT08Po0ePhrGxMVq1aiVVG/fmzRusW7cOY8aMKdVozdra2vj777/x119/cfeIrl27wtvbG4GBgTJ9wevWrYtdu3ZhwIABEAqFsLa2xuHDh7F27doyn7va2trw9PTEyZMnueOjalpaWpg3b57UuStPed5TJEpy3QsEAgwZMgTnzp2Tyte1a1ecPHkSK1euLNP4C5MmTcLGjRthZ2cHFDRbd3JywsmTJ7F69WqpGvOOHTti+/bt3IBikmv+4cOHmDZtGpePz8rKCtWrV8fGjRsVjhfxNY69pCk5Cvpa//zzz0q1CGjWrBlGjBiBM2fOICAgAM+ePUNgYCDs7OzkzuxQuOl6cHAw0tPTYWJiAkNDQ35WoGDwNEX355o1a8Le3h62trZo06YNP5kQQoiKCJiq2hYTQv4TGGPw9vbGrFmzcPTo0QobCBPFXr58iREjRsDZ2RmzZs2SerBPSUnBrFmzsGvXLgQHB5fpJSQhhBBCyH9ZhavZJoRUXJmZmdi7dy+mTp2Kfv36oW3btvws5Dtw584dhIaGIicnhxuoDQXjY0RGRiIqKgqWlpYKa8wIIYQQQkjxqGabEFKskJAQdOzYkft/7969sX37dlSvXl0qH/k+3Lt3D/3791c4AJlQKMTKlSsxfvx4mYGnCCGEEEJIyVCwTQgpliTYtrCwwG+//YaxY8fCyMiIn418J1jBdGO7du1CQEAAN8exhYUFHB0dMWbMGLRo0UJm9gRCCCGEEFJyFGwTQgghhBBCCCEqRn22CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQolIhISFo0aIFTpw4wU8ipZCdnY2JEydi4sSJyM7O5ieTCoqCbUIIIeQ7EBkZiZYtW0IgEEAgEMDDw4OfRYpYLEZAQAAGDhwIAwMDCAQCODg44NSpU8jNzeVnR1JSEjw9PWFlZQWBQIC6deti+vTpeP36NT8rJzMzE1u3boWNjQ0EAgEMDAwwbdo0ZGRk8LNyGGNYu3Yt+vbti5SUFKSmpmLdunVwcHCAnp4e9PT00KNHD/j7+0MsFvM/rvR2enh4cPus8OLk5ISPHz9K5WWMISgoCC4uLtDT00PdunWxYMECJCUlSeUrqaSkJPzxxx/w9vbmJwEFfy8sLAxjxoyBgYEBDAwMMH78eIXfRZ53795h+vTpqFu3LvT09DBw4EA8ePAAjDF+Vo5YLMaFCxcwYMAAmX3wrZ0+fRoCgUDhPlOkpN+Jv88FAgFsbGxw7do1ftZiFbWtubm5OHXqFBwcHCAQCNC4cWN4enqW+lySyM/Px+nTp6XK9fLyQmZmJj+rDMn12rlzZ+56HTNmDMLCwuSeL6q+HkojJiYGNjY2OH36ND+pzLy9vWXuC0XdH340pbl3SJT03ChqH0uWol4e8H8v+BTdY5X9vSxPFGwTQggh35GLFy8iOTkZbm5u/CQpp0+fxuDBg/HTTz9h165dOH/+PGrWrInRo0dj/fr1yM/P5/KmpKTA1dUVp0+fxsiRI+Hv7w83Nzf4+/tj8ODBeP78uVTZAPDy5Uv07dsXXl5ecHFxgb+/PzZu3AgtLS3k5eXxs3Pi4+Nx8eJFODs7o1q1akhOTkZUVBSGDBkCX19f7N+/H0KhEH379sXRo0elHvyU3c6cnBzEx8fD1tYWHh4e8PT05JY+ffpAS0uLy8sYw5EjRzBw4ECYmJjgyJEjmDp1Kk6ePAlXV1ckJCRIlV2UtLQ0bN68GT///DO2bNnCTwYK/b2ff/4ZOTk5OHDgANavX4+3b9/K/S7yPH/+HAMGDEB4eDg8PDywZ88epKWloW/fvrh+/To/O8RiMR48eIDhw4ejZ8+eSE9P52f5phITE7F//37+6iIp853y8/O545KRkYFdu3bh8uXLcHZ2RlpaGj97kYra1vz8fKxfvx6jR49GkyZNcPnyZUyYMAH79u1T+lwqLD8/H2vWrIGrqyusra1x8eJFDB06FEuWLMGsWbOKDLhzcnIwf/58eHl5oUuXLrh8+TLWrl2LV69eoXfv3jLniyquB1tbWzx+/BgDBgzg1olEIhw5cgTjx48vUTB77do1CIVCtG/fnp+kEqamppg1a5bUvUHe/eFHo+y9ozBlzo0GDRrI7FvJMmPGDBgbG6N169aoXLmy1N+Q4P9eSBR3j23QoAECAgLw/v17jB49mp/8VQlYSV5ffEMv43PwJikLWTn/91BQHtrVln17TgghhKiahoYGdHV1FT5cKBIZGYlBgwZh8+bNsLW15SfLuHLlCmrXro3GjRtz6/Lz8+Hp6YnDhw/jxIkTaNasGVAQxF6+fFnmAfP58+cYMmQIevXqhUWLFkFdXR0oeNCZPHkyPn/+jM2bN6NmzZrcZ4pz+vRprFq1CkeOHEG9evX4yUBBDZybmxtiYmJw4MABmJqaAqXYzuzsbLi5uaFKlSrw9PSEpqYm9xm+mJgYDBkyBC4uLpgxYwY0NDQAAEFBQRg1ahSmTJkCNzc3CAQC/kelrF+/Hh4eHjAyMoK9vT22bduGQ4cOYdiwYVL5nj17hgEDBmDUqFFSfy81NRVTpkxBlSpVsG7dOmhra0t9TiI3NxezZs3CmzdvsH37dlSvXh0AkJCQgAkTJkBNTQ27d+/mHlCjo6MxYcIE+Pv7o3///lxQ6u3tDQMDA6myvwXGGHbt2oUFCxYgOzsbW7duldlnfMp+p8uXL2PEiBFYs2YNhg4dCjW10tU5Fbet9+/fx6+//orZs2djypQp3Dlz7do1DB8+HIsWLcKECRMKlVgyd+7cwcCBA7F8+XIMGTIEAoGAC3wmTpyIffv2oW/fvvyPAQXXgo+PD5ydnSEUCrn18fHxGDduHIyMjLBp0ybo6OgAKrwe+CTX5OvXrxUeJ4mUlBSMHTsWHTt2LNXfKo63tzf++ecfHDt2DBYWFvzkCkey7wAUeW8ojrL3Dj5VnBsikQirVq3C7du3sXPnThgbG/OzAAp+L0p6j0WhfVazZk24u7vzk7+KChts7wrOwLHAF0hKy0M6fkIeKvGzqNS5YYqbvBFCCCGqkp+fj7S0NGhpaaFZs2b46aef+FnkUjbYVuT27dvo3r07Dh06hF69evGTpeTk5GDGjBlITEzErl27uG318fHBzJkzcfjwYbRp04b/MYUyMzPx559/onbt2lJBsTzKPAgr2s6srCxMnToVderUKfZBa/fu3di5c6fMSwDJg+mLFy9w8OBBGBoaSn2O759//oG6ujpGjRqFiIgIdOzYUe6D4Pbt27F161a53+/KlSuYOnWq1AsRPkmwvnTpUpkA6/Tp0/jtt99w8eJF7lx5+fIl/vnnH4wcORKtWrXCjBkzShTwfC0vX77EyJEj0b9/fxw8eBAzZ86U2Wd8ynynlJQUTJgwAXXq1MGyZcvKVGtZ3LYeO3YMgwcPRnBwsNS1+vHjRwwbNgy2trbFno98IpEIixcvxtOnT2UCIUlQamxsXKogbMWKFTh//jwOHz4MMzMzQIXXA58ywfbVq1fh5uaGI0eOKLwOykKZe0xFoKpgW9l7B58qzg3J7xn/2ilM0e9FSe+xoGBbsT8PxiL4yXu8gTlSoc9PLhcp81L5qwghhJByk5qaipiYGNSvX597wC2KqoLtx48fo3///vDw8MCgQYP4yVJEIhHc3d3x+PFjHDx4ENWqVeMeXrS0tLBq1SqlgpY7d+5g5MiROHDgANq1a8dPlqLMg7C87YQSwY0kWP/y5Qu8vLxQpUoVqfQDBw5g9erVOH78uFRLgeKEhIQofBD08PBASEiI3IBDcqyXLFmCXr16ITs7G/PmzcPdu3exb98+NGzYEMeOHcOyZcvkblNERAQGDhyIv/76CyNHjpRKg5IBjzKSkpJw+PBhODs7o2HDhvxkhbKzszFr1ixoaWlh5MiRGDlypNRDuEgkgpeXF/bs2YPt27fLPf+L+04hISEYNmwYjh8/Xuy5FxISggkTJmDMmDGYOnWq1Euh4rYVhWq2d+/ejR49enDrJTWCgwYN4oKmkkpJScGIESPQpk0bLFq0SKrWkDGGxYsXIyQkBIcPH4aRkZHUZ4uzfft27Ny5E0ePHkXDhg1Vdj3wz39vb28MHz6cnw1LliyRuT4lgRsAqfvM58+fsXv3bhw6dAgPHz6EhYUFBgwYgNmzZ0NXV1eqjOKU9B5T+D4yffp07Nq1C1u3bkVkZCS6du2KSZMmoVevXlwNr0RmZiYOHDiAY8eOISgoCPr6+nB0dMTkyZO5cS74Xr9+jY0bN+LChQuIjIyEhYUFXF1dMXnyZIhEIu68Wb58OY4fP459+/YhNDQUXbt2xbx582Bvby+33MLKcu9QxblR0lrtkvxe8M8xvooQbJeu/Uw5WnAmGYFPEvAYbb9aoE0IIYR8bVWrVoWlpSXevXuH+Ph4fnK5efXqFTIyMkoUDKWlpSEiIgKNGjXimpcmJibiyZMnaN26tVKBtkgkwvnz59G+fftia6ny8/MRHh6OVq1aoVatWvxkGfK2s7D69evzV0nJzMxEVFQU6tSpI/PwiIL+f0+ePClRH9OS0tXVRXx8PJKTk/lJyM3NRVZWVpH9j1+8eIEaNWrIDayMjIxQo0YNvHnzhp9ULrKzs3Ho0CH8/PPPuH79utx9qAhjDGfOnMG9e/cwYcIEpc4pZdy5cwfm5uYKuy6UREm31dLSElOnTsWyZcsQGhoKsViMuLg4rFixAiYmJhg6dCj/I8VKTEzE+/fvYW5uLhNMCQQCmJubIykpCZ8+fZJKK05ubi7Cw8NRr1497lwqr+uhY8eO8PPzQ58+fWBtbQ1fX18EBASgf//+/Kx49eoVAgIC0KtXL24/JyUlYfTo0Th69ChcXV3h7++P0aNHIyIiQu6gj6qWmpqK2bNn48GDB1izZg1Onz4NfX199OvXD+vWrYNIJOLyvn//HoMGDcLKlSvRtWtX+Pv7Y/369fj06RMcHR1x5MgRqfEoGGM4ffo0OnfujLt372Lu3LkICAiAm5sbvnz5IlV2bm4u1q5di+vXr8Pd3R379u1DWloaxowZg5s3b3L5FCnLvUMV58bbt2/h4+ODXr16KQy0lfm9qOgqVLAdFivC1VtP8BLNwCrWphFCCCEqp6amhkaNGuHhw4dSA5aVl4SEBBw4cACOjo4Kax0kGGO4ePEiQkJC4OzszPV3Tk5ORkxMDOrUqQN/f39uVGRFo9FKvH37FpcvX8avv/4qNyBGwd+Mj4/HmjVrcPHiRUyaNElhXglF2wkAGRkZSE9Px/DhwyEoGH154MCBuHTpktTDuSQfv2aKr6gHSGXZ2toiMTERZ8+elTr22dnZ2L9/P16+fMmt09bWxrp16xAcHIyGDRviy5cvSElJgYaGRpH9jlNSUvDlyxf+apURi8Xw9/eHnZ0dNmzYgDVr1uDw4cOoUaMGP6tCERER2LhxIyZPnqzwBZC6ujqmT5+Op0+fyq3VLs6XL1/w7t071K1bF5mZmViwYAHq1q0LQcEI/fyR721tbfH06VNMnz5dqla7JNuKgnEZpkyZgk6dOsHGxgbq6uqoUaMGoqOjsW7dOq6PrDIyMjKQmJhY5DmamJhY5EwA8ty+fRs+Pj5wcnLiul+U1/VgZmaGn3/+GdWrV0fVqlXRoUMHdO7cGU2aNJHKxxiDn58fGjZsCCsrK279o0ePcObMGaxevRq///47unbtitmzZ2Pv3r0l7o5TFmvXrkXbtm2xd+9eODs7w8XFBQcPHsTcuXOxd+9eREREAAXB8Pr165GcnIyLFy/C3d0dXbt2xYgRI3DmzBm4urpi6dKlCA8P58p+8uQJFixYAGdnZ1y8eBGjRo1C586dMWHCBMydO1eqyfi+ffsgFApx4MABODs7Y9SoUdi1axeqVKmCCxcuSAXmfGW9d5T13JC8sBIKhfjll1/4yZyS/F58LxTv5W9g57V4xKIOvkD+m0JCCCHkR6OpqYkaNWogNjaWn6RS6enpWLx4MeLj4zFr1qwiH2AYYwgICMC8efMwffp02NnZcWk5OTmIi4vD7t27sWDBAvTr1w/+/v4YM2YMtm7dKjMarURRowqHhIRAIBBATU0NpqameP78OXx9fdG2bVt+VilFbScAaGlpYfjw4bh8+TIuX76M2bNnIz4+Hj169MCSJUuQXTDdTE5ODjIzM1GnTh2pz/MVVdOsLCsrK0ydOhVz5syBi4sLli9fjuXLl8PFxQUfP37Ezz//zP8IRyQSISsrC7Vq1SpyoL2srKwiH7xLixVMnzV8+HCMGzcOQ4YMQUBAAJydnRXW9sqTlJSE+fPno02bNnBxcZGpsVUVyf6KiorC6NGj8fHjR2zevBknTpyAhoaG3JHv+ZTZ1uzsbKxatQqnT5/G2rVrERAQwNU+zp49W+71URzJdVdUS4+4uDjk5OTwVyv09OlTzJ49G87Ozhg8eDC3/ltcD4UpGoFagj/quq6ubrHBnyJhYWFo3LgxBLzpqPhTSQGAi4sLevXqJRWkamlpYfDgwcjPz8f9+/eBglr5f//9F66urjIvErS1tTFmzBgAQHBwMFBwPfn6+kJPTw9//fVXkfdmALC2tsbAgQOlvnPDhg1ha2uLpKSkImv5y3rvKOu5ITm2PXr0gImJCT+ZU9TvxfemQgXbkW8S8AnymxMQQgghPypDQ8NyDbajo6MxatQoPH78GLt375Z5ACwsNzcX+/btw/DhwzFp0iRMmzZN7oNseno6fHx8uBqmhQsX4vTp03j06BG8vb2lApeUlBRcuHBB4QOWhYUFAgICEBAQgG3btuHjx4+wtLTE/v37Fdb4l2Q7TUxM8Pvvv6Nbt27o1q0bZs2ahStXrmD58uXYuHGjTJPL4moFFTV5LA0NDQ1MmzYNJ0+eRHZ2NubPn48DBw7gl19+wfTp05GVlcX/iIysrCyF+wcA9PX1ixyBvTTi4uIwd+5cdO/eHXXq1MHdu3fh5uZWbIDAl5+fjz179kAsFmPBggWlHuxJGffu3YOrqyu2bNkCZ2dn9O/fH6dOncKIESOwYcMGhfObK7utp0+fxt69e7F9+3a4ubmhc+fOGDVqFM6dO4cvX75g6dKl3IseZRV1Xpibm0uNNK6IZE7y/v37o127dvD09JR7/L7m9VDYrVu3kJ6eji5dukitb9GiBXr37g03Nzds3LgRiYmJUumloWjqrwYNGvCzonHjxtDT0+OvRs2aNVG/fn2u6XVUVBRSU1PRsmVLflYAQI0aNVC/fn1ERkbiy5cvyMjIwIsXL9C6dWtu5oWiNG/eXKZ1hEAggLq6Ot69e1eic6us947Snhu3bt1CbGwsHB0dFb6wKu734ntToYLtnIxUZEH2YieEEEJ+ZNra2vj8+TN/dZkxxnDp0iX88ssvaNCgAXx9fYvs//bp0ydMnz4dXl5e3CjLimoq+/btK/PAZ2VlhU6dOuH58+dSD3wPHjzAq1evFD5gGRoaonPnzlyzSV9fX2zcuBGzZs3ChQsX+NmV2k4+SU1U06ZNcefOHaDg75uYmBT78C6vj2JZaGlpcS0DGGOIiIjAlClTkJeXhw8fPqBu3br8jwAF50vNmjWRmJgot6mnhLa2dpEjvpfG5cuXsXPnTrRv3x5DhgwpcsRhRRhjOH78ONc6gn8elRdra2t069ZN6hzU0dFBnz59EBoainfv3knlRym2NT09HX5+fujbt69MrVz16tUxcuRI+Pn5SXUTKAlDQ0NYWloWeY5WqVKl2OsgOzsby5cvh5ubGxYuXIi1a9fKBOjf6npAQa31uXPnuJc5hVWvXh07d+5Ev3798Pfff6Nhw4aYPHmy3ONWUsbGxhgzZgxmz54ttdjY2PCzlvh6Sk9Ph7GxscIB2ypXroxatWpxtcdfvnxBYmIiTExMiqxtllBXV5d7Hy2Jst47ynJu5ObmIigoCJ06dSpyHI3ifi++NxUq2CaEEEKIarCC+XenTJkCDw8PrFy5Evr6igceTUhIwJgxY5Ceno5Lly6ha9euch90DAwM0Lx5c7kPYhoaGqhSpQri4uK4poy5ubnw9fWFvb19kf1cCxMIBOjWrRtatGiBx48fS6WVdDuLoqenh6pVq3I1O5qamjA2NkZ8fLzcZrhRUVFo3bp1sYGWqkRERKBevXpFNhk2NjZGcnIyUlJS+ElISkrChw8f0KhRI35SmY0aNQpPnz5Fs2bN0KVLFwwbNgwPHjyQ6vNcnE+fPuHAgQN4+fIl2rZtK9V8t3HjxlwTdUXNeZVVuXJlmJiYQF1dXW4/VUlgEBcXx09SelslgVO1atXk1gxWr14dL1++VNjMVhGhUIhq1arh7du3/CQwxvDy5UvUqlVLYY0iCgLZv/76Czdu3ICvry+GDh0q0xoE3/h6ePbsGW7duoWePXvKvccYGRlh2bJlePHiBZYtW4YrV65gyJAhClslfA38fswaGhpF9p//8uULPn78yNUeq6mpQUNDQ2E/aVUry72jLOfG69evERQUhJ9//llh65DS/F5UdLJ3nG+osm5VVIF0PwxCCCHkR5edna3yAX5u3ryJVatWYdu2bejfv7/cIEMiOzsbS5cuRY0aNbB58+Yim+6ZmprCwsICCQkJMn1c8/PzkZWVBTMzM+5hSt6owqWlzHYWJS0tDampqVzNk66uLho1aoTw8HCZ0Zxzc3Nx//59NG/evERNPMsqJSUFPj4+6Nq1a5HB9v/+9z+8ePECMTEx/CSEh4cjPz8f//vf//hJKmFqaoply5bh6dOnqFevHrp164bhw4eXOOjW0tJCnz59ZJruenp6YtasWTA1NcXw4cMVNudVlkAggJWVFVJTU5GWlsZPRlZWFoRCodwp+JTdVknglJycLLfvbEJCAoRCodxAsijVqlWDhYUFnj9/LhOop6am4vHjx7CyspLbzBkFfXW3bNmC2NhY7Nmzp8gBEsvzehAUNHeWR5kRqA0MDDB58mRs2bIF0dHRCA0N5WdROUUvSV69eoUnT56gTZs2QEH/aRQM6CbP+/fv8ezZMzRo0ADq6urQ0dFBo0aNih3FW1XKcu8oy7kRGRkJANx+kkeVvxcVheJf3m/Awqw69FF0swRCCCHkR5OcnIyaNWvyV5faly9fcOrUKdjZ2cHW1rbYmt/w8HBcuXIFv/32m0yTUr6ffvoJTk5OOH78ODf6rsTdu3dx/fp1dO/eHZUrV1Y4qrBEdnY2jh49KvMAyxjDlStX8PjxY3To0IFbr8x2KpKbm4tDhw4hLS0N3bt3BwoCACcnJ8TExODYsWNSAePt27dx8eJF9OnTR26/VlXKzc3Ftm3b8PTpU/Tv319hUAIATZs2hZOTE3bs2CH10CsZcf6XX34p95ohSdD98OFD1KlTB926dcPQoUPx4cMHflYpurq6mDhxokzT3dmzZ2PMmDEwNjaGk5OTwua8pdG2bVsIhUIcP35cqq9qZmYmTpw4AScnJ7lzLSu7rVWrVkWXLl3g4+OD27dvS5UVGxuLAwcOwMHBQe7fKoq2tjZ69OgBPz8/XLlyhXvRxQpG43/69Cl69eql8Jx5//49zp49i5EjRxZ7rynP60HSyiA3N1fmZURxI1CnpaXJvMwxNDSErq6uwppSVTp79izX5UMiISEBGzduhL29PSwtLYGCa7NXr17YuXMnnj9/XqiE/3/P27NnD/T19dG1a1egoLbY2dkZoaGh2Ldvn8x+UbWy3DtKe26IRCLcvn0bLVq0UDhbQXG/F98r9b///vtv/spvpaahDq6HPkYiakAE2WYt5WnOz7JNIQghhJDy9uXLF7x48QItW7Yssvb548ePOHnyJJydnWX6MvKlpqZi+/btSE1NRVxcHEJCQmSWp0+fonHjxtDU1ERAQADOnz+PSpUq4cGDBzJ5Q0JCoKGhwdW01q5dG7du3YKXlxcyMzORkZGBvXv3Yvbs2Rg0aBBcXV2hqamJ+Ph4/PPPP+jbt69U0CyRn5+P7du3Y/HixUhPT0dOTg5u374NT09PrF27FrNnz8aQIUO4/aLsdp44cQKbN2/G58+f8fHjR/j5+WHRokU4duwYVq1ahS5dunAvIoyNjZGXl4clS5YgLi4OlSpVgq+vL/7++284ODhgwoQJcpsFF+Xdu3fYs2cP+vbtixYtWkilSfrOPnr0CJmZmQgJCcG8efPg5+eHlStXws7Ojtu27OxszJo1C0uXLkWnTp2gr68PbW1tGBsbY/PmzQgKCkLlypURHh6OhQsXIjk5GYsXL1ZY85+fnw8/Pz+kpqaiX79+cvtXKuOnn36Cg4MDhgwZgri4OFhYWBTZZaEokvPc1taW22cikQjr16+Hq6srLC0t5Z7/xX0noVAIbW1tzJ07Fw8fPoSGhgbu3buHBQsWICwsDB4eHlxtb0hICH755Rfk5OSgXbt2Cq9LedsqEAhQv3593Lt3D//88w8SExORmpqKM2fOYN68eXjz5g08PT25v5WSkoIBAwYgPj4eNjY2Rb4Yq127NmJjY7FixQrk5ubiy5cv8Pb2xqJFizB58mT07dsXampqcvfXs2fPsHjxYhgbG+PJkycy101ISAg+f/7MNR9WxfWg6Pz/+PEjVq1aBcYYGGO4e/cumjdvjtOnT+Ply5eYMmWK3JdpJ0+exOTJk7kRsW/cuIEVK1agdu3amDhxInR1dfHq1St0794dhoaGRQ4EiYLpts6ePQuxWCz3flK9enUYGBggOzsbp0+fRrt27XD37l08fvwYYrEYwcHBmDt3LiIjI7Fq1SquNr5SpUowNzfH+fPnsXXrVqSnpyM/Px8hISH4+++/ERgYiI0bN0oNoFanTh1oampi2bJluHPnDkQiEVJSUriXK5LaYD8/PwCAk5MTKlWqxH2+uPO/MGXuHa9evUKvXr3w5MkT2NnZoVKlSqU6Nz5//oxt27ahWbNm6N69u9zzvLjfC3kUnWMSkv2ip6cnM1vF1yL/7vGNWNZUR9f2zWGOZxCg+GZIhBBCyPdMLBbjxYsXaNWqldy+k2X177//Ys6cOXKXs2fPStWgxMXFYdWqVTL5JEtUVBSX18jICLt378bvv/+OvXv3okePHggNDcWOHTuwcuVKrlZD0ajCEtra2pgzZw66deuGPXv2wMHBAfPmzUOVKlXg5+eHmTNnyuwXZbbTxMQE7969w+TJk2Fvb48VK1agVq1auHHjBoYMGSL1wKehoYEZM2Zgx44duHv3Lrp3745jx45h2rRpWL16tUxNTVmpq6vDyMiIq+l0d3dH06ZNucG15D2M8tnZ2eHs2bPQ09PDmDFj8Ndff6Fp06Y4ceJEsYFGeahTpw4WLFigsFbsW+vfvz+uXLkCFPQ9l+zzS5cuoVOnTvzspVa9enV4e3tj6dKlCAkJweDBg7F79244ODggMDBQ6qH/3bt3eP/+PTp27FjsMdfR0cHq1auxaNEiHD58GN27d8f169exc+dOzJgxQ+ZakWfTpk0y14xkefDgAZevPK+HHj16YP369Th9+jSGDRuGtLS0Eo1AbWVlhUaNGmHlypWwt7fHunXr4OjoiJ07d3J9hGNiYqChoYFWrVrxPy5XUfcT/iBg9evXx/r165GUlITBgwdj+vTpaNiwIc6dOycTyFlYWODMmTOYOHEizpw5w13j5ubmMucACu1vHx8faGho4I8//oC9vT2OHDmCGjVqyASvZVWWe0dpzo3ExES8f/8e5ubmCs/z4n4vvlcCxu9wVQH8eTAWwU/e4w3MkYrSvRlVVsq8VP4qQgghpNykpqYiJiYG9evXl9tXlC8yMhKDBg3C5s2bYWtry0+ucDIzM/Hnn3+idu3aWLRokcLmrYT8l/n6+mLr1q04ePBgqUZ2/1FcvXoVbm5uOHLkSLH9tYuybt06REZGYt26dSprWv7x40cMGzYMtra2cHd35ycTFSiv34vs7Gy4ubmhZs2a3+zYVchgGwB2BWfgWOALJKXlIR0/IQ//11SiPJwbJn/EQEIIIUSV8vPzkZaWBi0tLTRr1qzEA6NJgm1PT0+0bdsWWlpaCqeWqQju3LmDkSNH4sCBA2jXrh0/mZD/PMYYFi9eDA0NDcyfP19hjd+PLjc3F7NmzQIArFq1qtQDY2VlZWHq1Kno3Lkzhg0bxk8uNQq2y5+qfy/y8/ORnp6OrKwsuLu7o169et/s2FXYYFviZXwO3iRlIStH8cTrqtCuNjVbJ4QQUv40NDSgq6tbovlUC5ME22FhYQCAJUuWfLOHB0JI2X3+/Bmurq6YOnXqd9FapaJ78+YNpkyZglWrVik9AF1RKNj+/lSk38sKH2wTQgghhBBCyLdAwTYpiwo1QBohhBBCCCGEEPIjoJptQgghhBBCCCFExahmmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVq1ADpCUnJ/NXEULKiaGhIX8VIYQQQgghREWoZpsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUTEKtgkhhBBCCCGEEBWjYJuoxO3bt2FnZ4f27dvjwoULYIzxs5QIYww+Pj6wsrJCt27dcO/ePX4WQgghhBBCCKnwfohgOzc3F4GBgZgyZQrat28PIyMjGBkZoUuXLpg3bx5evnzJ/8gPJzs7GzNmzOC+e3HLjBkzkJ2dzS+mVLKzs3HixAmEh4fj1atXuHbtGnJycvjZSiQlJQWHDx/Gu3fv8OjRIwQFBfGzfBOfPn3CoEGDuP0XGhrKz0IIIYQQQgghnO8+2E5OTsb06dMxYMAAHDlyBK9eveLSnjx5gp07d+Lx48fcuqysLISEhGDZsmXYtGkTt56Unra2NgYMGICmTZuiYcOG6N69O7S1tfnZSqRatWoYOnQoateujZYtW6Jr1678LIQQQgghhBBS4X3XwbZIJMKhQ4dw/PhxfpJCFy9eRJ8+fbB+/Xrk5ubyk0kp2djYICgoCLdu3YKjoyM/ucQEAgF69+6NBw8e4MqVK2jZsiU/CyGEEEIIIYRUeN91sJ2RkYE7d+5w/582bRoiIyORmJiIhIQEPHnyBJs3b4aurq7U535E2traWLNmDZKSkrhlzpw5XPqcOXOk0tasWVPq2mdCCCGEEEIIIUX7roNtkUgEkUjE/b9GjRqoVq0aBAIB1NTUYGJigoEDB8LR0RGvXr1C586dMXHiRC6/p6cnjIyM0LlzZ6nm52/fvoW7uzusrKy4vt9eXl5ITk7m8vD7SJ86dQqJiYmYOXMmzM3NYWVlheXLl0t9piKR7A/J94+IiMCBAwdgZWWF2bNnIycnB0lJSdi7dy+GDx/O7Yv27dtj4cKF+PDhg1R5a9as4fbFmjVrFK7PyMjAxo0bYWVlBXNzc8ycOROJiYlcfnn7VdH6xMRELFq0iNvfq1atQkZGBlcWCgZce/bsGaZMmcLl27FjB+Li4rg+2PzjrwpisRh3797FtGnTuH1nZWWFadOm4dmzZzIDyOXm5uLcuXMYNGgQ6tWrx513s2bN4vaPWCxGcHAwxo4dC3Nzc+54jB8/XuXbTwghhBBCCCmb7zrY1tTUhKGhIff/5cuXY/v27cjKypLKp4ybN2+iT58+2LZtG969ewcU9P1eunQpJk2ahNevX/M/AgCIj4/HrFmzsG/fPqSmpuLdu3dYt24dpk+fjqSkJH72CufChQtYtGgR3r17h/z8fDDGcP36dcyaNQt+fn7cvnj16hW2bt2KMWPGIDo6ml9MkTIyMrBs2TIsWbIE7969Q2pqKvbt24clS5YgPT2dn71Ikv29ZcsWbn//888/8PLyknoBc+vWLYwYMQJHjhzh8s2fPx+7d++WyqdK2dnZWLNmDX755Rd4e3tz++7du3fw9vZGz549pf5+VlYWFi5ciDFjxuDatWvcC4MnT57gzp07SEtLg0gkwqZNm+Di4gJfX1+kpqYCBcfjzJkz+PjxY6EtIIQQQgghhHxr33Wwrauri549e3LNxFNTU+Hu7o6OHTti/fr1SEhI4PI2bNgQgYGB2LZtG7dO0rQ6MDAQDRs2RHR0NBcIduvWDXfv3kVCQgJOnjyJ2rVrIyAgAEePHpUbpG3duujzk/kAAGjCSURBVBUXLlzgr8bFixdx+fJl/uoK5c2bN9i1a5dMrbCuri62bdvGNc2Pj4/Hpk2boKuri/v378PPz0+mhrYoR48exdGjR/mrERgYqHTgrmh/37x5k6t1T0hIwNq1a7lgtzAvL69yG+n833//xapVq/irOZKXDiEhIQCAe/fuceMOdOvWDQ8ePEBiYiKePXuGUaNGQV1dHS9fvuT2XbNmzRAYGIjExES8evUKGzZsQOXKlaX+BiGEEEIIIeTb+q6DbQBwcnLCpk2b0LBhQ27du3fvsGzZMvTo0QOnTp2SGxzLc/nyZdy/fx8AMHToUNStWxdqampo3bo12rdvDxQERikpKbxPArVq1cKVK1eQmJiI58+fY8SIEVzavXv3VDbNVnnIyMhA27Zt8ejRI65ptqamJhwdHdGvXz/o6+tDLBYjIyMD1atXR506dQAAL168UGqKL21tbRw8eBAJCQk4c+YMateuDRQExTExMfzsRapVqxauX7+OhIQEbNmyhXvhEh4ezr1kKTx1WO3atXH48GEkJCQgOjoaM2fOlCpPVdLS0qReAowYMQJPnz5FUlISnj59yp0XGRkZuHLlCvLy8pCUlMS96NDW1oa2tjYEAgGMjY0xevRo1KtXD58/f+amsKtcuTKqVKkCgUCAn376CUOGDIGlpSX3NwkhhBBCCCHf3ncfbAsEAjg7O+PChQtYsWIFmjZtyqW9e/cOf/31F3x9faU+I092drbUfNyjR4/m+gfXq1ePq3lMTk7mmvAWNnr0aLRs2RICgQCGhoYYNGgQFwB++PChQgfburq6GDduHGrWrAmBQAA9PT2oq6sjLS0Nu3btgpOTE0xMTNCwYUMMGDAA4eHhQCm+V79+/dC+fXvuBYadnR2XVtIXIhJDhw5FkyZNoKamBnt7e7Ru3RooCGIlZT19+pTLP3z4cDg4OEBNTQ1CoRBDhgxBu3btuHRVSUxM5GrpmzVrhkmTJqF69eoAgOrVq0udFy9evEB6ejoaNmzI5fH19UX37t3h7u6O58+fQywWAwXjEUi29/79++jevTumTJmC0NBQpfcdIYQQQgghpPx998G2hL6+PsaNG4dLly5hy5YtXK1pRkYGzp07h7S0NP5HVKpu3bpS/9fX1+dqgCs6MzMz1KhRQ2pdcnIypkyZgrlz5+L+/fto3rw5fvnlF0ybNg3m5uZSeUuqcuXKUFdX5/6vplb6009HR4f7t5qamlS58piZmUEgEHD/19HRKfdR6qtXrw59fX2pdQYGBjAzM5Na97///Q8zZsxA1apVgYKXRNu2bUOnTp3g7u6OrKws1KpVC5MmTeJacKSmpuLIkSPo2bMnxo8f/12MC0AIIYQQQsh/SemjnQrg8+fPuHbtGlf7h4JmuP3798fkyZO5dZmZmcjPz+f+LyESiRT2Od67d6/UVFmSRdK/m+/9+/dS/4+NjeVqgHV0dKChoSGVXtHdv3+faw69Zs0a+Pv7Y//+/Rg5ciSqVavGz17hJSYmSh3rT58+SfXpLw+fP3+WGfjt48ePePPmDQBAXV2dW0aPHo1bt25hxYoVXC09AOzYsQNnzpzhWnBcvnyZC8QlfH19y3XAN0IIIYQQQojyvutgWyQSYfv27Zg/fz7Cw8O5YCMtLU1maip5Hj16hPj4eKCg1rVmzZpc2oEDB/Do0SMuIP/06ROOHDmicDTyU6dO4fXr12CMITo6Gjt27ODSGjRoIFUT+z14/vw59++cnBzk5eUhKysLR44ckZrbvCIzMjLi/u3j44P79+9DLBYjOTkZW7ZswbNnz6Tyq4KRkRHXyuH+/fs4ePAgPn/+DACIi4vDgQMHuP7ZzZo1k6pdNzQ0xLhx4+Dj4yM1R7rkHAUAoVCIfv364cSJE9i4cSO3PikpCV++fOH+TwghhBBCCPm2vutgW2LXrl2ws7ODiYkJjIyM0LBhQ6xfv55Lb9WqFX766SegoBmvxJUrV9CiRQt07twZUVFR6NKlC9f8PCAgAN26dYOJiQmMjY1hYWGB7du3y60hR0FZbdu2hbGxMaytrXHlyhWgoClxly5dim3mXNGYmppy/54/fz5q1aoFMzMz7N27t9ybX6tKu3btuCbv9+/fR48ePVC9enU0adIEFy9eLNP36NmzJ9enX7LMmDEDlSpVQteuXbl8Xl5eaNiwIYyMjNCiRQuu73/t2rXh7OwMdXV1+Pn54dSpU1xQnpOTI1XrbmRkhLCwMOzatYuroc/NzZUKwvX19aGpqcn9nxBCCCGEEPJt/RDBdlF69eqFsWPHcsFukyZNpAbmKqx58+aYNGlSqYKwMWPGyO3LPH78+HIZiKu8tW3bVqo5MwoCxLlz58r0Oa6ozM3NMXjwYP5q6OrqYu7cueVyXAQCAfr164dp06bxkzhVq1aFu7s7N4J4RkYGJk6cyAXlDRs2xN69ewEAPXr0QI8ePZCTk4O5c+eiWbNmMDY2Rp06dbBs2TIAQOvWrTFw4MDv7oUOIYQQQgghP7LvOtiuVq0ali5diunTp6NNmzbc+tq1a6N3797Yt28ftmzZItWcuHr16li1ahU3KnTt2rUxZswY1KtXD+rq6hg7dixOnjyJYcOGcbXctWvXxuDBg7F8+XLUr1+fK6uwNm3aYP369bCxsQEAdOrUCXv37sUff/zxXQZB9evXx+rVq9GrVy/o6urCxsYGGzZsQIcOHfhZKyx1dXX88ccf2Lt3L9fH2cbGBvv27YOdnZ3MvOKqUqVKFcydOxdnzpxBr169uIHPGjZsiN9//x3//vsv+vTpww3YVrt2bbi4uHBjAejq6qJLly5Yv349tm3bxtWcDxkyBM2bN+f+TocOHbB48WIcOnRI7oseQgghhBBCyLcjYIpGCPsGkpOT+asqrOzsbCxYsAAHDhwAAGzbtg39+vXjZyMVVFhYGIYNG4aEhAR069YNW7du5boa/FcYGhryVxFCCCGEEEJU5Luu2SakNEQiEc6fP8/1i27RokWpug4QQgghhBBCiCIUbJMfVnZ2NjZu3Ihbt24hOzsbKJiOa9u2bdi1axdQ0N+5X79+32VTf0IIIYQQQkjFRcE2+aG9fv0avXr1Qp06dbjBx/7++29kZGSgWbNm8PDwoP7OhBBCCCGEEJWjYJv8sDQ1NWFra4vu3btzg5RJBh9bs2YNTp8+jbZt2/I/RgghhBBCCCFlRgOkEfIfRQOkEUIIIYQQUn6oZpsQQgghhBBCCFExCrYJIYQQQgghhBAVo2CbEEIIIYQQQghRMQq2CSGEEEIIIYQQFaNgmxBCCCGEEEIIUbEKNRo5IYQQQgghhBDyI6CabUIIIYQQQgghRMUo2CaEEEIIIYQQQlSMgm1CCCGEEEIIIUTFKNgmhBBCCCGEEEJUjIJtQgghhBBCCCFExSjYJoQQQgghhBBCVIyCbUIIIYQQQgghRMUo2CaEEEIIIYQQQlSMgm1CCCGEEEIIIUTFKNgmhBBCCCGEEEJUjIJtQgghhBBCCCFExSjYJoQQQgghhBBCVIyCbUIIIYQQQgghRMUo2FYxsViMCxcuYMCAAfj48SM/mciRmZmJrVu3onPnzhAIBDAwMMCYMWMQFhYGxhg/e5FKs/9Pnz4NgUAAb29vfhJyc3Nx6tQpODg4QCAQoHHjxvDy8kJmZiY/KxhjCAsLw5gxY2BgYAADAwOMHz8er1+/5mcFAOTn5+P06dMlLjsoKAguLi7Q09ND3bp1sWDBAiQlJfGzEkIIIYQQQioACrZVRCwW48GDBxg+fDh69uyJ9PR0fhYiR05ODubPnw8vLy906dIFly9fxtq1a/Hq1Sv07t0b169f539ErtLu/8TEROzfv5+/Gih4CTB9+nSMHz8enTp1wuXLlzF69Ghs3boVs2bNkgqKGWM4cuQIfv75Z+Tk5ODAgQNYv3493r59i8GDB+P58+dSZefn52PNmjVwdXWFtbU1Ll68iKFDh2LJkiUKyx44cCBMTExw5MgRTJ06FSdPnoSrqysSEhKkyiaEEEIIIYRUAIyUWVRUFHNwcGAAWP/+/ZmjoyNzdHRkycnJ/KyEJysrix05coSlpaVJrY+Li2POzs7st99+YxkZGVJpfKXd/2KxmO3YsYMZGxszoVDIDh06JJW+a9cuZmZmxgIDA6XW379/nzVp0kQq/9OnT1mTJk2Yp6cny8vL49anpKSwESNGsAkTJrCsrCxufWhoKDMzM2Pe3t5MLBYzVrA93t7eTCgUslOnTnF5o6OjmbW1tUzZgYGBzMzMjK1Zs4YrgxBCCCGEEFIxUM22CohEItSrVw83btzAvn37ULduXX4WooC2tjYGDx4MoVAotd7ExAS2trZ48eIFkpOTpdL4Srv/X716hT179mDWrFmoX7++VFp6ejqCgoLg5OSEdu3aSaVZWlqiT58+8Pf352qgg4ODoampiT59+kBDQ4PLW7VqVYwYMQLXr19HdHQ0ULC958+fh5WVFXr06AGBQAAAEAgE6NGjBxwcHHD58mVkZ2cDAK5duwYAGDhwoFTZNjY26N27N65cuVLiJvOEEEIIIYSQr4OCbRUwNzfHjh070LFjR6ip0S5VFX19feTm5iIvL4+fJKU0+z87OxsbNmyAra0tunXrxk/Gly9fkJiYiJo1a0JbW1sqTV1dHc2aNZN6EZCYmAgTExMYGhpK5QWAOnXqQFNTE1FRUQCAtLQ0PHjwAC1atEDVqlWl8latWhUtWrRAVFQUMjIykJOTgwcPHqB58+aoXr26VF4tLS20bt0asbGxxb6QIIQQQgghhHxdJYtMCPnKcnNzER4ejnr16sHIyIifXCaMMZw5cwb37t3DhAkToKWlxc8CDQ0NCIVCxMbGcjXMhWVkZCAzMxM5OTkAAF1dXcTHx8sNenNzc5GVlcX1I09MTMT79+9hbm7O1WpLCAQCmJubIykpCZ8+fUJmZiaioqJQp04dVKlSRSovADRo0ABPnjyhmm1CCCGEEEIqGAq2SYV0+/Zt+Pj4wMnJCT/99BM/uUwiIiKwceNGTJ48GQ0bNuQnAwD09PTQvn17XLp0CXfu3JFKi4qKkhm53NbWFomJiTh79izy8/O59dnZ2di/fz9evnzJrcvIyEBiYqJUk3C+xMREZGRkICMjA+np6UXmBUDBNiGEEEIIIRUMBdukwnn69Clmz54NZ2dnDB48mJ9cJklJSZg/fz7atGkDFxcXmZplCYFAgGHDhqFly5bo27cvJk6ciJUrV2L+/Pno3bu3TD9uKysrTJ06FXPmzIGLiwuWL1+O5cuXw8XFBR8/fsTPP//M5c3JyUFcXBxq1aolVUZhcXFxyMnJQU5ODjIzM1GnTh1+FiklHX2dEEIIIYQQ8nVQsE0qDMkc2f3790e7du3g6ekJHR0dfrZSy8/Px549eyAWi7FgwQKZvth81atXx86dOzFz5kwEBgZizpw5CA0NxYYNG9CmTRupvBoaGpg2bRpOnjyJ7OxszJ8/HwcOHMAvv/yC6dOnIysrSyo/ALnrJMzNzaUGjcvIyJBK5zM2NuavIoQQQgghhHxDFGyTCiE7OxvLly+Hm5sbFi5ciLVr18qMUF4WjDEcP34cu3fvxoIFC2QGG1PEyMgIc+bMQUREBBhj8Pf3R5cuXRAXFyczIJqWlhb69esHf39/MMYQERGBKVOmIC8vDx8+fOBGSTc0NISlpSUSExML/SVpVapUgZaWFgwNDWFiYlJkXhTkJ4QQQgghhFQcFGyTby4zMxN//fUXbty4AV9fXwwdOrTYPsrK+vTpEw4cOICXL1+ibdu2EAgE3NK4cWOEhYVh+PDhEAgEMv2x+bKzsxEZGYmmTZuW6IVAREQE6tWrxzUbFwqFqFatGt6+fcvPCsYYXr58iVq1asHY2BiampowNjZGfHw8NxhbYVFRUWjdunWJXx4QQgghhBBCvg4Ktsk3JRKJsGXLFsTGxmLPnj1o3LgxP4tKaGlpoU+fPvD09JRZZs2aBVNTUwwfPhyenp5o0KAB/+NSnj17hmvXrsHZ2Rmampr8ZCkpKSnw8fFB165duWC7WrVqsLCwwPPnz2X6WqempuLx48ewsrKCnp4edHV10ahRI4SHh+PTp09SeXNzc3H//n00b94cpqamUmmEEEIIIYSQb4uCbfJNvX//HmfPnsXIkSNRs2ZNfrLK6OrqYuLEiZg9e7bMMmbMGBgbG8PJyQmzZ8+GjY0N/+Oc2NhYLF26FO3bt0fbtm35yVJyc3Oxbds2PH36FP3794e6ujoAQFtbGz169ICfnx+uXLkCxhhQUKt98eJFPH36FL169YK6ujoEAgGcnJwQExODY8eOQSwWc+Xfvn0bFy9eRJ8+fVTat50QQgghhBBSdup///333/yVpPTy8/Ph5+eH1NRU9OvXj/rSFuPZs2dYvHgxjI2N8eTJE4SEhMgsnz9/RqNGjZCdnY1Zs2Zh6dKl6NSpE/T19fnFlWr/f/z4ESdPnoStrS1atGjBrX/16hUWLlyI1NRUJCYm4sSJE3Bzc4OOjg48PT1Ro0YNLq+kz/mjR4+QmZmJkJAQzJs3D35+fli5ciXs7OykRj6vXbs2YmNjsWLFCuTm5uLLly/w9vbGokWLMHnyZPTt2xdqav//XZixsTHy8vKwZMkSxMXFoVKlSvD19cXff/8NBwcHTJgwodgadkIIIYQQQsjXRTXbpELYtGkT5syZI3d58OABP/tXoaurC5FIhHnz5sHBwQFnzpzBn3/+iePHj8Pc3Fwqr7q6OoyMjHDgwAE4ODjA3d0dTZs2hZ+fH/r27SszxZiOjg5Wr16NRYsW4fDhw+jevTuuX7+OnTt3YsaMGVJ91jU0NDBjxgzs2LEDd+/eRffu3XHs2DFMmzYNq1evplptQgghhBBCKiABk7RhJYQQQgghhBBCiEpQzTYhhBBCCCGEEKJiFGwTQgghhBBCCCEqRsE2IYQQQgghhBCiYhRsE0IIIYQQQgghKkbBNiGEEEIIIYQQomIUbBNCCCGEEEIIISpGwTYhhBBCCCGEEKJiFGwTQgghhBBCCCEqRsE2IYQQQgghhBCiYhRsE0IIIYQQQgghKiZgjDH+ym8lKyuLv4oQUk6qVKnCX0UIIYQQQghREarZJoQQQgghhBBCVIyCbUIIIYQQQgghRMUo2CaEEEIIIYQQQlSMgm1CCCGEEEIIIUTFKNgmhBBCCCGEEEJUjIJtQgghhBBCCCFExSjYJoQQQgghhBBCVIyCbUIIIYQQQgghRMUo2CaEEEIIIYQQQlSMgm1CCCGEEEIIIUTFKNgmhBBCCCGEEEJUjIJtQgghhBBCCCFExSjYJoQQQgghhBBCVIyCbRXIysrCzp074ejoCB0dHdSuXRsTJ07EkydPwBjjZyc8qtx/YrEYly5dwrBhw/Dx40eptBcvXsDGxgY6OjoKFxsbG7x48YL7zNGjR2XyyMsHAMnJyVi9ejU6dOgAHR0dNGnSBLNnz8bbt2+l8knk5+fDx8cHPXv2hI6ODlq1aoUtW7YgKyuLnxWMMQQHB2Pw4MEwMTFBkyZNsHjxYiQnJ/OzEkIIIYQQQioAAVM2milH8oKMii43NxcLFiyAv78/Bg4ciHbt2iEuLg779+/H+/fvsXPnTnTs2JH/MVJAVftPLBbj8ePHWL9+PU6cOAEHBwfs2bMHBgYGXJ6EhAScPHkSOTk5Up8FgLy8PFy6dAmNGjXC6tWroaenBwD4559/cPToUfTt2xeVK1fm8leuXBn9+/dH9erVAQApKSmYOHEi4uLiMGjQIDRt2hTPnz/Hvn37UKVKFWzbtg2NGzfmPp+fnw8vLy+sX78eY8eOha2tLe7du4ctW7agf//+WLZsGapUqQIUBNrHjx/H7Nmz0bt3b/To0QOvXr3C7t270ahRI2zcuBHGxsZc2SUlKZ8QQgghhBCiehRsl1FOTg7Onz8PR0dHCIVCbn1CQgImTZoEQ0NDrFu3jgIbBVSx/16/fo3Jkyfj2rVr6NOnDzIyMgBAJtguSkhICP78809s2rQJtra23HpPT088efIEW7ZswU8//ST1mcJSUlJw7do1/Prrr9DU1OTWR0RE4LfffoOzszPmzZsHdXV1AMC9e/cwYsQI/P333xg4cCAEAgEXVE+dOhXbt29H7969AQBv3rzBqP/X3r3G13Tm/R//bokgEUTkUKcgRdT5UIdByyQlJi2qRJCplmbqVv2ralXrWAZRVaNVBqXaTsahKCZu6tRqaZ2rzqJxmiKyJdocdvaOJOv/4Gbf9koEtTXu13zer9d+kOv3u5bVnfTBd61rXWvgQD311FMaPny4PD09JUk7duxQXFychg4dqmHDhslisTj/3TtR3HcKAAAA4N6wjPwe3bjDeXNQlKSgoCC1bdtWp06dYqlvMdzx/eXn5yskJERbtmzRwoULFRISYm4plt1u1/Lly9WxY0e1bNnSOW4YhvLz8+Xv768yZcq4zDHz8/PTM8884xK0Jal27dpq166dTpw44bwIkJ+frw0bNqhZs2bq0qWLMyRbLBZ16dJFnTt31tatW5134L/++mtJUu/evZ1BW5Jat26tqKgobd26Venp6c5xAAAAACWPsH0fVa5cWbm5ucrLyzOXcAfu9PsLDQ3VnDlz1K5dO5Uqdfd/0vv379e3336rvn37uiwVdzgcunz5skvv3fL09FTFihVlt9tVUFAgScrIyNDBgwfVqFEjVapUyaW/UqVKatSokc6cOaOsrCw5HA4dPHhQDRs2LLRU3MvLS82bN9fFixdve0ECAAAAwO/r7pMJ7khubq6OHz+ukJCQO17KjP/1e31/ubm5+uKLL9S6dWs1b97cXJYkVa1a1SWE342MjAydPHlSDz/8sHx8fCRJVqtVFy5cUGhoaKGl3xaLRaGhobJarUpPT1dWVpZOnz6tGjVqqFy5ci69un6h4ejRo7p69aq5BAAAAKAEEbbvkz179mj9+vXq0qVLsc/6omi/1/d36NAhrV+/XjExMYWeYXY4HEpLS9PkyZPl4+Oj4OBg9ezZU8uXL7+j/QUMw9CmTZv0/fffKzIy0rnEPDs7W1arVaVLlzZPcbJarcrOzlZ2drYyMzOdz3rfinnndQAAAAAli7B9Hxw9elRjx45VZGSk+vTpYy7jNn6v7+/mZ6ebNWtmLsvT01MRERFatWqVEhMTNXHiRHl6emrQoEEaMWJEsQHXMAxt375dEydO1Msvv6yOHTs6aw6HQykpKapWrZrLnJulpKTI4XDIbrfLZrOpRo0a5hYXmZmZ5iEAAAAAJYiw7UY33vEcGxurli1batKkSYXuluLWfu/v7+eff9bWrVsVGRkpPz8/c1k+Pj56/vnnFRkZqc6dO2vIkCFasWKFPvroI61du1aJiYnmKdL1pen/+Mc/NHjwYMXFxenll1922djshuzsbPOQU2hoqMqXL+/8ubheSQoICDAPAQAAAChBhG03sdvtmjFjhkaNGqXRo0dr+vTphXbYxq2VxPd38OBB2e12tWvXzly6pVKlSjnD98GDB+VwOFzq6enpeuONN/Thhx/qo48+0ogRIwrtUF65cmU1btxYVqvVZfxm3t7e8vLykr+/v4KCgort1fULAwAAAAAeHIRtN7DZbHrzzTf13Xff6fPPP1ffvn2LvJOJopXE92e327V161a1bt1aNWvWNJeLVa5cOQUEBCg/P183v6Y+NTVV//Vf/6WsrCytXbtWnTt3LrQBmiT5+vrKz89P//73v80lGYah5ORkVatWTQEBAfLy8lJgYKAuX75cKNhLUnJyspo3b15op3IAAAAAJYuwfY/y8/M1f/58Xbx4UfPmzVP9+vXNLShGSX1/qampOnLkiP7whz/c9U7jOTk5slqtKleunPNVY3a7XdOnT9dDDz2k9957T0FBQeZpTn5+fqpbt66SkpIKPWv9yy+/6MiRI2rWrJkqVKig8uXL6+GHH9aJEycKvUs7NzdXP/zwgxo2bKjg4GCXGgAAAICSRdi+RxcuXNC//vUv9e/fX1WrVjWXcRsl9f2dOnVKV65cUaNGjcylYhUUFGj9+vX66quvXHYYP378uLZt26bY2NjbLn8vW7asunTpos2bN2vbtm3Ou+M3di8/evSooqKi5OHhIYvFoieeeEJnz57VqlWrnO/q1vUd2zdt2qSnnnrqvj7bDgAAAODuWYyb18GWsDt5ndKDZteuXQoPD9eQIUNuGRYbNmyoyMhI8zDu8vuz2+0aP3689u/fr4ULF6pOnTrmVtntdo0aNUrnzp3T4sWLb/mO7hkzZuibb77Rxx9/rCpVqpjL0vUwu3DhQnXo0EG1a9fWpUuX9K9//UtffPGFJk2apOHDhzuXuy9btkxjxoxR//79ValSJfOhJEkdO3ZU69atpet/62PGjNHKlSs1dOhQtW7dWnv27NHcuXP1yiuvuBw7Ly9Ps2fP1owZMzRgwAD96U9/0vHjx7Vo0SJ16tRJU6ZM+U1h+7fMAQAAAHBnCNv36EZYLM64ceM0evRo8zDu8vtzV9i+0SNJ77zzzi2XkSclJWny5Mnat2+fzp8/r8qVK+vxxx9XXFycOnbs6FxCruthe/DgwS7zzRYtWqSYmBjnz3a7XR9//LEWLFigpKQkde7cWXFxcYqKiir0zHpeXp7WrFmjOXPmaO/evXr00UcVGxur2NjYW57/7RC2AQAAgPuHsA38hyJsAwAAAPcPz2wDAAAAAOBmhG0AAAAAANyMsA0AAAAAgJsRtgEAAAAAcDPCNgAAAAAAbkbYBgAAAADAzQjbAAAAAAC4GWEbAAAAAAA3I2wDAAAAAOBmhG0AAAAAANzMYhiGYR4EAAAAAAC/HXe2AQAAAABwM8I2AAAAAABuRtgGAAAAAMDNCNsAAAAAALgZYRsAAAAAADcjbAMAAAAA4GaEbQAAAAAA3IywDQAAAACAmxG2AQAAAABwM8I2AAAAAABuRtgGAAAAAMDNCNsAAAAAALgZYRsAAAAAADcjbLvBzp079fzzzyssLEwWi0VhYWEaP368rFaruRVFyM7O1rx589SpUydZLBb5+/tr0KBB+vHHH2UYhrm9kLuZf/HiRY0bN07t2rVz9vbv318HDhwo1CtJCQkJslgshT7NmjXTyZMnXXoNw9CPP/6oQYMGyd/fv9jzkKS8vDytXr1aERERzr+b2bNnKzs729wqwzC0fft2Pf3006pQoYJq1aqlsWPH8jcGAAAAPKAsRlEpAHfl/fffV3p6uh599FGVLVtW+/bt08cff6ywsDDNnz9fQUFB5im4zm63a/To0dq4caP69++vdu3a6eLFi1q0aJHOnz+vTz75RI8//rh5mtPdzt+7d68+/vhjdejQQVWrVtW///1vJSQk6MSJE4V6JWnatGn67LPPFB0drXLlyjnHy5Qpo5iYGAUHBzvHtm/froEDByosLEwvvPCCJOmjjz4q8th5eXmaOXOm3nnnHQ0ZMkQdO3bUnj17NHv2bMXExOidd96Rj4+PdD1oL126VCNGjFCvXr305JNPKikpSfPnz+dvDAAAAHhAEbbvk23btik2NlaTJ0/W4MGDzWVcl5OTo7Vr1yoqKkq+vr7O8ZSUFL3wwgsKCAjQnDlznMHT7F7n6za9kydP1qFDh/TRRx+pYsWKLvNu9uuvv+qFF16QxWLR/Pnz5efnJ9107MqVK+vDDz90nuOePXsUHR2tqVOnql+/frJYLM5QPWTIEC1ZskS9evWSJJ05c0b9+vXT008/rZEjR8rT01O6Kdz/v//3/zRixAhZLJabzggAAABASWIZ+X3StGlTNWnSRBcvXjSXcJNy5copJibGJShLUnBwsNq3b6+kpCRduXLFpXaze52v671t2rTRpUuXZLfbneOGYSg/P1/+/v7y8vJymWNmtVp15swZNW7c2Bm0ddOxU1NTlZubK0nKz89XYmKiWrRooW7dujlDssViUbdu3RQREaFNmzYpJydHun7hRpKio6OdQVuS2rZtqx49emjz5s1KS0tzjgMAAAAoeYRtPLAqV64sh8Oha9eumUt35F7n2+12paSkmIeLFBAQoNq1aysrK0t5eXnOcYfDoStXrsjX19cZlDMyMnTgwAE1adJElSpVuukoUqVKldSkSRMlJycrKytLdrtdBw4cUOPGjQstFS9TpoxatmypCxcu3PaCAgAAAIDfF2H7Prlw4YIuXryoP/zhD+YS7oDD4dCxY8dUu3ZtBQQEmMu3dTfzMzIy9NNPP6l169aFwq8kVatWzeV57aJUrFhRw4YN09q1a/Xpp58qJydHOTk5+vTTT7VhwwYNGzbMuQw9NTVVP//8s+rWrVto6bfFYlHdunVltVqVnp6u7OxsJScnq2bNmvL29nbplaTQ0FAdPnyYO9sAAADAA4aw7Wa5ubn65ptv9Prrr+upp54ibP9Gu3bt0tq1axUZGVnss9K3cifzCwoKdPr0aU2cOFEpKSl67rnn5OHh4azn5uYqLS1N48ePl8ViUYUKFdStWzf985//LHLH8Mcee0wzZszQm2++KW9vb3l7e2vs2LGaOXOmHnvsMWdfVlaWUlNTXZaEm6WmpiorK0tZWVnKzMwstlcSYRsAAAB4wBC23SAnJ0dDhgyRxWJRmTJl9Pzzz2vQoEF6++23b3tHFIUdOXJEb7zxhqKiohQTE2Mu39bt5t94nZeHh4dCQ0NVpkwZLVu2THXq1HHp8/DwUJcuXZSYmKgtW7ZoypQp8vT01IABA/TSSy+5BFzDMLRx40aNHDlS0dHR2rRpkxITExUeHq7XX39d3377rbPXbrfr0qVLql69unPM7Mbz43a7XdnZ2apZs6a5xUVmZqZ5CAAAAEAJImy7gZeXl4YMGaKvvvpKiYmJio6O1htvvKHo6GidO3fO3I5bKCgo0Pr169W7d2+1bt1a8fHxxe4ibnan8zt06KCvvvpKW7Zs0axZs7Rz5061a9dOGzdudHkfdvny5RUXF6eoqCiFh4fr5Zdf1tq1a/XZZ59p9erVWrNmjbM3KSlJb775pgYPHqxZs2bpiSeeUFRUlD799FM9//zzGjVqlE6dOuXslySbzeby883q1q3rsulbVlaWS90sMDDQPAQAAACgBBG23cDDw0PNmjVTp06dFBUVpWnTpmn37t0qKCjQ66+/rqtXr5qnwCQnJ0dTp07ViBEjNH78eL333nuFdhgvzt3MDwkJUadOnRQeHq5XXnlF27Zt0/PPP68hQ4Zo//795nYXpUqVUlRUlCIiInTgwAHn7uVff/21fHx8FBMT47Lk29PTU9HR0dJNu4pXqVJFTZs2VWpqqrPPzNvbW2XKlFGVKlUUHBxcbK+u9wMAAAB4cBC275OgoCD16tVLSUlJtw1K/+mys7P12muv6dtvv9W6devUv3//2z6jfLN7ne/p6akePXqoQoUKOnnypLlcSNmyZRUYGKj8/HznnfDU1FT5+vqqfPny5nZVqFBBlSpVcv4d+Pr6ys/PT+fPnze3yjAMnTp1StWrV1dgYKC8vLwUGBiolJQUl9eS3ZCcnKyWLVsW2qkcAAAAQMkibKNE5efna+7cubpw4YIWL16ssLAwc0ux7nX+b2G325Wamipvb2/nhmqenp765ZdflJGRYW5XRkaGfvnlF+cFAD8/P9WvX1/Hjx8v9Kz1L7/8okOHDqlFixaqUKGCypcvr3r16unYsWNKT0936XU4HNq/f78aN26shx56yKUGAAAAoGQRtu/RwYMHtXPnTpdnfSXp8uXLWr16tZo3b17sRlj/6X7++WetWbNGzz77rKpVq2Yu39bdzL969aqWL18uh8PhMp6Xl6e1a9cqLy9PzZo1c6mZFRQUaN26ddqyZYuioqLk5eUlSQoPD9f58+e1YsWKQu/ZXrZsmc6fP6/w8HBJUrly5dStWzd9+eWX2rx5s/NvxzAMbdiwQUeOHFH37t3l4eEhi8WiyMhInTlzRsuXL1dBQYHz2Lt27dKGDRvUs2fPIp9NBwAAAFByLIY5JeKu7Ny5U926ddOTTz6pPn36qHz58tq1a5c+//xzeXt76+OPP1aDBg3M03Ddzp071aFDBw0bNuyWFyUaNWqkqKgo5eTk6K233tLevXu1ZMkSPfzww3c1Py0tTQMGDJDNZlO/fv3UoEEDnTx5UomJidq+fbv+/ve/q1+/fs53X+/atUtz587V448/rtDQUF28eFFr1qzR559/rvj4eI0cOdJ5tzovL08zZ87U6NGj1adPH3Xt2lV5eXlKTExUYmJiof7s7GyNGjVKy5Yt0/Dhw9WuXTt9//33mj17tkaNGlXksadMmaKBAweqe/fuOnLkiObPn6/w8HC98847hG0AAADgQWPgntjtdmPlypVGZGSk4evra0gyHn/8ceODDz4wrly5Ym6HyY4dOwxJxX4mTZpkGIZh2Gw245VXXjHat29vnDp16q7nFxQUGAcPHjQGDx5shISEGJKM+vXrG6+++qpx7Ngxo6CgwOXcTpw4YURHRzt7K1eubPTp08fYtm2bkZ+f79JrGIaRn59vbNu2zejZs6fh6+tr+Pr6Gj179rxlv81mM2bPnm3Ur1/fkGSEh4cbq1atMq5du2ZuNa5du2YsXbrUaNOmjSHJaNOmjTFv3jzDZrOZWwEAAAA8ALizDQAAAACAm/HMNgAAAAAAbkbYBgAAAADAzQjbAAAAAAC4GWEbAAAAAAA3I2wDAAAAAOBmhG0AAAAAANyMsA0AAAAAgJsRtgEAAAAAcDPCNgAAAAAAbkbYBgAAAADAzSyGYRjmwZJis9nMQwDuE29vb/MQAAAAADfhzjYAAAAAAG5G2AYAAAAAwM0I2wAAAAAAuBlhGwAAAAAANyNsAwAAAADgZoRtAAAAAADcjLANAAAAAICbEbYBAAAAAHAzwjYAAAAAAG5G2AYAAAAAwM0I2wAAAAAAuBlhGwAAAAAANyNsAwAAAADgZoTt+yA3N1evvfaa2rZtq6SkJHMZJjabTQsXLlTXrl3l4+OjGjVqaMiQITp8+LAMwzC3F6ugoEAbN27UgAEDlJaWZi5LN/17nTp1ko+Pjzp16qSFCxfKZrOZWwvZt2+fQkNDFR8fby4VcvXqVfXr1089evQo8lzy8vK0du1aPfnkk/Lx8VHz5s01d+7cIs/DMAzt2LFDMTExCg4OVoMGDfT222/rypUr5lYAAAAADwDC9n2wd+9effHFF+ZhFMHhcGjChAmaO3euOnXqpHXr1mnatGlKTk5WdHS0du7caZ5SpIKCAh08eFCDBg3SM888o6ysLHOLdD1ojxkzRpMmTVLXrl21bt06de3aVZMmTdKYMWOKDLo32O12ffrpp0pJSTGXivTll19q3bp15mHpetCePXu2hg0bplatWmnNmjWKjo7WtGnTCp2HYRhasWKFYmNjFRQUpCVLluill17SmjVr9NJLLyk1NdXl2AAAAABKHmHbzTIyMvTJJ5+Yh3ELhmGoTZs2+uabb/Tmm28qPDxcsbGx+sc//qFHHnlEn332WbEBWJLOnj2rHj16qH379rp27ZoiIiLMLU5fffWVli5dqnnz5jn/vdGjR+udd97RggUL9P3335unOO3YsUPr16+Xr6+vuVTIxYsXlZCQoICAAHNJknTw4EF99NFHevfddzVhwgQ98cQTGj16tN59910tXbpUmzdvdvaeP39e8+bN08svv6yZM2cqMjJSw4YN0wcffKBDhw5p+fLld70CAAAAAMD9Rdh2s02bNunSpUv6y1/+Yi6hCGXLllXv3r0LBdigoCC1bdtWp06duu1S6fz8fIWEhGjLli1auHChQkJCzC1Ox44dU61atVSvXj3nmMViUcuWLdW4cWNZrVaX/huuXLmiRYsWKS4uTo8++qi57CI/P1/Lly/XQw89pN69e5vLys/P14YNG9SsWTN16dJFFotFun4eXbp0UefOnbV161bZ7XZJ0tdffy1J6t27tzw9PZ3Had26taKiorR161alp6c7xwEAAACUPMK2GyUnJ+vDDz/Uiy++WGzgw52pXLmycnNzlZeXZy65CA0N1Zw5c9SuXTuVKlX8n/Qjjzwim82mnJwcl/GMjAxZrdZCoV/Xw/Fnn30mDw8PRUdH3/bf2Lt3r5YvX64hQ4aoSpUq5rIyMjJ08OBBNWrUSJUqVXKpVapUSY0aNdKZM2eUlZUlh8OhgwcPqmHDhgoMDHTp9fLyUvPmzXXx4sXbXpAAAAAA8PsqPjXgjtlsNs2ZM0ctWrQodhkz7kxubq6OHz+ukJAQ+fv7m8u/WefOnRUeHq74+HglJyeroKBAycnJmjZtmrp3767OnTubp+j777/X8uXLNWzYMFWsWNFcdpGamqpZs2apb9++atq0qbksSbJarbpw4YJCQ0Odd7VvsFgsCg0NldVqVXp6urKysnT69GnVqFFD5cqVc+nV9QsNR48e1dWrV80lAAAAACWIsO0GhmHoX//6lw4ePKi4uDiVLVvW3IK7tGfPHq1fv15dunS5bcC9G97e3ho/frx8fHzUpEkT+fr6qkmTJvLz89P48ePl7e3t0p+amqoPPvhAffr0UatWrVxqZnl5efrss88kSQMGDJCHh4e5RZKUnZ0tq9Wq0qVLm0tOVqtV2dnZys7OVmZm5i2PdUNRu50DAAAAKDmEbTfYvXu3pkyZopdeeklhYWHmMu7S0aNHNXbsWEVGRqpPnz7m8j1JT0/XuHHjlJSUpPnz52vDhg2aPn269u/fr6lTp7psxma32zV9+nSVKlVKAwcOdHleuihr1qzRJ598ohEjRhRa8n0zh8OhlJQUVatWzVxySklJkcPhkN1ul81mU40aNcwtLjIzM81DAAAAAEoQYfse3Vg2PHDgQPXs2dNcxl248Y7s2NhYtWzZUpMmTSp0p/le5Ofna9GiRTpy5IgWLFig2NhYPfbYYxo2bJiWLl2qvXv3av78+crPz5dhGFq7dq0OHDigCRMmFPns9c1OnDihDz/8UGPGjFGbNm3M5SJlZ2ebh5xCQ0NVvnx558/F9Uq65a7nAAAAAEoGYfse2Gw2TZs2TaVLl9agQYNue+cTt2a32zVjxgyNGjVKo0eP1vTp04vcrOxepKamatu2berdu3ehFQj169fXM888o02bNik1NVU7d+7UlClTNHLkyEK9ZqmpqZowYYLatWunp59+utBz2GaVK1cududzXV/u7uXlJX9/fwUFBRXbK0k+Pj7mIQAAAAAliLB9Dw4dOqQFCxboiy++UPXq1eXj4+P8DB48WIcPH1bz5s3Vtm1bJSUlmafjOpvNpjfffFPfffedPv/8c/Xt2/e+XLjIzMzU1atXi7xLbbFYFBAQoKtXr+rKlStasWKFkpOT1bdvX5ffa82aNbVlyxZNnjxZPj4+io+P17Zt25SYmKjZs2fLz8/PpX/y5MnasmWLatasqR49eigtLU2+vr7y8/PTv//9b/NpyDAMJScnq1q1agoICJCXl5cCAwN1+fJlORwOc7uSk5PVvHnzYpetAwAAAPj9WQzDMMyDJeXm52X/L0hOTtaaNWvMw5Kk48eP66uvvlL//v0VHBys3r17KygoyNz2Hy8/P1/vv/++du3apVmzZqlq1armlrtit9s1atQonTt3TosXL3bZyfz06dN69tln1bdvXw0bNszlDrRhGJozZ46WL1+uRYsW6dChQzp//ryzfoPdbtfq1asVEhKi9u3bq2HDhqpcubK+/fZbc6skaefOnTp37px69eqloKAg9evXTx4eHho1apRycnL03nvvudzBv3r1qoYOHapHHnlEb731lkqVKqV33nlHX3/9tRYvXqyHHnrI2Zubm6u33npLmZmZmjVr1l0vub/bfgAAAAB3jrB9nyxbtkx/+9vf9Omnn6pevXrmMq47f/68nnvuOQ0fPlw9evQwl+9acWH7Rjjdt2+f/v73v7ssDz969KheeukltWrVSlOnTpWXl5ezdrO0tDQNGjRI7dq10+jRo83lQuLj4/X9998XOpfExEQNGzZMs2fPVvfu3WWxWGQYhlasWKEpU6ZoyZIlatGihSTpwIEDGjBggF566SUNHTrU+Z7vHTt2aOjQoZo6daqefPJJ57HvFGEbAAAAuH9YRo4SdfHiRe3evVvffPONZs6cWeRn48aN0k1BOjw8XKdPnzYf6ra8vLz0wgsvSJL69eunyZMna+XKlZowYYJ69+4tSXrhhRduGbTd6Y9//KOefvppDRs2TPHx8dq6davi4+P12muvaeDAgWrSpImzt0mTJnrhhRf017/+Va+//rq2bt2qOXPm6OWXX1Z4eLj++Mc/uhwbAAAAQMkjbOOB8Pe//13jx48v8nPw4EFz+28WFhamlStXasCAAVq9erUGDhyodevWafDgwVq5cuVtN0NzF29vb02bNk1vvfWWVqxYoe7du2vnzp2aM2eOhg8f7vLMuqenp4YPH645c+Zo//796t69u1auXKmXXnpJ06ZN4w41AAAA8ABiGTnwH4qQDgAAANw/3NkGAAAAAMDNCNsAAAAAALgZYRsAAAAAADcjbAMAAAAA4GaEbQAAAAAA3IywDQAAAACAmxG2AQAAAABwM8I2AAAAAABuRtgGAAAAAMDNCNsAAAAAALiZxTAMwzwIAAAAAAB+O+5sAwAAAADgZoRtAAAAAADcjLANAAAAAICbEbYBAAAAAHAzwjYAAAAAAG5G2AYAAAAAwM0I2wAAAAAAuBlhGwAAAAAANyNsAwAAAADgZoRtAAAAAADcjLANAAAAAICbEbYBAAAAAHAzwjYAAAAAAG5G2HaDtLQ0RUZGymKxFPpMnjzZ3A6T7OxszZs3T506dZLFYpG/v78GDRqkH3/8UYZhmNuLVVBQoPXr16tPnz5KS0tzqZ08eVLNmjUr9Du6+dOsWTOdPHnSZd7N9uzZo6pVqxb5e7VarYqPj1eLFi1ksVhUq1Ytvfrqqzp79qy5VZKUl5en1atXKyIiQhaLRWFhYZo9e7ays7PNrTIMQ9u3b9fTTz+tChUqqFatWho7dqysVqu5FQAAAMADwGLcbZpBIT/99JNiYmLUoEEDNWrUyKXWqFEjRUVFuYzhf9ntdo0ePVobN25U//791a5dO128eFGLFi3S+fPn9cknn+jxxx83TyukoKBABw8e1LvvvqulS5eqa9euSkhIkL+/v7MnJSVFy5Ytk8PhcJkrSbm5uVq/fr3q16+vDz74QBUqVDC3KCcnRyNGjND8+fM1adIkjRs3zlm7evWqnn/+eV28eFH9+/dX48aNdeTIES1atEje3t76+OOP1aBBA2d/Xl6eZs6cqXfeeUdDhgxRx44dtWfPHs2ePVsxMTF655135OPjI10P2kuXLtWIESPUq1cvPfnkk0pKStL8+fMVFham+fPnKygoyHlsAAAAAA8AA/fsxIkTRtOmTY21a9eaS7gNm81mLF261MjIyHAZv3TpkhEVFWU899xzRlZWlkvNLDk52YiIiDAkGb179za6du1qdO3a1bhy5Yq59Za2b99u1K9f39i+fbu55PTll18aDz30kOHr62tMmjTJpZaenm4sW7bMsNvtLuPHjh0zmjZtaowbN87Iy8tzju/evdsICQkxEhISjIKCAsMwDKOgoMBISEgwfH19jVWrVjl7T58+bbRp08aIj483rl275hz/+uuvjZCQEGPmzJnOYwAAAAB4MLCM3A0Mw1BBQYHLXVTcmXLlyikmJka+vr4u48HBwWrfvr2SkpJ05coVl5pZfn6+ateurW+//VZLlixRrVq1zC3FysnJ0T//+U916tRJjz76qLksXV8i/ve//13/9V//pTZt2pjL8vPzU9++fVWmTBmX8dq1a6t9+/Y6fvy4srKypOvnm5iYqBYtWqhbt26yWCySJIvFom7duikiIkKbNm1STk6OJGnbtm2SpOjoaHl6ejqP3bZtW/Xo0UObN28utGQeAAAAQMkibLtBWlqaDh8+bB7GPapcubIcDoeuXbtmLrmoW7euFixYoA4dOqhUqbv/k967d6++/vpr9e/fX+XKlTOXlZ+fryVLlsjT01P9+vWTh4eHueWWSpcurYoVKyonJ0cFBQWSpIyMDB04cEBNmjRRpUqVXPorVaqkJk2aKDk5WVlZWbLb7Tpw4IAaN25caKl4mTJl1LJlS124cOG2FyQAAAAA/L7uPpmgSE2bNlWVKlXMw/iNHA6Hjh07ptq1aysgIMBcdhuHw6FVq1apXbt2atmypbksSdqxY4cSEhI0fPhw+fn5mcvFysjI0IkTJ1SvXj3nM9ipqan6+eefVbduXedd7RssFovq1q0rq9Wq9PR0ZWdnKzk5WTVr1pS3t7dLrySFhobq8OHD3NkGAAAAHjCEbTdIS0vTjz/+qLCwMOeu0iNHjtTx48fvejdt/I9du3Zp7dq1ioyMVMWKFc1lt/nxxx+1du1axcbGOsPwzS5fvqxZs2apX79+RS4fL45hGNqwYYN27typqKgoeXl5SZKysrKUmprqsiTcLDU1VVlZWcrKylJmZmaxvbr+NwgAAADgwUHYdoPAwEB99NFH+uqrr7R69Wo99dRTWrVqlbp166a1a9ea23EbR44c0RtvvKGoqCjFxMSYy25z87PTLVq0MJeVk5Ojv/71rypVqpQGDRp028B7M8Mw9NVXX+mtt97Sq6++6rKjut1u16VLl1S9enWXOTe7dOmS7Ha77Ha7srOzVbNmTXOLi8zMTPMQAAAAgBJE2HaDtm3bavDgwerUqZOefvppzZgxQzt37lSzZs304Ycf6tKlS+YpKMKNd2T37t1brVu3Vnx8fJF3m93l/Pnz2rRpk6KiogotDzcMQ1988YX27dunKVOm3NVSdofDoSVLlig2NlZDhw7VK6+8UmRQt9ls5iGnunXrumwad2NztVsJDAw0DwEAAAAoQYTt+6RatWp69tlntXv3bp07d85chklOTo6mTp2qESNGaPz48XrvvfcK7VDubj/88IPsdrs6dOhgLumbb77RxIkTNXr0aJf3Y99Oenq6Xn31Vc2ePVufffaZXn/99UI7lFepUkVNmzZVamqqy/jNvL29VaZMGVWpUkXBwcHF9up6PwAAAIAHB2H7PgoKClJmZqby8/PNJdwkOztbr732mr799lutW7dO/fv3L/JOsDvl5ORo06ZNatu2baEl2jk5OVq6dKlOnTqlnj17ymKxOD9VqlTRl19+qfHjx8tisWjy5MnOeZcvX9agQYOUmZmpjRs3Kjw8vNAGaJLk6+srPz8/nT9/3lySYRg6deqUqlevrsDAQHl5eSkwMFApKSmy2+3mdiUnJ6tly5aFdioHAAAAULII2/fR5cuX9dBDD6ls2bLmEq7Lz8/X3LlzdeHCBS1evFhhYWHmlvsiNTVVhw8fVseOHQu97svDw0OPP/644uPjC30mTJigBg0a6E9/+pPi4+Odz3rfeL67atWq+vDDDxUcHOxyzJv5+fmpfv36On78eKFnrX/55RcdOnRILVq0UIUKFVS+fHnVq1dPx44dU3p6ukuvw+HQ/v371bhxYz300EMuNQAAAAAli7B9n/z8889avHixunbt+rsFyP+Lfv75Z61Zs0bPPvusqlWrZi7fN0lJSbJarWrcuLG5JC8vL/Xr109vvPFGoc/LL7+smjVrqm3bts5N3CTp2LFj2rx5s5577rnbLn8vV66cunXrpi+//FKbN2927lh/Y/fyI0eOqHv37vLw8JDFYlFkZKTOnDmj5cuXO9/Vres7tm/YsEE9e/a8r8+2AwAAALh7HhMnTpxoHsSdy8nJ0YQJE/Ttt98qOztb586dU0JCgl5//XUVFBRoxowZqlq1qnkarjt69KjefvttBQYG6vDhw9q5c2ehz6+//qp69eopJydHo0aN0l//+lc99thjqly5svlwysvL05dffqlffvlFzzzzzC2fZV6xYoWuXr2q55577q6Cak5OjlavXq2aNWu67DD+1VdfKTExUaVLl9aBAwcK/Tfs3LlTnp6ezh3Ia9SooQsXLmjatGlyOBzKzc1VQkKCJkyYoJdfflm9evVSqVL/cy0sMDBQ165d06RJk3Tp0iWVLl1a69at08SJExUREaEXX3zR+VoxAAAAAA8Gi8GLoO9Jbm6uPvroI3322WfatWuXJKl58+aKjo7W4MGD72oX6/9EO3fuLHKDsptNmjRJ48aNU05Ojt566y3t3btXS5Ys0cMPP2xuVU5OjkaMGKGzZ88qISFB/v7+5hZnjyTNmjWr0DLy4qSlpWnAgAFq3769xo0b5xxPSEhQbGysS6/ZP/7xDw0YMMD5c05OjhYuXKi5c+fq5MmTCg8P19ChQ9W9e/dCz6zn5eVp5cqV+tvf/qbdu3erTZs2eu655zRw4MC7On8AAAAAvw/CNgAAAAAAbsYz2wAAAAAAuBlhGwAAAAAANyNsAwAAAADgZoRtAAAAAADcjLANAAAAAICbEbYBAAAAAHAzwjYAAAAAAG5G2AYAAAAAwM0I2wAAAAAAuBlhGwAAAAAAN7MYhmGYB0uKzWYzDwG4T7y9vc1DAAAAANyEO9sAAAAAALgZYRsAAAAAADcjbAMAAAAA4GaEbQAAAAAA3IywDQAAAACAmxG2AQAAAABwM8I2AAAAAABuRtgGAAAAAMDNCNsAAAAAALgZYRsAAAAAADcjbAMAAAAA4GaEbQAAAAAA3IywDQAAAACAmxG23chms2nhwoXq1KmTfHx8VKNGDb3++uvKzs42t+ImN763rl27Or+3IUOG6PDhwzIMw9xeyN3MLygo0Pbt2xUbG6saNWrIx8dHTz75pNauXavc3FyX3qLs27dPoaGhio+PdxnPz8/Xf//3f+vPf/6zGjRoIB8fH/3hD3/Q3LlzZbPZXHpvyMvL09q1a/Xkk0/Kx8dHzZs3v2W/YRjasWOHYmJiFBwcrAYNGujtt9/WlStXzK0AAAAAHgCEbTdJTk5Wv379NHfuXD311FNKTEzUu+++q7Jly95RiPtP5XA4NGHCBM2dO1edOnXSunXrNG3aNCUnJys6Olo7d+40T3Fxt/PXrVungQMHqmLFivrwww+1atUqVa1aVS+++KI++OAD5eXlufTfzG6369NPP1VKSoq5pGvXrungwYNq3ry55syZo3Xr1qlLly6aNm2axowZUyhA5+Xlafbs2Ro2bJhatWqlNWvWKDo6ush+wzC0YsUKxcbGKigoSEuWLNFLL72kNWvW6KWXXlJqaqrLsQEAAACUPIthvvVXgsyB5P+KjIwMvfbaa/r11181a9YsVa1a1dyCW7Db7UpMTFTXrl3l6+vrHL98+bKGDh2qKlWqaNasWfL29naZd8Pdzt+6dauqV6+u+vXrO3vz8vI0c+ZMrVixQp999pkeeeQRZ+1mW7Zs0Ysvvqjs7Gy98sorGj16tLnFhWEYWrZsmUaMGKGlS5eqc+fOztq+ffv05z//WRMnTlR0dLQsFoszVA8fPlzz589Xjx49JEnnzp3TwIED9dRTT2n48OHy9PSUJO3YsUNxcXEaOnSohg0bJovF4jz+nbjVdwoAAADg3nFn2w2++eYb7dq1S2+88QZB+y6VLVtWvXv3dgnKkhQUFKS2bdvq1KlTxS6Vvtv54eHhLkFbkjw9PdW5c2dduHBBZ86ccandcOXKFS1atEhxcXF69NFHzeUiWSwWtWzZUrVq1dLly5ed4/n5+dqwYYOaNWumLl26OEOyxWJRly5d1LlzZ23dulV2u12S9PXXX0uSevfu7QzaktS6dWtFRUVp69atSk9Pd44DAAAAKHmE7Xtkt9u1adMmdenSRY0aNTKXcQ8qV66s3NzcYpd2F+du5nt7eyswMNAZcG+Wn5+vzz77TB4eHoqOjlapUvf2v01GRoYOHjyoRo0aqVKlSi61SpUqqVGjRjpz5oyysrLkcDh08OBBNWzYUIGBgS69Xl5eat68uS5evFjsBQkAAAAAv797Sw1Qamqqjhw5oubNm8vLy8tcxm+Um5ur48ePKyQkRP7+/ubybd3t/OTkZGVnZ6tOnTrmkr7//nstX75cw4YNU8WKFc3lYp0/f14FBQVq0qSJc8xqterChQsKDQ0ttPTbYrEoNDRUVqtV6enpysrK0unTp1WjRg2VK1fOpVeSQkNDdfToUV29etVcAgAAAFCCCNv3KC0tTefOnVONGjW0bds2587S7BZ9b/bs2aP169erS5cudx1wdZfzU1NT9c9//lMRERGFlpinpqbqgw8+UJ8+fdSqVSuXWnFsNpv++7//W+PHj1dcXJzLcbOzs2W1WlW6dGmXOTezWq3Kzs5Wdna2MjMz5eHhYW5xkZaWZh4CAAAAUIII2/fI4XAoJSVFn376qSZNmqQePXooMTFRzz77rD766CN2i/4Njh49qrFjxyoyMlJ9+vQxl2/rbuZnZmZq6tSpunz5skaMGOGyaZjdbtf06dNVqlQpDRw40OV56aKkpaWpR48e8vHxUUBAgP7617/qnXfe0eDBg13C8o2/mWrVqrnMv1lKSoocDofsdrtsNptq1KhhbnGRmZlpHgIAAABQggjbbpKZmakVK1YoLi5OnTt31ptvvqmlS5fq0KFDWr58eaH3PaOwgoICbdy4UbGxsWrZsqUmTZp0Vztm3+38s2fP6i9/+YsOHz6suXPnKiwszFkzDENr167VgQMHNGHCBFWpUsVlblF8fX01ZswYbdiwQUuXLlWrVq3Ur18/jRw5ssgNzIp7/3poaKjKly/v/Lm4XkkKCAgwDwEAAAAoQYRtN+nevXuhDayaNm2qDh066MSJE0VuvIX/ZbfbNWPGDI0aNUqjR4/W9OnTC+0wXpy7mW8YhjZv3qynn35aderU0eeff17odV87d+7UlClTNHLkSJcQXhwvLy+1bt1ajz32mLp37673339fmzdv1g8//KB3333X+b71ypUrq3HjxrJareZDOHl7e8vLy0v+/v4KCgoqtleSfHx8zEMAAAAAShBh+x75+fmpYcOGRT5TW7p0aZUrV04pKSmE7WLYbDa9+eab+u677/T555+rb9++t12yfbO7mX/jXdYjR47U+PHjNXnyZFWuXNmlx263a8WKFUpOTlbfvn3l4+Pj/NSsWVNbtmzR5MmT5ePjo/j4eJe5ZvXr19cTTzyho0ePOpd6+/r6ys/PT//+97/N7TIMQ8nJyapWrZoCAgLk5eWlwMBAXb58WQ6Hw9yu5ORkNW/evNCFHgAAAAAli7B9j4KDg1W3bl1ZrdZCS8WvXbumnJwc1axZs9jlzP/J8vPzNX/+fF28eFHz5s0rtEHZ7dzt/N27d2vWrFl6//331bNnzyJf41WqVCl17NhRkyZNKvR56623FBYWpq5du2rSpElq1qyZefpt+fn5qW7dukpKSir0rPUvv/yiI0eOqFmzZqpQoYLKly+vhx9+WCdOnCi0FD03N1c//PCDGjZsqODgYJcaAAAAgJJVOGngrlSsWFFdunTRqlWrdPLkSZfa/v37tWPHDoWHh6tMmTIuNfyPCxcu6F//+pf69++vqlWrmsu3dTfzc3Nz9cUXX6hDhw76wx/+UOi1Wzd4eXmpT58+GjlyZKHPkCFDVL16dbVu3VojR45UZGSkJGnr1q2Ffv+SdPLkSW3evFmtWrVyvlO7bNmy6tKlizZv3qxt27Y5L9IYhqFNmzbp6NGjioqKkoeHhywWi5544gmdPXtWq1atUkFBgfPYe/bs0aZNm/TUU09xMQcAAAB4wFgM8+3YEmSz2cxD/ydcuXJFr776qg4fPqzY2Fg1adJEO3fu1KJFixQTE6O3336bMHQLu3btUnh4uIYMGXLLsNywYUNFRkbKbrdr/Pjx2r9/vxYuXKg6derc1fy0tDQNGjRIHh4eat++vblNun7xpF+/frd8BvrGMdq1a6fRo0c7x5ctW6bXX39dffr0UZcuXaTr7+desWKFmjRpog8++MBlqbfNZtOYMWO0cuVKDR06VK1bt9aePXs0d+5cvfLKKxo+fLhzKXxeXp5mz56tGTNmaMCAAfrTn/6k48ePa9GiRerUqZOmTJnym/6+fsscAAAAAHeGsO0mNptNS5Ys0cKFC5WUlKTOnTsrLi5O3bp1k5eXl7kd190Iy8UZN26cRo8eXWzYLs6N+TeC8pYtW8wtThEREVq8eLH8/f3NJamYsG2z2ZSQkKCVK1dqx44d8vX1Vdu2bdWvX79b3nm22+36+OOPtWDBApe/maioqELPnOfl5WnNmjWaM2eO9u7dq0cffVSxsbGKjY1V2bJlXXrvVFHnBAAAAMA9CNvAfyjCNgAAAHD/8Mw2AAAAAABuRtgGAAAAAMDNCNsAAAAAALgZYRsAAAAAADcjbAMAAAAA4GaEbQAAAAAA3IywDQAAAACAmxG2AQAAAABwM8I2AAAAAABuRtgGAAAAAMDNLIZhGOZBAAAAAADw23FnGwAAAAAANyNsAwAAAADgZoRtAAAAAADcjLANAAAAAICbEbYBAAAAAHAzwjYAAAAAAG5G2AYAAAAAwM0I2wAAAAAAuBlhGwAAAAAANyNsAwAAAADgZoRtAAAAAADcjLANAAAAAICbEbYBAAAAAHAzwvY9SkhIkMViKfYzZMgQ5eTkmKfiuuzsbM2bN0+dOnWSxWKRv7+/Bg0apB9//FGGYZjbi5Senq74+HiFhYXJYrEoIiJCW7ZsUUFBgblVeXl5Wr16tSIiImSxWBQWFqbZs2crOzvb3CrDMLR9+3Y9/fTTqlChgmrVqqWxY8fKarWaW104HA59/PHHeumll275u/89zgMAAABAySBs36PQ0FDFx8cX+Rk5cqQCAwPVsmVLlS1b1jwVkux2u8aMGaPZs2frj3/8ozZt2qT33ntPP/30k3r06KFvvvnGPKWQy5cva9CgQVq9erWGDx+uxMREBQYGqlevXlq2bJlLYM/Ly9PMmTMVFxenNm3aaMOGDerfv78mTZqkUaNGuQRdwzC0dOlSRUdHKzg4WEuXLtXw4cO1cuVKxcXF6fLly87eG/Ly8rR161b16NFDgwYNUn5+vrlF+h3OAwAAAEAJM3Bf5OXlGVOnTjW6d+9uXL582VzGdTabzVi6dKmRkZHhMn7p0iUjKirKeO6554ysrCyX2s0KCgqMmTNnGm3atDGOHTvmHM/KyjKGDh1qtGnTxjh9+rRzfPfu3UZISIiRkJBgFBQUOI+RkJBg+Pr6GqtWrXL2nj592mjTpo0RHx9vXLt2zTn+9ddfGyEhIcbMmTOdxzAMw9i3b59Rv359w9fX1xg0aJDRtGlT48UXXzRsNpuz54b7eR4AAAAASh53tu+Tn376ScuXL1d0dLQCAwPNZVxXrlw5xcTEyNfX12U8ODhY7du3V1JSkq5cueJSu1lKSoo2bNig6OhohYWFOcd9fHw0cOBAnT9/Xt99950kKT8/X4mJiWrRooW6desmi8UiSbJYLOrWrZsiIiK0adMm57Lvbdu2SZKio6Pl6enpPHbbtm3Vo0cPbd68WWlpac7xa9eu6bHHHtPu3bs1ffp0BQcHO2s3u9/nAQAAAKDkEbbvg/z8fK1evVohISF64oknzGXcocqVK8vhcOjatWvmktO5c+e0e/dutWrVyhlab6hdu7YaNmyoU6dOyTAMZWRk6MCBA2rSpIkqVark0lupUiU1adJEycnJysrKkt1u14EDB9S4cWMFBQW59JYpU0YtW7bUhQsXXC4EtG3bVgsWLFCDBg0KncvN7vd5AAAAACh5hO374Pz581q7dq26d+/OXe3fyOFw6NixY6pdu7YCAgLMZafk5GTVqVOnUBCVpPLlyys0NFQpKSmy2+1KTU3Vzz//rLp16xYKwxaLRXXr1pXValV6erqys7OVnJysmjVrytvb26VX15/VP3z48G+6o/ygnAcAAACA+4ew7WaGYeiLL76Qr6+v/vSnP5nLuEO7du3S2rVrFRkZqYoVK5rLTqmpqfL09JSHh4e55JSWlqbc3FxlZWU5+28lNTVVWVlZysrKUmZmZrG9un7su/WgnAcAAACA+4ew7WY3niHu1q3bLZ/ZRfGOHDmiN954Q1FRUYqJiTGXXWRlZalKlSry8/Mzl5wyMzOVl5cnu92uS5cuqXr16uYWp0uXLslut8tutys7O1s1a9Y0t7jIzMw0D93Wg3IeAAAAAO4fwrabff/997pw4YK6du1aaIkwildQUKD169erd+/eat26teLj4+Xj42NuK8ThcMjhcJiHnQIDA+Xl5eX82WazudRvVrduXZfN2rKyslzqZvfymMCDch4AAAAA3I+w7UYOh0Pbt2/XY489pjp16pjLKEZOTo6mTp2qESNGaPz48XrvvfcK7VBelDp16ujq1avF3tn19vaWp6enqlSpoqZNmyo1NdXc4uTt7a0yZcqoSpUqCg4OLrZX1/vv1oNyHgAAAADuH8K2G509e1bbt29Xx44dVa5cOXMZt5Cdna3XXntN3377rdatW6f+/fvf9hnlGwIDA3X69GldvXrVXFJWVpaSk5MVEhKicuXKydfXV35+fjp//ry5VYZh6NSpU6pevbrzTnhgYKBzczWz5ORktWzZssiN2W7nQTkPAAAAAPcPYduNTp48KUlq1aqVuYRbyM/P19y5c3XhwgUtXrzY5V3Zd6JmzZqqU6eOfvzxR3NJZ86c0alTp9S6dWtJkp+fn+rXr6/jx48XuhP+yy+/6NChQ2rRooUqVKig8uXLq169ejp27JjS09Ndeh0Oh/bv36/GjRvroYcecqndiQflPAAAAADcP4RtN8nPz9euXbvUpEkTVa1a1VzGLfz8889as2aNnn32WVWrVs1cvq1atWopIiJCn332mX766SfneHZ2tj755BO1bt1aLVq0kCSVK1dO3bp105dffqnNmzfLMAzp+t3kDRs26MiRI+revbs8PDxksVgUGRmpM2fOaPny5SooKHAee9euXdqwYYN69ux5R8+Umz0o5wEAAADg/vGYOHHiRPMg7t6vv/6qv//972rYsKG6dOnC5mh36OjRo3r77bcVGBiow4cPa+fOnYU+v/76q+rVq6ecnByNGjVKf/3rX/XYY4+pcuXK8vT0VI0aNfTFF19o5cqV8vLy0oULFzRlyhRt27ZNU6dOdblbXqNGDV24cEHTpk2Tw+FQbm6uEhISNGHCBL388svq1auXSpX6n2tQgYGBunbtmiZNmqRLly6pdOnSWrdunSZOnKiIiAi9+OKLLhuv3SwnJ0erV69WpUqVFBkZqdKlS7vUf6/zAAAAAFBCDLjFiRMnjKZNmxr/+Mc/zCUUY8eOHYakYj+TJk0yDMMwbDab8corrxjt27c3Tp065XKc8+fPG3FxcUblypWNypUrG88//7xx6NAhl54bbDabMXv2bKN+/fqGJCM8PNxYtWqVce3aNXOrce3aNWPp0qVGmzZtDElGmzZtjHnz5hk2m83c6uLKlStG165djRdffPGWvb/HeQAAAAAoGRbjxhpWAAAAAADgFjyzDQAAAACAmxG2AQAAAABwM8I2AAAAAABuRtgGAAAAAMDNCNsAAAAAALgZYRsAAAAAADcjbAMAAAAA4GaEbQAAAAAA3IywDQAAAACAmxG2AQAAAABwM4thGIZ5sKTYbDbzEID7xNvb2zwEAAAAwE24sw0AAAAAgJsRtgEAAAAAcDPCNgAAAAAAbkbYBgAAAADAzQjbAAAAAAC4GWEbAAAAAAA3I2wDAAAAAOBmhG0AAAAAANyMsA0AAAAAgJsRtgEAAAAAcDPCNgAAAAAAbkbYBgAAAADAzQjbAAAAAAC4GWHbTc6fP6833nhDDRo0UHBwsGJiYrR9+3YVFBSYW2Fis9m0cOFCde3aVT4+PqpRo4aGDBmiw4cPyzAMc3uxCgoKtHHjRg0YMEBpaWnmsnTTv9epUyf5+PioU6dOWrhwoWw2m7lVy5Ytk4+PT6FP27ZtlZSU5NKblJSktm3bFur18fHRsmXLXHolKS8vT2vXrtWTTz4pHx8fNW/eXHPnzi3yPAzD0I4dOxQTE6Pg4GA1aNBAb7/9tq5cuWJuBQAAAPAAsBh3m2buo6JCxv8FJ06c0JAhQ1RQUKC//OUvqlKlipYtW6aNGzdq9uzZio6OlsViMU+DJIfDobFjx2rLli2Kjo5W69atdenSJX3yySf6+eeftXDhQnXo0ME8rZCCggIdOnRIf/vb3/T5558rIiJCixcvlr+/v0ufzWbTmDFjtHLlSg0dOlStW7fWnj17NHfuXPXu3VtTpkyRt7e3s3/GjBlatmyZevXqpbJlyzrHy5Ytq969eysoKMg5tmfPHnXv3l0DBgxQ1apVneOS1LFjR7Vu3dr5c15enmbPnq2//e1vGjx4sNq3b699+/YVeR6GYWjFihV644031KNHD3Xr1k0//fSTFi1apHr16umDDz5QYGDgTf/anbn5vxMAAACAexG271Fubq7GjRunffv26eOPP1bNmjWl6/8to0aNUnJyshYvXqyHHnrIPBWS7Ha7EhMT1bVrV/n6+jrHL1++rKFDh6pKlSqaNWtWscHw7Nmzevnll7Vt2zb17NlTWVlZklRk2F6/fr0GDx6sjz76SE8++aR0PcwuW7ZML7zwgtatW6fw8HBnf3x8vA4fPqy5c+eqYsWKNx2psF27dmnAgAFauXKlmjdvbi672Ldvn/785z9r4sSJzosxN0L18OHDNX/+fPXo0UOSdO7cOQ0cOFBPPfWUhg8fLk9PT0nSjh07FBcXp6FDh2rYsGF3fUGnuO8UAAAAwL1hGfk9ys7O1k8//aSGDRu63F309vZWhw4ddPXqVWVmZrrMwf+6cYf45qAtSUFBQWrbtq1OnTp126XS+fn5CgkJ0ZYtW7Rw4UKFhISYW5yOHTumWrVqqV69es4xi8Wili1bqnHjxrJarc5xwzCUn58vf39/lSlTxjl+K/n5+QoICJCPj4+55CI/P18bNmxQs2bN1KVLF2dItlgs6tKlizp37qytW7fKbrdLkr7++mtJUu/evZ1BW5Jat26tqKgobd26Venp6c5xAAAAACWPsH2PfHx89PDDDys7O1sOh8M5bhiGrFarvL29XZYf485VrlxZubm5ysvLM5dchIaGas6cOWrXrp1KlSr+T/qRRx6RzWZTTk6Oy3hGRoasVqtL6Hc4HLp8+bJLX3H+/e9/m4eKlJGRoYMHD6pRo0aqVKmSS61SpUpq1KiRzpw5o6ysLDkcDh08eLDQxRxJ8vLyUvPmzXXx4sXbXpAAAAAA8PsqPpngtry8vPT888/r9OnTmjdvnjIyMpSXl6cNGzZo4cKFevHFF1WjRg3zNNxGbm6ujh8/rpCQkEJLwe9F586dFR4ervj4eCUnJ6ugoEDJycmaNm2aunfvrs6dO5unqGrVqnd8wSQoKOi252u1WnXhwgWFhoYWWvptsVgUGhoqq9Wq9PR0ZWVl6fTp06pRo4bKlSvn0qvrFxqOHj2qq1evmksAAAAAShBh2w3CwsL07rvv6vPPP9dDDz2kihUrqk+fPnr55ZfVu3fvQoEKt7dnzx6tX79eXbp0ue2z0nfD29tb48ePl4+Pj5o0aSJfX181adJEfn5+Gj9+vMtzzA6HQ2lpaZo8ebJ8fHwUHBysnj17avny5UXuL3DlyhVt2bJFNWvWlI+Pj/7whz9o+vTphe54Z2dny2q1qnTp0i7jN7NarcrOzlZ2drYyMzPl4eFhbnFxq53XAQAAAJQMwrYb/PDDDxo5cqQaN26sVatWad26dRoyZIji4+P1xRdf3PXrq/7THT16VGPHjlVkZKT69OljLt+T9PR0jRs3TklJSZo/f742bNig6dOna//+/Zo6dapLiPb09FRERIRWrVqlxMRETZw4UZ6enho0aJBGjBhRKOA+/PDDWrx4sTZs2KDFixerVatWmjNnjvr06aP9+/c7+xwOh1JSUlStWjWX+TdLSUmRw+GQ3W6XzWa77eoI9gUAAAAAHiyE7XtktVo1bdo05zuSIyMjFR4ernfffVdTp07VpEmTtHv3bvM0FOHGO7JjY2PVsmVLTZo0ya07Zufn52vRokU6cuSIFixYoNjYWD322GMaNmyYli5dqr1792r+/PnKz8+Xrj+P//zzzysyMlKdO3fWkCFDtGLFCn300Udau3atEhMTXY4fGRmpvn376rHHHlPfvn31/vvva+PGjSpbtqwWLFhQ6G54dna2y883Cw0NVfny5Z0/F9crSQEBAeYhAAAAACWIsH2PDh06pP379+vZZ591CYY3dpZu2LChNm7c6AxwKJrdbteMGTM0atQojR49WtOnTy+0Q/m9Sk1N1bZt29S7d2+FhYW51OrXr69nnnlGmzZtUmpqqkvtZqVKlXKG74MHD7psileURx55RM8884yOHj2qlJQU6frGb+adz828vb3l5eUlf39/BQUFFdur6xcGAAAAADw4CNv3yGq13vJ1T+XKlVNAQIDS09N17do1cxnX2Ww2vfnmm/ruu+/0+eefq2/fvi6vuHKXzMxMXb16VVWqVDGXZLFYFBAQcEevarvxe83Pz7/tIwI3jpuXl+fcVd3X11d+fn6FnuXW9V3sk5OTVa1aNQUEBMjLy0uBgYG6fPlykcE+OTlZzZs3L7RTOQAAAICSRdi+R6VLl5bValVGRoa5pJycHFmtVnl4eLBJ2i3k5+dr/vz5unjxoubNm6f69eubW9zG09NTnp6eslqthULyjVe13egpzo3fa7ly5W77qrGiXgHn5+enunXrKikpqVCw/+WXX3TkyBE1a9ZMFSpUUPny5fXwww/rxIkThd6lnZubqx9++EENGzZUcHCwSw0AAABAySo+KeC2WrVqpRo1aighIcHlmdwbzx9/9dVX6tSpk8qUKeMyD//jwoUL+te//qX+/furatWq5rJbVa9eXW3bttWqVat08uRJl9qxY8e0atUqtW3bVtWrV3ep3aygoEDr16/XV199pcjISHl5eZlbXBw7dkwJCQnq3Lmzc0O0smXLqkuXLtq8ebO2bdvmDP6GYWjTpk06evSooqKinBdpnnjiCZ09e1arVq1SQUGB89h79uzRpk2b9NRTT7n12XYAAAAA985j4sSJE82DJeX/4lLrihUrytvbWzNmzNCuXbv066+/KikpSe+9954+/PBD9evXT3FxccW+5uk/2fHjxzV16lQFBAToyJEj+v777wt9MjIy9PDDD8tut+utt95SfHy8OnbsKD8/P/PhlJeXp82bN+vXX39Vjx49XEKoh4eHatasqcTERCUkJMhqtcpqtWr58uWaMGGCypcvr/HjxysoKEi6HmYnTpyoq1evKiMjQ7t27dL06dM1Y8YMjR07VtHR0c4722lpaRo1apROnDghm82mpKQkffLJJ3rttdfUuHFjjRs3zuUZ9OrVq+vixYt699135XA4lJubq+XLl2vKlCkaMmSIevTo4Tx2QECArl27pvj4eKWkpMjT01P//d//rSlTpuiPf/yjXnjhhd/09/Vb5gAAAAC4MxbDvJ62BJl3a/6/wjAMHTlyRB9++KHWr1+v9PR0de7cWXFxcerWrdtt737+J9u1a5fCw8PNwy7GjRun0aNHy263a/z48dq/f78WLlyoOnXqmFtlt9s1atQonTt3TosXL5a/v7+5RVeuXNGSJUuUkJCgpKQk1atXTwMGDNBzzz3n8jx3UlKSJk+erH379un8+fOqXLmyHn/8ccXFxaljx44uS8izs7M1c+ZMbdy4UT/++KMkqUOHDurdu7cGDBhQ5J1nu92ujz/+WAsWLFBSUpLzbyYqKqrQUva8vDytWbNGc+bM0d69e/Xoo48qNjZWsbGxzuXpd6uocwIAAADgHoRt4D8UYRsAAAC4f3hmGwAAAAAANyNsAwAAAADgZoRtAAAAAADcjLANAAAAAICbEbYBAAAAAHAzwjYAAAAAAG5G2AYAAAAAwM0I2wAAAAAAuBlhGwAAAAAANyNsAwAAAADgZhbDMAzzIAAAAAAA+O24sw0AAAAAgJsRtgEAAAAAcDPCNgAAAAAAbkbYBgAAAADAzQjbAAAAAAC4GWEbAAAAAAA3I2wDAAAAAOBmhG0AAAAAANyMsA0AAAAAgJsRtgEAAAAAcDPCNgAAAAAAbkbYBgAAAADAzQjbAAAAAAC4GWHbTc6ePatXX31VtWrVUoUKFRQdHa0DBw7IMAxzK0yys7M1b948derUSRaLRf7+/ho0aJB+/PHHO/7+0tPTFR8fr7CwMFksFkVERGjLli0qKCgwtxayevVqWSwWJSQkuIz/8ssvmjVrliIiIlShQgVVqFBB3bp1u+VxDcPQjz/+qEGDBsnf31/+/v76y1/+orNnz5pbJUl5eXlavXq1IiIiZLFYFBYWptmzZys7O9vcKsMwtH37dj399NOqUKGCatWqpbFjx8pqtZpbAQAAADwACNtusH37dnXq1EnHjh3Tu+++q08++USS1KtXL33zzTfmdtzEbrdrzJgxmj17tv74xz9q06ZNeu+99/TTTz+pR48ed/T9Xb58WYMGDdLq1as1fPhwJSYmKjAwUL169dKyZcuKDeypqanO35fZlStXlJycrH79+mndunX65JNP5OvrW+RxDcPQ0qVL1bFjR9ntdn366af629/+pvPnzysmJkbHjx93OXZeXp5mzpypuLg4tWnTRhs2bFD//v01adIkjRo1yiVw3zh2dHS0goODtXTpUg0fPlwrV65UXFycLl++7HJsAAAAAA8AA/fk4sWLRkREhDF06FAjKyvLOW63240333zT6N69u3H58mWXOfhfNpvNWLp0qZGRkeEyfunSJSMqKsp47rnnXL5Xs4KCAmPmzJlGmzZtjGPHjjnHs7KyjKFDhxpt2rQxTp8+7TLnhoKCAmPBggVGYGCg4evra/zjH/8wtxSSlZVlxMXFGREREcbFixed40eOHDEaNGhgxMfHG9euXXOOX7161fjzn/9svPjii4bNZnOO79692wgJCTESEhKMgoICw7h+PgkJCYavr6+xatUqZ+/p06eNNm3aFDr2119/bYSEhBgzZ850HgMAAADAg4E72/foyJEj2r17t/r37y8fHx/neJkyZRQTE6NTp07p8OHDLnPwv8qVK6eYmBj5+vq6jAcHB6t9+/ZKSkrSlStXXGo3S0lJ0YYNGxQdHa2wsDDnuI+PjwYOHKjz58/ru+++c5lzw08//aTFixdr1KhRqlOnjrlcJB8fHz3++OOyWq3KyMhwju/YsUNeXl7q2bOnPD09neOVKlXSn//8Z33zzTc6ffq0JCk/P1+JiYlq0aKFunXrJovFIkmyWCzq1q2bIiIitGnTJuXk5EiStm3bJkmKjo52OXbbtm3Vo0cPbd68WWlpac5xAAAAACWPsH2PUlNTVadOHVWpUsVcUrVq1VSzZk0dOnTIXMIdqFy5shwOh65du2YuOZ07d067d+9Wq1atnKH1htq1a6thw4Y6depUoaXkOTk5ev/999W+fXs98cQTLrXfIjU1VcHBwUX+HdSsWVNeXl5KTk6WJGVkZOjAgQNq0qSJKlWq5NJbqVIlNWnSRMnJycrKypLdbteBAwfUuHFjBQUFufSWKVNGLVu21IULF4q9IAEAAADg90fYvke+vr46ffp0kWHH4XDI4XAoKyvLXMJtOBwOHTt2TLVr11ZAQIC57JScnKw6deoUCqKSVL58eYWGhiolJUV2u905bhiGvvjiC+3bt08vvviiypQp4zKvOHl5eTp27JiaN2+u6tWrO8fLly+vlJSUW/4d2Gw2ZWZmSteD+c8//6y6desWukBgsVhUt25dWa1WpaenKzs7W8nJyapZs6a8vb1deiUpNDRUhw8f5s42AAAA8ID5/8bXYkckWEE8AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer orchestrates the fine-tuning process by combining the model, tokenizer, dataset, and hyperparameters. It ensures efficient training, taking advantage of mixed precision (fp16 or bf16) and memory-efficient optimizations (e.g., AdamW with 8-bit precision). Additionally, it handles sequence preprocessing, gradient accumulation, and learning rate scheduling.  \n",
    "\n",
    "eg:  \n",
    "![image-2.png](attachment:image-2.png)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:46:03.414275Z",
     "iopub.status.busy": "2025-01-28T19:46:03.413997Z",
     "iopub.status.idle": "2025-01-28T19:52:29.397069Z",
     "shell.execute_reply": "2025-01-28T19:52:29.396239Z",
     "shell.execute_reply.started": "2025-01-28T19:46:03.414249Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 52,002 | Num Epochs = 1 | Total steps = 50\n",
      "O^O/ \\_/ \\    Batch size per device = 6 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (6 x 4 x 1) = 24\n",
      " \"-____-\"     Trainable parameters = 1,089,536/5,000,000,000 (0.02% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.936900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.979700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.728800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.825200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.753500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.742700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.740700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.605600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.704400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.688100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/pytorch_model\\\\tokenizer_config.json',\n",
       " 'outputs/pytorch_model\\\\special_tokens_map.json',\n",
       " 'outputs/pytorch_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存完整模型（结构+权重）和分词器 （pytorch原生格式）\n",
    "model.save_pretrained(\"outputs/pytorch_model\")\n",
    "tokenizer.save_pretrained(\"outputs/pytorch_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67d9b64a-528fa7e41fcffba160b613b8;a6821da6-ffea-4101-80dc-b05a45478ef5)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<string>:92\u001b[0m, in \u001b[0;36munsloth_push_to_hub\u001b[1;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\transformers\\utils\\hub.py:872\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[1;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    870\u001b[0m organization \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 872\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_repo(\n\u001b[0;32m    873\u001b[0m     repo_id, private\u001b[38;5;241m=\u001b[39mprivate, token\u001b[38;5;241m=\u001b[39mtoken, repo_url\u001b[38;5;241m=\u001b[39mrepo_url, organization\u001b[38;5;241m=\u001b[39morganization\n\u001b[0;32m    874\u001b[0m )\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\transformers\\utils\\hub.py:679\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[1;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[0;32m    677\u001b[0m         repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 679\u001b[0m url \u001b[38;5;241m=\u001b[39m create_repo(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, token\u001b[38;5;241m=\u001b[39mtoken, private\u001b[38;5;241m=\u001b[39mprivate, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\u001b[38;5;241m.\u001b[39mrepo_id\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3511\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[1;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[0;32m   3510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3511\u001b[0m     hf_raise_for_status(r)\n\u001b[0;32m   3512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:481\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67d9b649-553b5bc96e61922e43754c1c;db795c08-ca9b-4d02-8560-3fc8af8e68f8)\n\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 需提前登录：huggingface-cli login\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscdow/deepseek-r1-distill-qwen-1.5b-unsloth-bnb-4bit-vicgalle-alpaca-gpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscdow/deepseek-r1-distill-qwen-1.5b-unsloth-bnb-4bit-vicgalle-alpaca-gpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<string>:95\u001b[0m, in \u001b[0;36munsloth_push_to_hub\u001b[1;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\transformers\\utils\\hub.py:872\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[1;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    870\u001b[0m organization \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 872\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_repo(\n\u001b[0;32m    873\u001b[0m     repo_id, private\u001b[38;5;241m=\u001b[39mprivate, token\u001b[38;5;241m=\u001b[39mtoken, repo_url\u001b[38;5;241m=\u001b[39mrepo_url, organization\u001b[38;5;241m=\u001b[39morganization\n\u001b[0;32m    874\u001b[0m )\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[0;32m    877\u001b[0m model_card \u001b[38;5;241m=\u001b[39m create_and_tag_model_card(\n\u001b[0;32m    878\u001b[0m     repo_id, tags, token\u001b[38;5;241m=\u001b[39mtoken, ignore_metadata_errors\u001b[38;5;241m=\u001b[39mignore_metadata_errors\n\u001b[0;32m    879\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\transformers\\utils\\hub.py:679\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[1;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[0;32m    676\u001b[0m             repo_id \u001b[38;5;241m=\u001b[39m repo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    677\u001b[0m         repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 679\u001b[0m url \u001b[38;5;241m=\u001b[39m create_repo(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, token\u001b[38;5;241m=\u001b[39mtoken, private\u001b[38;5;241m=\u001b[39mprivate, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\u001b[38;5;241m.\u001b[39mrepo_id\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3511\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[1;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[0;32m   3508\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   3510\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3511\u001b[0m     hf_raise_for_status(r)\n\u001b[0;32m   3512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   3513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[0;32m   3514\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\anaconda3\\envs\\xinference\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:481\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67d9b64a-528fa7e41fcffba160b613b8;a6821da6-ffea-4101-80dc-b05a45478ef5)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "# 需提前登录：huggingface-cli login\n",
    "model.push_to_hub(\"scdow/deepseek-r1-distill-qwen-1.5b-unsloth-bnb-4bit-vicgalle-alpaca-gpt\")\n",
    "tokenizer.push_to_hub(\"scdow/deepseek-r1-distill-qwen-1.5b-unsloth-bnb-4bit-vicgalle-alpaca-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 0.0 out of 15.78 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 93.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained_gguf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\unsloth\\save.py:1856\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[1;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[0;32m   1853\u001b[0m is_sentencepiece_model \u001b[38;5;241m=\u001b[39m check_if_sentencepiece_model(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;66;03m# Save to GGUF\u001b[39;00m\n\u001b[1;32m-> 1856\u001b[0m all_file_locations, want_full_precision \u001b[38;5;241m=\u001b[39m save_to_gguf(\n\u001b[0;32m   1857\u001b[0m     model_type, model_dtype, is_sentencepiece_model,\n\u001b[0;32m   1858\u001b[0m     new_save_directory, quantization_method, first_conversion, makefile,\n\u001b[0;32m   1859\u001b[0m )\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;66;03m# Save Ollama modelfile\u001b[39;00m\n\u001b[0;32m   1862\u001b[0m modelfile \u001b[38;5;241m=\u001b[39m create_ollama_modelfile(tokenizer, all_file_locations[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\unsloth\\save.py:1085\u001b[0m, in \u001b[0;36msave_to_gguf\u001b[1;34m(model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, _run_installer)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     quantize_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: The file (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/llama-quantize.exe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you are on Windows WSL) or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama.cpp/quantize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m   1087\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1088\u001b[0m     )\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# See https://github.com/unslothai/unsloth/pull/730\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;66;03m# Filenames changed again!\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\nBut we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file."
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"model\", tokenizer)\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用微调过的模型\n",
    "from IPython.display import Markdown\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"deepseek_finetuned_model\",\n",
    "                       messages=[{\"role\": \"user\",\n",
    "                                  \"content\": \"How to add chart to a document?\"},\n",
    "                      ])\n",
    "\n",
    "Markdown(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLAMA is a powerful tool that enables you to run large language models (LLMs) directly on your own computer (laptop or desktop).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T20:22:33.733920Z",
     "iopub.status.busy": "2025-01-28T20:22:33.733430Z",
     "iopub.status.idle": "2025-01-28T20:23:25.235067Z",
     "shell.execute_reply": "2025-01-28T20:23:25.233937Z",
     "shell.execute_reply.started": "2025-01-28T20:22:33.733884Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "############################################################################################# 100.0%##                                                                                     11.3%############################                                                         41.9%###################################                                               52.2%1.4%####################################################################             88.3%#########################################################################        94.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# !curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T20:23:25.236859Z",
     "iopub.status.busy": "2025-01-28T20:23:25.236588Z",
     "iopub.status.idle": "2025-01-28T20:29:56.857808Z",
     "shell.execute_reply": "2025-01-28T20:29:56.856335Z",
     "shell.execute_reply.started": "2025-01-28T20:23:25.236835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'ggml/src/ggml-kompute/kompute'\n",
      "Cloning into '/kaggle/working/llama.cpp/ggml/src/ggml-kompute/kompute'...\n",
      "Submodule path 'ggml/src/ggml-kompute/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\n",
      "Requirement already satisfied: gguf in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from gguf) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from gguf) (6.0.2)\n",
      "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.10/dist-packages (from gguf) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from gguf) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->gguf) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->gguf) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->gguf) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->gguf) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->gguf) (2024.2.0)\n",
      "make: Entering directory '/kaggle/working/llama.cpp'\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
      "-- Configuring done (1.7s)\n",
      "-- Generating done (0.3s)\n",
      "-- Build files have been written to: /kaggle/working/llama.cpp/build\n",
      "[  0%] Generating build details from Git\n",
      "[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n",
      "[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n",
      "[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n",
      "[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n",
      "[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n",
      "[ 11%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "[ 15%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n",
      "[ 19%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n",
      "[ 19%] Built target build_info\n",
      "[ 19%] Linking CXX static library libggml-base.a\n",
      "[ 19%] Built target ggml-base\n",
      "[ 19%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n",
      "[ 23%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\n",
      "[ 26%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\n",
      "[ 26%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n",
      "[ 30%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\n",
      "[ 30%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\n",
      "[ 34%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n",
      "[ 34%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n",
      "[ 38%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n",
      "[ 38%] Linking CXX static library libggml-cpu.a\n",
      "[ 38%] Built target ggml-cpu\n",
      "[ 42%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n",
      "[ 42%] Linking CXX static library libggml.a\n",
      "[ 42%] Built target ggml\n",
      "[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n",
      "[ 46%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n",
      "[ 50%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n",
      "[ 50%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n",
      "[ 53%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n",
      "[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n",
      "[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n",
      "[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n",
      "[ 61%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n",
      "[ 61%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n",
      "[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n",
      "[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n",
      "[ 69%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n",
      "[ 69%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n",
      "[ 73%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n",
      "[ 73%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n",
      "[ 76%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n",
      "[ 76%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n",
      "[ 80%] Linking CXX static library libllama.a\n",
      "[ 80%] Built target llama\n",
      "[ 84%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n",
      "[ 84%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n",
      "[ 84%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n",
      "[ 88%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n",
      "[ 92%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n",
      "[ 92%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n",
      "[ 92%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n",
      "[ 96%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n",
      "[ 96%] Linking CXX static library libcommon.a\n",
      "[ 96%] Built target common\n",
      "[ 96%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n",
      "[100%] Linking CXX executable ../../bin/llama-quantize\n",
      "[100%] Built target llama-quantize\n",
      "[  3%] Built target build_info\n",
      "[ 19%] Built target ggml-base\n",
      "[ 38%] Built target ggml-cpu\n",
      "[ 42%] Built target ggml\n",
      "[ 80%] Built target llama\n",
      "[ 96%] Built target common\n",
      "[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\n",
      "[100%] Linking CXX executable ../../bin/llama-export-lora\n",
      "[100%] Built target llama-export-lora\n",
      "[  3%] Built target build_info\n",
      "[ 19%] Built target ggml-base\n",
      "[ 38%] Built target ggml-cpu\n",
      "[ 42%] Built target ggml\n",
      "[ 80%] Built target llama\n",
      "[ 96%] Built target common\n",
      "[ 96%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\n",
      "[100%] Linking CXX executable ../../bin/llama-cli\n",
      "[100%] Built target llama-cli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n",
      "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
      "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
      "Unsloth: Will remove a cached repo with size 6.0G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 21.16 out of 31.35 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 14/32 [00:00<00:00, 32.79it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|██████████| 32/32 [00:15<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model/pytorch_model-00001-of-00004.bin...\n",
      "Unsloth: Saving model/pytorch_model-00002-of-00004.bin...\n",
      "Unsloth: Saving model/pytorch_model-00003-of-00004.bin...\n",
      "Unsloth: Saving model/pytorch_model-00004-of-00004.bin...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at model into q8_0 GGUF format.\n",
      "The output location will be /kaggle/working/model/unsloth.Q8_0.gguf\n",
      "This might take 3 minutes...\n",
      "2025-01-28 20:27:27.181333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-28 20:27:27.208793: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-28 20:27:27.215399: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Writing:  53%|█████▎    | 4.54G/8.53G [02:19<01:55, 34.4Mbyte/s]Traceback (most recent call last):\n",
      "  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 5140, in <module>\n",
      "    main()\n",
      "  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 5134, in main\n",
      "    model_instance.write()\n",
      "  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 443, in write\n",
      "    self.gguf_writer.write_tensors_to_file(progress=True)\n",
      "  File \"/kaggle/working/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 454, in write_tensors_to_file\n",
      "    ti.tensor.tofile(fout)\n",
      "  File \"/kaggle/working/llama.cpp/gguf-py/gguf/lazy.py\", line 211, in tofile\n",
      "    return eager.tofile(*args, **kwargs)\n",
      "OSError: Not enough free space to write 62390272 bytes\n",
      "Writing:  53%|█████▎    | 4.54G/8.53G [02:21<02:04, 32.0Mbyte/s]\n",
      "Unsloth: Conversion completed! Output location: /kaggle/working/model/unsloth.Q8_0.gguf\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7ea4c770b5c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained_gguf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodelfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0mmodelfile_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_save_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Modelfile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained_gguf(\"model\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the OLLAMA Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-28T20:29:56.858594Z",
     "iopub.status.idle": "2025-01-28T20:29:56.859025Z",
     "shell.execute_reply": "2025-01-28T20:29:56.858824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3)\n",
    "print(tokenizer._ollama_modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command registers the fine-tuned model (deepseek_finetuned_model) with Ollama. Once registered:\n",
    "\n",
    "- The model can be run locally using Ollama.\n",
    "- It becomes accessible for further tasks, such as querying, evaluating, or deploying in specific applications.\n",
    "- Ollama ensures the model is formatted and stored correctly for efficient usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ollama create deepseek_finetuned_model -f ./model/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"deepseek_finetuned_model\",\n",
    "            messages=[{ \"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"\n",
    "            },\n",
    "                      ])\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, 1, 2, 3, 5, 8, 13, 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"deepseek_finetuned_model\",\n",
    "                       messages=[{\"role\": \"user\",\n",
    "                                  \"content\": \"How to add chart to a document?\"},\n",
    "                      ])\n",
    "\n",
    "Markdown(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a chart to a document, follow these steps:\n",
    "\n",
    "- Insert a Table: Start by inserting a table into the document. You can do this using the 'Table' tool in most word processors.\n",
    "- Insert Data: Add data into the table. Ensure that your data is properly formatted and organised before adding the chart.\n",
    "- Choose a Chart Type: Select the type of chart you want to create from the available options (e.g., bar chart, pie chart, line graph, etc.).\n",
    "- Edit the Chart Data: Add the necessary data points and formatting to the chart using the chart editor that appears once the chart is selected.\n",
    "- Format the Table and Chart Together: Make sure the table and chart work well together by adjusting alignment, spacing, and other design elements as needed.\n",
    "\n",
    "For more detailed instructions, you may want to consult a guide or use a tool such as Microsoft Word's chart features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "xinference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
